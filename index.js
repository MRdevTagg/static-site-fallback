const getArticleNumberFromURL = () => {
  const urlParams = new URLSearchParams(window.location.search);
  return urlParams.get("manuscript");
};

function splitTextIntoParagraphs(text) {
  const lines = text.split("\n").filter(Boolean);
  const paragraphs = lines.map((line) => `<p>${line.trim()}</p>`);
  return paragraphs.join("\n");
}

const renderImage = (num, fig, alt = "") => {
  return `<img src="./assets/article-${num}/fig${fig}.png" alt="${alt}" /><figcaption>${alt}</figcaption>`;
};

const generateOrderedList = (items) => {
  let ol = "<ol>";
  items.forEach((item) => {
    ol += `<li>${item}</li>`;
  });
  ol += "</ol>";
  return ol;
};

const generateUnorderedList = (items) => {
  let ul = "<ul>";
  items.forEach((item) => {
    ul += `<li>${item}</li>`;
  });
  ul += "</ul>";
  return ul;
};

const article9Sections = {
  Abstract: /* html */ `
    <p>It is traditionally believed that peer review is the backbone of anacademicjournal and scientific communication, ensuringhigh quality and trust in the published materials. However, peer review only became an institutionalized practice in the second half of the 20th century, although the first scientific journals appeared three centuries earlier. By the beginning of the 21st century, there emerged an opinion that the traditional model of peer review is in deep crisis. This study aims to synthesize thekeycharacteristics, practices, and outcomes of traditional and innovative peer review models in scholarly publishing.The article discusses the evolution of the institution of scientific peer review and the formation of the current crisis. Weanalyzethe modern landscape of innovations in peer review and scientific communication. Based on this analysis, three main peer review models in relation to editorial workfloware identified: pre-publication peer review (traditional model), registered reports,and post-publication (peer)review (including preprints(peer) review). Wearguethatthe third model offers the best way to implementthe main functions of scientific communication.Keywords:scientific communication, academicjournal, peer review, pre-publication peer review, prereview, post-publication review, preprints, registered reports</p>
    `,
  Introduction: /* html */ `
    <p>International journals often require additional assessment of the level of English. However, this is not a comprehensive list, and the list of criteria to be evaluated may vary from journal to journal. Nevertheless, as early as the beginning of the 21st century, it was argued that the system of peer review is "broken" (McCook, 2006). The main problem noted by McCook is the increasing number of manuscripts and the burden on reviewers. However, this is just the tip of the iceberg. Allen et al. (2022) highlighted the issue of the "black box": the anonymity of traditional peer review should maintain honesty and ethical norms, but it also can stifle discussion, generate biases, and reduce the overall effectiveness of peer review. In fact, the function of being the "supreme judge" in deciding what is "good" and "bad" science is taken on by peer review, defending the dominant scientific paradigm and stifling the emergence of new ideas that always arise on the periphery. However, as academician L.I. Abalkin once remarked, "no one has the right to usurp the truth" (Sukharev, 2020, p. 44). If we do not change our approach, science will either stagnate or transition into other forms of communication. Moreover, the current system has become an "exploitation machine": publishers benefit in most cases, while reviewers work voluntarily. There is a point of view that peer review is included in the implicit contract of the researcher. Nevertheless, given that most of the research and, accordingly, research positions are funded from public funds, we nonetheless observe a tendency to "reap where they did not sow." R. Smith (2006) strongly criticized the review while at the same time comparing it to democracy: "a system full of problems but the least worst we have" (p. 178). Is this really the case? And can we talk about peer review as a uniform concept, given the variety of existing models? This study aims to synthesize the key characteristics, practices, and outcomes of traditional and innovative peer review models in scholarly publishing. In the following section, we will attempt to demonstrate how the traditional peer review model has developed and how it has come to the current crisis. Furthermore, we will discuss possible ways to overcome the crisis and how the institution of peer review is evolving in the context of global changes in scientific publishing. A separate section discusses modular publishing, which incorporates various innovations in the publishing process, and in particular, the review process. The scope of this piece is limited to peer review in the context of the publication of scientific articles, but its findings are quite applicable to the publication of books or conference proceedings. At the same time, review for other purposes, e.g., evaluation of grant applications, is a topic for a separate discussion</p>
    `,
  "Evolution and crisis of peer review": /* html */ `
    <p>The practice of prepublication peer review as we understand it today emerged much later than the founding of first academic journals. E.g., Journal des Sçavans, which was published from 1665 and is considered the first academic journal, printed a warning on the first page “We aim to report the ideas of others without guaranteeing them” (Rennie, 1999, p. 2).</p>

<p>However, Kronick (1990) argued that peer review as feedback from peers, in the broad sense of the word, existed as soon as scientists began to exchange research results. Peer review emerged in the form of letters, reviews, and comments that appeared after publication (usually in the case of books).</p>

<p>A narrower understanding of peer review, as an evaluation of scientific work by peers before publication (prereview), first appeared in 1731 in the first issue of Medical Essays and Observations, published by the Royal Society of Edinburgh.</p>
<p>In 1752, the Royal Society of
London took responsibility for publication of Philosophical Transactions and established the
"Committee on Papers." The review process was conducted by the members of the Royal Society
with the highest expertise in the topics under consideration. In the late 1890s, the printed peer
review report was introduced as a supplement to invitation letter (Fyfe, 2019).
Despite the emergence of pre-publication peer review in the 18th century, it remained a noninstitutionalized practice for a long time. In many journals, peer review was not conducted, and
the decision on publication was mainly made by the editor. From this point of view, academic
journals of the 17th-19th centuries more closely resembled modern newspapers or popular
magazines. Peer review became a standard practice only after World War II (Chapelle, 2014).
E.g., the well-known British medical journal The Lancet introduced the practice of obligatory
peer review in 1976. The rapidly increasing flow of manuscripts played a key role in the
institutionalization of peer review, prompting journals to conduct an "entry filtration" of content.
Thus, by the second half of the 20th century, the traditional model of pre-publication peer review
had been definitively established (Fig. 1).</p>
${renderImage(
  9,
  1,
  "Figure 1: Evolution of peer review models in scientific communication"
)}
<p>The institutionalization of peer review is manifested in the development of ethical principles
which have been adopted by the majority of the academic community. One of the most wellknown documents, Ethical Guidelines for Peer Reviewers (2013), was developed by the
Committee on Publication Ethics (COPE). This document contains basic principles for
reviewers, which have become common practice in the workflow of academic publishers
worldwide. Furthermore, most academic journals have a section on their websites that describes
the peer review policy applied by the journal.
Despite the progress in editorial policies, which we observe in the formalization of requirements
for the peer review process, opinions have been expressed since the end of the 20th century
about a crisis in peer review as an institution. In the Introduction, we have already mentioned</p>
<p>
some of the existing issues, and now we will consider them in more detail. In particular, the
following issues can be highlighted:
</p>
${generateOrderedList([
  "The rapidly growing volume of manuscripts, coupled with an increasing workload for researchers and faculty, leads to a shortage of reviewers. The primary reason for declining to review is the simple lack of time (Tite & Schroter, 2007; Willis, 2016). This issue causes extension of review periods and frustration of authors.",
  "The shortage of reviewers forces journals to expand their search. Sometimes, this results in manuscripts being reviewed by researchers who do not possess sufficient expertise in the subject. Several studies have noted a low level of consensus among reviewers (Bornmann, 2011), leading some research to refer to peer review as a “game of chance” (Neff & Olden, 2006). The low level of peer review also contributes to the crisis of reproducibility in scientific research (Stoddart, 2016). Although this crisis is due to a multitude of factors, the peer review system bears a significant responsibility for it.",
  "The current peer review system exacerbates inequality in science. Bias often hides behind anonymity, creating a 'black box' problem. Despite constant calls for equality and inclusivity in science (COPE, 2021), a few groups still dominate scientific periodicals, such as male authors from the United States and the United Kingdom. O. M. Smith et al. (2023) analyzed 300,000 manuscripts in biological sciences and concluded that authors from historically excluded communities face worse outcomes in peer review, and journal efforts to eliminate reviewer bias have not yet been successful. Nevertheless, we must recognize the debatable nature of this issue. For instance, Squazzoni et al. (2021) found no systematic bias against manuscripts submitted by women in the peer review process, with some evidence of favorable treatment for women in certain fields. Walker et al. (2015) found that the gender of the author and the characteristics of the author's institution had a significant impact on the review outcomes. However, it is impossible to determine whether this was due to objective differences in scientific merit or to biases.",
  "Continuing from point 3, peer review is also often seen to protect widely accepted approaches and concepts to the detriment of novelty. Peer review can inadvertently stifle innovation and radical new ideas (Steinhauser et al., 2012). The process tends to favor established concepts and discourage the publication of unusual or disparate discoveries (Hess, 1975). As a result, it may limit opportunities for game-changing scientific discoveries (Braben & Dowler, 2017). The neoclassical school in economics may be seen as an example of this phenomenon. The crisis of the neoclassical school began in the early 21st century (Williams & McNeill, 2005), partly due to the inability to explain the global financial crisis of 2008 (Keen, 2015). However, a paradigm shift has still not occurred - the neoclassical school still occupies a central position in the economic science (and the policies of many countries).",
  "Finally, the current form of peer review is simply inefficient. On the one hand, long peer review slows down the process of disseminating new knowledge (see point 1), and on the other hand, often a large number of reviews are required for a single article. The reason for this is that when authors receive a rejection from one journal, they often submit the same article to another journal, starting the entire process anew. Aczel et al. (2021) found that in 2021, reviewers worldwide spent over 100 million hours, equivalent to more than 15,000 years. If we evaluate this time in terms of money, the cost for reviewers in the USA amounted to over $1.5 billion, in China over $600 million, and in the UK around $400 million. Therefore, peer review is a quite costly activity, and currently, doubts arise regarding the efficiency of its utilization.",
])}
    `,
  "Innovations in peer review": /* html */ `
    <p>We have demonstrated the current crisis of the traditional peer review model. In this regard, the
question arises about the possible ways to overcome the crisis. Recently, a lot of literature has
been published on innovations in the field of peer review (see reviews by Kaltenbrunner et al.,
2022; Woods et al., 2022). Waltman et al. (2023) classified innovations in peer review into four
"schools of thought." We propose adding a parameter to this typology that will characterize
innovations relative to the currently dominant publication workflow (incremental / radical). It
enables creation of a two-factor matrix (Table 1).</p>
${renderImage(
  9,
  2,
  "Source: compiled by the author based on Waltman et al. (2023)."
)}
<p>We should acknowledge that the above-mentioned innovations can simultaneously be placed in different groups. For example, registered reports not only aim to improve the quality of peer review, but also aim to contribute to its efficiency. Now, let's consider each of the directions in detail.</p>
<h4>3.1. Quality and reproducibility</h4>
<p>Training of reviewers through seminars and online courses is part of the strategies of many
publishers3
. At the same time, we have not been able to find statistical data or research to assess
the effectiveness of such training. Software for automatic evaluation of scientific papers based on artificial intelligence (AI) has emerged relatively recently (StatReviewer4
, UNSILO5
)
6
. We can
also allocate here the package for checking statistical analysis statcheck7
. Currently, these are just
auxiliary tools that cannot replace human labor (Baker, 2015; Heaven, 2018), but considering the
pace of development of generative AI technologies, these tools have a great future. The
increasing role of data in scientific research has led some publishers to recognize the need for
review of datasets (e.g., PLOS (A Reviewer’s Quick Guide to Assessing Open Datasets, n.d.).
This also applies to review of code in research papers8
.
We have identified registered reports as a radical innovation because it changes the view of the
publication workflow and the object of peer review (Registered Reports: Peer Review before
Results Are Known to Align Scientific Values and Practices., n.d.). Registered reports are a
special type of empirical publication that reflects a hypothetico-deductive approach in science
(Fig. 2). Studies are registered and undergo the first stage of review at the early stages of
research process. In this case, the research question and methodological approach are evaluated
directly. If the peer review results are positive, the study is provisionally accepted for
publication, after which data collection, analysis, and interpretation are carried out. These steps
are followed by the second stage of peer review, during which the conducted study is compared
to the previously registered methodological approach (study protocol).</p>
${renderImage(
  9,
  3,
  "Figure 2: Registered reports - publication workflow (Model 2)."
)}
<p>It should be noted that most initiatives aimed at improving the quality of peer review
simultaneously increase the costs.</p>

<h4>3.2. Democracy and transparency</h4>
<p>The approach to peer review in which only the rigor and soundness of the methodology are
reviewed (as applied, for example, in PLOS ONE and Scientific Reports) somewhat resembles
registered reports, with the difference that the review is conducted in a single stage. This
preserves the traditional publication workflow but changes the object of review. The key
motivation in this case is that the broader academic community will be better able to assess the
significance and contribution of the study than just editor and peer reviewers (Spezi et al., 2017).
The next level of "openness" is open peer review, where the reviews are available to readers
along with the published article (biomedical journals such as BMJ and BMC were pioneers in adopting this innovation). Wolfram et al. (2020) identified 617 journals that published at least
one article with open identities or open peer review reports as of 2019. Though a steady growth
of open peer review adoption has been observed recently, publishers have implemented this
practice in different ways, resulting in different levels of transparency. Another issue is that in the
case of rejection, only authors see the reviews.
Post-publication review, which is most often implemented in the form of open review of
preprints, can be considered the most transparent approach. This approach radically changes the
essence of peer review. It is no longer a tool for deciding whether to publish a paper or not, but
rather a platform for discussion. Publication is no longer the final stage of work; it becomes its
starting point. Platforms such as eLife9
, Peer Community in10, and F1000Research11 use a model
called “Publish-Review-Curate” (PRC). The MetaROR project using this model of review is
expected to launch this year (Kaltenbrunner et al., 2023). The PRC model is shown in Fig. 3. It is
important to note that for each specific case it will be slightly different. E.g., in the case of
MetaROR, the publication is initially hosted on preprint servers such as arXiv, MetaArXiv,
SocArXiv, bioRxiv, or OSF Preprints.</p>
${renderImage(9, 4)}
${renderImage(
  9,
  5,
  "Figure 3: “Publish-Review-Curate” model (Model 3). Model 3a involves uploading the manuscript directly to a platform (e.g., F1000Research). Model 3b, on the other hand, involves initially posting a preprint on a preprint server followed by peer review on a peer review platform (e.g., eLife and Peer Community in)."
)}
<p>In addition to the projects mentioned, there are other platforms, for example, PREreview12
,
which departs even more radically from the traditional review format due to the decentralized
structure of work.</p>

<h4>3.3. Equity and inclusion</h4>
<p>
    The principles of equity and inclusion, as well as the inappropriateness of biases of different origins (geographic, gender, ethnicity), are reflected in numerous recommendations (e.g., COPE, 2021; Royal Society of Chemistry, 2020) and policies of most major academic publishers. However, as mentioned above, the results of implementing these policies are still far from successful, and perhaps these processes require more time. Double-blind peer review is intended to protect the identity of the author and thereby prevent bias in the review. This practice has been used for quite a time in the social sciences and humanities (Horbach & Halffman, 2020; Karhulahti & Backe, 2021). However, anonymity is very conditional - there are still many “keys” left in the manuscript, by which one can determine, if not the identity of the author, then his country, research group, or affiliated organization. On the other hand, the reviewer's identity is much more securely protected. This issue is especially evident in localized communities: in Russia we often encounter deliberately positive or deliberately negative reviews (Sukharev, 2020). The same is true in specialized fields where reviewers may have conflicts of interest (Rühli et al., 2009). Thus, “closeness” is not a good way to address biases.
</p>

<h4>3.4. Efficiency and incentives</h4>
<p>
    Any work requires not only an internal motive, but also external incentives. Peer review, as one of the key activities in science, requires appropriate recognition. This practice is implemented in the form of certificates of recognition from academic publishers, as well as records reflected in the profiles of researchers on various platforms (Web of Science, ORCID). Unfortunately, at the moment, peer review is practically not taken into account in the systems of reward and recognition of researchers and faculty adopted at universities and at the national level. Note that open review increases the visibility of reviewers' work, which should potentially affect recognition. As mentioned, traditional peer review faces efficiency issues. This is largely due to the fact that the same article, having been rejected in one journal, is submitted to another, where peer review process begins from scratch. One way to solve this problem would be to transfer reviews between journals, also known as “portable peer review.” At the moment, this model is used by large publishing houses (manuscript transfer to another journal of the same publishing house). There are also consortia of journals, such as the Neuroscience Peer Review Consortium (Saper et al., 2009), as well as Manuscript Exchange Common Approach (MECA), an initiative that supports the exchange of manuscripts and reviews between journals and platforms, including preprint servers (NISO RP-30-2023, Manuscript Exchange Common Approach (MECA) (Version 2.0.1), 2023). Although review exchange reduces peer review costs, it doesn’t significantly change the editorial workflow; thus, it is simply an add-on to Model 1 (the traditional model). The idea of exchanging reviews has evolved into journal-independent peer review. The Reviewer Commons, a consortium of 23 life sciences journals, brought this idea into practice. A manuscript is published on a preprint server and undergoes independent review, following which the author can revise the paper and submit it to one of the consortium members. Improving the quality of peer review is achieved by ensuring that reviewers focus on the manuscript itself,
rather than the question whether it fits a particular journal. However, we believe that journalindependent peer review is a special case of Model 3 (“Publish-Review-Curate”).
</p>
    `,
  "Modular Publishing": /* html */ `
    <p>Strictly speaking, modular publishing is primarily an innovative approach for the publishing
workflow in general rather than specifically for peer review. This approach allows for a more
detailed and in-depth exploration of the research process. Besides, modular publishing, which is
a type of deconstructed publication14, combines different models of peer review. This is why we
have placed this innovation in a separate category. Nevertheless, modular publications can
potentially have a significant impact on peer review practices. Modular publication platforms are
like preprint servers, except that they publish not an entire manuscript, but individual significant
fragments of it (hypotheses, methodologies, datasets, program code, etc.). These items
essentially represent the different stages of a research process. This approach could potentially
allow for feedback on each stage completed.
The most well-known initiatives at the moment are ResearchEquals15 and Octopus16
.
ResearchEquals allows to upload 37 research modules, one of which is "Other". There is a
separate "Review" item. The research modules can be uploaded in any order. Octopus assumes
uploading eight research elements in a certain sequence (one of which is peer review), which is
more consistent with empirical research. Thus, both platforms offer open post-publication
review. Octopus assumes the possibility of revising previously published modules; in
ResearchEquals, there is no possibility of versioning. Based on this, we can conclude that the
review in the modular publishing resembles Model 3, while the idea itself may be seen as an
extension of Model 2. Currently, some of the features are not being implemented due to the
technical limitations of the platforms.
A related initiative is currently being developed by the Centre for Open Science, which plans to
launch a new approach to scientific communication called Lifecycle Journals (Lifecycle
Journals, n.d.). These journals will combine the idea of modular publishing with features such as
post-publication peer review and registered reports. Although the initiative is currently in the
planning stage, it seems promising.</p>
    `,
  "Discussion and Conclusion": /* html */ `
    <p>In the previous sections, we briefly examined the evolution of the peer review and its current
crisis in relation to scientific communication. Next, we explored the main innovations in peer
review, which can be classified according to the course of proposed changes and the degree of
influence on the editorial workflow, incremental and radical. As a result, we can conclude that, at
present, there are three major models of peer review and related editorial workflow:</p>
${generateUnorderedList([
  "Model 1: traditional model (pre-publication peer review),",
  "Model 2: registered reports,",
  "Model 3: “Publish-Review-Curate” (post-publication review).",
])}
<p>Table 2 presents comparative characteristics of these models.</p>
<p>Table 2 – Comparative analysis of the three review models in terms of editorial workflow</p>
${renderImage(9, 6)}
<p>We can also compare the three models in terms of the main functions of science communication</p>
<>(Table 3). The four main functions of scientific communication are registration, dissemination,
certification and archiving (Roosendaal & Geurts, 1997; Taubert, 2017).</p>
<p>Table 3 - Comparative analysis of the three models of review in terms of functions of scientific communication</p>
${renderImage(9, 7)}
${renderImage(9, 8, "* Preprint is optional for Models 1 and 2.")}
<p>Thus, in Model 3, all functions of scientific communication are implemented most quickly and
transparently. The additional costs arising from the independent assessment of information based
on open reviews are more than compensated by the emerging opportunities for scientific
pluralism. Model 3 corresponds to the vision of the International Science Council (ISC) on
"more efficient and effective modes of peer review that are inspired by open norms"
(International Science Council, 2023, p. 12).
The traditional publication process model with a “black box” peer review inside is increasingly
proving its inadequacy. Registered reports are promising but are exclusively focused on
empirical research. The "Publish-Review-Curate" model is universal that we expect to be the
future of scientific publishing. The transition will not happen today or tomorrow, but in the next
5-10 years, the number of projects such as eLife, F1000Research, Peer Community in, or
MetaROR will rapidly increase. We should also note that the constructive elements of Model 3
can be transferred to Model 2 (in terms of openness of the review process, especially on the first
stage).</p>
<p>At the same time, we must recognize the complexity of institutional change. The possibilities for
normative regulation here are quite limited - much depends on the traditions embedded in the
academic community, and it will take a lot of time to change them. Openness is a complex
process that requires three conditions:</p>
${generateOrderedList([
  "Group of people willing to take responsibility for the quality of scientific communication in this academic community,",
  "Authors and reviewers willing to accept this practice,",
  "Appropriate infrastructure.",
])}
<p>Avissar-Whiting et al. (2024) provided a useful toolbox of recommendations for all parties
potentially involved in the preprint review process.
A significant development was the announcement by the Bill & Melinda Gates Foundation of its
new Open Access Policy (Brembs & Drury, 2024). The foundation has discontinued Article
Processing Charges (APCs) and introduced a mandatory requirement for grantees to publish
preprints. In addition, the foundation is supporting the development of an open infrastructure for
preprinting. Later, it was announced that the foundation would be launching a new verified
preprint platform in collaboration with F1000Research (“Gates Foundation Collaborates with
F1000 to Launch Verified Preprint Platform,” 2024). This is another step towards normalizing
post-publication reviews, while the publication of preprints is increasingly becoming the norm
(Drury, 2022).
Post-publication review is a return to the roots of scientific communication. This model will
allow all actors involved to take greater responsibility for their work, authors for their articles,
reviewers for their assessments, and editors for supporting the process of scientific
communication. This is the atmosphere of scientific discussion that we need very much.
However, it is important to acknowledge that current peer review practices vary significantly
across fields. While pre-publication peer review is prevalent in almost all fields, there are
numerous variations in terms of its openness or anonymity. Additionally, while some fields have
successfully incorporated innovations, others continue to resist. E.g., preprint publishing has
been an essential form of publication in physics since 1990s (Ginsparg, 2011), and open peer
review was introduced in biomedical journals prior to other domains (e.g., BMJ and BMC). At
the same time, the social sciences and humanities (SSH), in terms of their peer review and
publication process, remain relatively closed (Ross-Hellauer & Horbach, 2024).
We should also acknowledge that the list of innovations in peer review presented in this paper is
not comprehensive. For instance, we could mention ranking papers instead of reviewing them or
bidding for papers (Birukou et al., 2011). However, these initiatives are not yet widely adopted,
so they do not significantly affect the publishing landscape</p>

<h5>Acknowledgements</h5>
<p>
    The author gratefully acknowledges the peers’ contributions. Ludo Waltman and Wolfgang Kaltenbrunner reviewed draft versions of this paper and provided valuable suggestions for improvement.
</p>

<h5>Competing interests</h5>
<p>
    The author is affiliated with the Centre for Science and Technology Studies of Leiden University, which is involved in the development of the MetaROR project.
</p>

<h5>Contributions</h5>
<p>
    Visualization: DK<br>
    Writing – original draft: DK
</p>

<h5>Data availability statement</h5>
<p>
    Data sharing is not applicable to this article as no new data were created in this study.
</p>
    `,
  References: /* html */ `
<div class="references">
<p>
    A Reviewer’s Quick Guide to Assessing Open Datasets. (n.d.). PLOS. Retrieved January 23, 2024, from https://plos.org/resource/peer-reviewing-data/
</p>
<p>
    Aczel, B., Szaszi, B. & Holcombe, A. O. (2021). A billion-dollar donation: estimating the cost of researchers’ time spent on peer review. Research Integrity and Peer Review, 6(1), 14. https://doi.org/10.1186/s41073-021-00118-2
</p>
<p>
    Allen, K., Reardon, J., Lu, Y., Smith, D. V., Rainsford, E. & Walsh, L. (2022). Towards improving peer review: Crowd-sourced insights from Twitter. Journal of University Teaching & Learning Practice, 19(3). https://ro.uow.edu.au/jutlp/vol19/iss3/02
</p>
<p>
    Avissar-Whiting, M., Belliard, F., Bertozzi, S. M., Brand, A., Brown, K., Clément-Stoneham, G., Dawson, S., Dey, G., Ecer, D., Edmunds, S. C., Farley, A., Fischer, T. D., Franko, M., Fraser, J. S., Funk, K., Ganier, C., Harrison, M., Hatch, A., Hazlett, H., … Williams, M. (2024). Recommendations for accelerating open preprint peer review to improve the culture of science. PLOS Biology, 22(2), e3002502. https://doi.org/10.1371/journal.pbio.3002502
</p>
<p>
    Baker, M. (2015). Smart software spots statistical errors in psychology papers. Nature. https://doi.org/10.1038/nature.2015.18657
</p>
<p>
    Birukou, A., Wakeling, J. R., Bartolini, C., Casati, F., Marchese, M., Mirylenka, K., Osman, N., Ragone, A., Sierra, C. & Wassef, A. (2011). Alternatives to Peer Review: Novel Approaches for Research Evaluation. Frontiers in Computational Neuroscience, 5. https://doi.org/10.3389/fncom.2011.00056
</p>
<p>
    Bornmann, L. (2011). Scientific peer review. Annual Review of Information Science and Technology, 45(1), 197–245. https://doi.org/10.1002/aris.2011.1440450112
</p>
<p>
    Braben, D. & Dowler, R. (2017, 17. September). Peer review processes risk stifling creativity and limiting opportunities for game-changing scientific discoveries. LSE Impact Blog. https://blogs.lse.ac.uk/impactofsocialsciences/2017/09/17/peer-review-processes-riskstifling-creativity-and-limiting-opportunities-for-scientific-discoveries/
</p>
<p>
    Brembs, B. & Drury, L. (2024, 27. March). The Open Access rising tide: Gates Foundation ends support to Article Processing Charges. International Science Council. https://council.science/current/blog/the-open-access-rising-tide-gates-foundation-endssupport-to-article-processing-charges/
</p>
<p>
    Chapelle, F. H. (2014). The History and Practice of Peer Review. Groundwater, 52(1), 1–1. https://doi.org/10.1111/gwat.12139
</p>
<p>
    COPE. (2013). Ethical guidelines for peer reviewers (English). https://doi.org/10.24318/cope.2019.1.9
</p>
<p>
    COPE. (2021). Diversity and inclusivity. https://doi.org/10.24318/RLqSoVsZ
</p>
<p>
    Drury, L. (2022). The Normalization of Preprints. SRELS Journal of Information Management, 79–85. https://doi.org/10.17821/srels/2022/v59i2/169462
</p>
<p>
    Fyfe, A. (2019, 15. September). Quality in peer review: a view through the lens of time. The Royal Society. https://royalsociety.org/blog/2019/09/quality-in-peer-review-a-view-throughthe-lens-of-time/
</p>
<p>
    Gates Foundation Collaborates with F1000 to Launch Verified Preprint Platform. (2024, 4. April). Taylor & Francis News. https://newsroom.taylorandfrancisgroup.com/gatesfoundation-collaborates-with-f1000-to-launch-verified-preprint-platform/
</p>
<p>
    Ginsparg, P. (2011). ArXiv at 20. Nature, 476(7359), 145–147. https://doi.org/10.1038/476145a
</p>
<p>
    Heaven, D. (2018). AI peer reviewers unleashed to ease publishing grind. Nature, 563(7733), 609–610. https://doi.org/10.1038/d41586-018-07245-9
</p>
<p>
    Hess, E. L. (1975). Effects of the review process. IEEE Transactions on Professional Communication, PC-18(3), 196–199. https://doi.org/10.1109/TPC.1975.6591188
</p>
<p>
    Horbach, S. P. J. M. & Halffman, W. (2020). Journal Peer Review and Editorial Evaluation: Cautious Innovator or Sleepy Giant? Minerva, 58, 139–161. https://doi.org/10.1007/s11024-019-09388-z
</p>
<p>
    International Science Council. (2023). The Case for Reform of Scientific Publishing. https://doi.org/10.24948/2023.14
</p>
<p>
    Johnson, R. (2024, 28. June). Beyond the journal: The future of scientific publishing. FEBS. https://network.febs.org/posts/beyond-the-journal-the-future-of-scientificpublishing?channel_id=728-viewpoints
</p>
<p>
    Kaltenbrunner, W., Pinfield, S., Waltman, L., Woods, H. B. & Brumberg, J. (2022). Innovating peer review, reconfiguring scholarly communication: An analytical overview of ongoing peer review innovation activities. SocArXiv. https://doi.org/10.31235/osf.io/8hdxu
</p>
<p>
    Kaltenbrunner, Wolfgang, Waltman, L., Barnett, A., Byrne, J., Chin, J. M., Holcombe, A., Pinfield, S., Vazire, S. & Wilsdon, J. (2023). MetaRoR - a new form of scholarly publishing and peer review for STS. EASST Review, 421. https://easst.net/easst-review/easst-reviewvolume-421-july-2023/metaror-a-new-form-of-scholarly-publishing-and-peer-review-forsts/
</p>
<p>
    Karhulahti, V.-M. & Backe, H.-J. (2021). Transparency of peer review: a semi-structured interview study with chief editors from social sciences and humanities. Research Integrity and Peer Review, 6(1), 13. https://doi.org/10.1186/s41073-021-00116-4
</p>
<p>
    Keen, S. (2015). Post Keynesian Theories of Crisis. The American Journal of Economics and Sociology, 74(2), 298–324. https://doi.org/10.1111/ajes.12099
</p>
<p>
    Kelly, J., Sadeghieh, T. & Adeli, K. (2014). Peer review in scientific publications:benefits, critiques, & a survival guide. The Journal of the International Federation for Clinical Chemistry and Laboratory Medicine, 25(3), 227–243.
</p>
<p>
    Kronick, D. A. (1990). Peer review in 18th-century scientific journalism. JAMA, 263(10), 1321–1322. http://www.ncbi.nlm.nih.gov/pubmed/2406469
</p>
<p>
    Lifecycle Journals. (n.d.). Center for Open Science. Retrieved April 19, 2024, from https://www.cos.io/lifecyclejournals
</p>
<p>
    McCook, A. (2006. February). Is peer review broken? The Scientist. https://www.thescientist.com/uncategorized/is-peer-review-broken-47872
</p>
<p>
    Neff, B. D. & Olden, J. D. (2006). Is Peer Review a Game of Chance? BioScience, 56(4), 333–340. https://doi.org/10.1641/0006-3568(2006)56[333:IPRAGO]2.0.CO;2
</p>
<p>
    NISO RP-30-2023, Manuscript Exchange Common Approach (MECA) (Version 2.0.1). (2023). https://doi.org/10.3789/niso-rp-30-2023
</p>
<p>
    Nosek, B. A., Spies, J. R. & Motyl, M. (2012). Scientific Utopia. Perspectives on Psychological Science, 7(6), 615–631. https://doi.org/10.1177/1745691612459058
</p>
<p>
    Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251). https://doi.org/10.1126/science.aac4716
</p>
<p>
    Registered Reports: Peer review before results are known to align scientific values and practices. (n.d.). Center for Open Science. Retrieved January 22, 2024, from https://www.cos.io/initiatives/registered-reports
</p>
<p>
    Rennie, D. (1999). Editorial peer review: Its development and rationale. In F. Godleeand & T. Jefferson (Eds.), Peer Review in Health Sciences (pp. 1–13). BMJ Books.
</p>
<p>
    Roosendaal, H. E. & Geurts, P. A. T. M. (1997). Forces and functions in scientific communication: an analysis of their interplay. Co-Operative Research in Information Systems in Physics, 1–32.
</p>
<p>
    Ross-Hellauer, T. & Horbach, S. P. J. M. (2024). Additional experiments required: A scoping review of recent evidence on key aspects of Open Peer Review. Research Evaluation. https://doi.org/10.1093/reseval/rvae004
</p>
<p>
    Royal Society of Chemistry. (2020). Joint commitment for action on inclusion and diversity in publishing. https://www.rsc.org/policy-evidence-campaigns/inclusion-diversity/jointcommitment-for-action-inclusion-and-diversity-in-publishing/#:~:text=The Joint commitment for action,a workshop with other publishers
</p>
<p>
    Rühli, F. J., Finnegan, M., Hershkovitz, I. & Henneberg, M. (2009). Peer‐review for the peer‐review system. Human_ontogenetics, 3(1), 3–6. https://doi.org/10.1002/huon.200900004
</p>
<p>
    Saper, C. B., Maunsell, J. H. R. & Sagvolden, T. (2009). The Neuroscience Peer Review Consortium. Behavioral and Brain Functions, 5, 4. https://doi.org/10.1186/1744-9081-5-4
</p>
<p>
    Smith, G. D. & Jackson, D. (2022). Integrity and trust in research and publication: The crucial role of peer review. Journal of Advanced Nursing, 78(11). https://doi.org/10.1111/jan.15438
</p>
<p>
    Smith, O. M., Davis, K. L., Pizza, R. B., Waterman, R., Dobson, K. C., Foster, B., Jarvey, J. C., Jones, L. N., Leuenberger, W., Nourn, N., Conway, E. E., Fiser, C. M., Hansen, Z. A., Hristova, A., Mack, C., Saunders, A. N., Utley, O. J., Young, M. L. & Davis, C. L. (2023). Peer review perpetuates barriers for historically excluded groups. Nature Ecology & Evolution, 7(4), 512–523. https://doi.org/10.1038/s41559-023-01999-w
</p>
<p>
    Smith, R. (2006). Peer Review: A Flawed Process at the Heart of Science and Journals. Journal of the Royal Society of Medicine, 99(4), 178–182. https://doi.org/10.1177/014107680609900414
</p>
<p>
    Spezi, V., Wakeling, S., Pinfield, S., Creaser, C., Fry, J. & Willett, P. (2017). Open-access mega-journals. Journal of Documentation, 73(2), 263–283. https://doi.org/10.1108/JD-06-2016-0082
</p>
<p>
    Squazzoni, F., Bravo, G., Farjam, M., Marusic, A., Mehmani, B., Willis, M., Birukou, A., Dondio, P. & Grimaldo, F. (2021). Peer review and gender bias: A study on 145 scholarly journals. Science Advances, 7(2). https://doi.org/10.1126/sciadv.abd0299
</p>
<p>
    Steinhauser, G., Adlassnig, W., Risch, J. A., Anderlini, S., Arguriou, P., Armendariz, A. Z., Bains, W., Baker, C., Barnes, M., Barnett, J., Baumgartner, M., Baumgartner, T., Bendall, C. A., Bender, Y. S., Bichler, M., Biermann, T., Bini, R., Blanco, E., Bleau, J., … Zwiren, N. (2012). Peer review versus editorial review and their role in innovative science. Theoretical Medicine and Bioethics, 33(5), 359–376. https://doi.org/10.1007/s11017-012-9233-1
</p>
<p>
    Stoddart, C. (2016). Is there a reproducibility crisis in science? Nature. https://doi.org/10.1038/d41586-019-00067-3
</p>
<p>
    Sukharev, O. S. (2020). Topos of Russian peer review (on peer review as creativity, subject to amateurism). Investments in Russia, 10(309), 43–48.
</p>
<p>
    Taubert, N. (2017). Formale wissenschaftliche Kommunikation. In Forschungsfeld Wissenschaftskommunikation (pp. 125–139). Springer Fachmedien Wiesbaden. https://doi.org/10.1007/978-3-658-12898-2_7
</p>
<p>
    Tite, L. & Schroter, S. (2007). Why do peer reviewers decline to review? A survey. Journal of Epidemiology & Community Health, 61, 9–12.
</p>
<p>
    Walker, R., Barros, B., Conejo, R., Neumann, K. & Telefont, M. (2015). Bias in peer review: a case study. F1000Research, 4, 21. https://doi.org/10.12688/f1000research.6012.1
</p>
<p>
    Waltman, L., Kaltenbrunner, W., Pinfield, S. & Woods, H. B. (2023). How to improve scientific peer review: Four schools of thought. Learned Publishing, 36(3), 334–347. https://doi.org/10.1002/leap.1544
</p>
<p>
    Waltman, L., Mulati, B., Ni, R., Wang, J., Lai, K. H. (Adrian), Luwel, M., Noyons, E., van Leeuwen, T. & Weimer, V. (2023). Preprinting and open peer review at the STI 2023 conference: Evaluation of an open science experiment. Leiden Madtrics. https://www.leidenmadtrics.nl/articles/preprinting-and-open-peer-review-at-the-sti-2023-conference-evaluation-of-an-open-science-experiment
</p>
<p>
    Watling, C., Shaw, J., Field, E. & Ginsburg, S. (2023). ‘For the most part it works’: Exploring how authors navigate peer review feedback. Medical Education, 57(2), 151–160. https://doi.org/10.1111/medu.14932
</p>
<p>
    Williams, J. B. & McNeill, J. M. (2005). The Current Crisis In Neoclassical Economics and the Case for an Economic Analysis Based on Sustainable Development. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.1606342
</p>
<p>
    Willis, M. (2016). Why do peer reviewers decline to review manuscripts? A study of reviewer invitation responses. Learned Publishing, 29(1), 5–7. https://doi.org/10.1002/leap.1006
</p>
<p>
    Wolfram, D., Wang, P., Hembree, A. & Park, H. (2020). Open peer review: promoting transparency in open science. Scientometrics, 125(2), 1033–1051. https://doi.org/10.1007/s11192-020-03488-4
</p>
<p>
    Woods, H. B., Brumberg, J., Kaltenbrunner, W., Pinfield, S. & Waltman, L. (2022). Innovations
    in peer review in scholarly publishing: A meta-summary. SocArXiv.
    https://doi.org/10.31235/osf.io/qaksd
</p>
</div>
    `,
};

const manuscriptData = {
  9: {
    title: "Evolution of Peer Review in Scientific Communication",
    doi: "10.31235/osf.io/b2ra3",
    published: "Published on Nov 20, 2024",
    reviews: [
      {
        name: "Balazs Aczel",
        orcid: "0000-0001-9364-4988",
        review: splitTextIntoParagraphs(`
                    The work ‘Evolution of Peer Review in Scientific Communication’ provides a consise and radable summary of the historical role of peer review in modern science. The paper categorises the peer review practices into three models: (1) traditional pre-publication peer review; (2) registered reports; (3) post-publication peer review. The author compares the three models and draws the conclusion that the “third model offers the best way to implement the main funtion of scientific communication”. 

I would contest this conclusion. In my eyes the three models serve different aims - with more or less drawbacks. For example, although Model 3 is less chance to insert bias to the readers, it also weakens the filtering function of the review system. Let’s just think about the dangers of machine-generated articles, paper-mills, p-hacked research reports and so on. Although the editors does some pre-screening for the submissions, in a world with only Model 3 peer review the literature could easily get loaded with even more ‘garbage’ than in a model where additional peers help the screening. 

Compared to registered reports other aspects can come to focus that Model 3 cannot cover. It’s the efficiency of researchers’ work. In the care of registered reports, Stage 1 review can still help researchers to modify or improve their research design or data collection method. Empirical work can be costly and time-consuming and post-publication review can only say that “you should have done it differently then it would make sense” . 

Finally, the author puts openness as a strength of Model 3. In my eyes, openness is a separate question. All models can work very openly and transparently in the right circumstances. This dimension is not an inherent part of the models. 

In conclusion, I would not make verdict over the models, instead emphasise the different functions they can play in scientific communication.

A minor comment: I found that a number of statements lack references in the Introduction. I would have found them useful for statements such as “There is a point of view that peer review is included in the implicit contract of the researcher.”
`),
      },
      {
        name: "Martin Bush",
        orcid: "0000-0001-9018-4373",
        review: splitTextIntoParagraphs(`
In "Evolution of Peer Review in Scientific Communication", Kochetkov provides a point-of-view discussion of the current state of play of peer review for scientific literature, focussing on the major models in contemporary use and recent innovations in reform. In particular, they present a typology of three main forms of peer review: traditional pre-

publication review; registered reports; and post-publication review, their preferred model. The main contribution it could make would be to help consolidate typologies and terminologies, to consolidate major lines of argument and to present some useful visualisations of these. On the other hand, the overall discussion is not strongly original in character.

The major strength of this article is that the discussion is well-informed by contemporary developments in peer-review reform. The typology presented is modest and, for that, readily comprehensible and intuitive. This is to some extent a weakness as well as a strength; a typology that is too straightforward may not be useful enough. As suggested at the end it might be worth considering how to complexify the typology at least at subordinate levels without sacrificing this strength. The diagrams of workflows are particularly clear.

The primary weakness of this article is that it presents itself as an 'analysis' from which they 'conclude' certain results such as their typology, when this appears clearly to be an opinion piece. In my view, this results in a false claim of objectivity which detracts from what would otherwise be an interesting and informative, albeit subjective, discussion, and thus fails to discuss the limitations of this approach. A secondary weakness is that the discussion is not well structured and there are some imprecisions of expression that have the potential to confuse, at least at first.

This primary weakness is manifested in several ways. The evidence and reasoning for claims made is patchy or absent. One instance of the former is the discussion of bias in peer review. There are a multitude of studies of such bias and indeed quite a few meta-analyses of these studies. A systematic search could have been done here but there is no attempt to discuss the totality of this literature. Instead, only a few specific studies are cited. Why are these ones chosen? We have no idea. To this extent I am not convinced that the references used here are the most appropriate. Instances of the latter are the claim that "The most well-known initiatives at the moment are ResearchEquals and Octopus" for which no evidence is provided, the claim that " we believe that journal-independent peer review is a special case of Model 3" for which no further argument is provided, and the claim that "the function of being the "supreme judge" in deciding what is "good" and "bad" science is taken on by peer review" for which neither is provided.

A particular example of this weakness, which is perhaps of marginal importance to the overall paper but of strong interest to this reviewer is the rather odd engagement with history within the paper. It is titled "Evolution of Peer Review" but is really focussed on the contemporary state-of-play. Section 2 starts with a short history of peer review in scientific publishing, but that seems intended only to establish what is described as the 'traditional' model of peer review. Given that that short history had just shown how peer review had been continually changing in character over centuries - and indeed Kochetkov goes on to describe further changes - it is a little difficult to work out what 'traditional' might mean here; what was 'traditional' in 2010 was not the same as what was 'traditional' in 1970. It is not clear how seriously this history is being taken. Kochetkov has earlier written that "as early as the beginning of the 21st century, it was argued that the system of peer review is 'broken'" but of course criticisms - including fundamental criticisms - of peer review are much older than this. Overall, this use of history seems designed to privilege the experience of a particular moment in time, that coincides with the start of the metascience reform movement.

Section 2 also demonstrates some of the second weakness described, a rather loose structure. Having moved from a discussion of the history of peer review to detail the first model, 'traditional' peer review, it then also goes on to describe the problems of this model. This part of the paper is one of the best - and best -evidenced. Given the importance of it to the main thrust of the discussion it should probably have been given more space as a Section all on its own.

Another example is Section 4 on Modular Publishing, in which Kochetkov notes "Strictly speaking, modular publishing is primarily an innovative approach for the publishing workflow in general rather than specifically for peer review." Kochetkov says "This is why we have placed this innovation in a separate category" but if it is not an innovation in peer review, the bigger question is 'Why was it included in this article at all?'.

One example of the imprecisions of language is as follows. The author also shifts between the terms 'scientific communication' and 'science communication' but, at least in many contexts familiar to this reviewer, these are not the same things, the former denoting science-internal dissemination of results through publication (which the author considers), conferences and the like (which the author specifically excludes) while the latter denotes the science-external public dissemination of scientific findings to non-technical audiences, which is entirely out of scope for this article.

A final note is that Section 3, while an interesting discussion, seems largely derivative from a typology of Waltman, with the addition of a consideration of whether a reform is 'radical' or 'incremental', based on how 'disruptive' the reform is. Given that this is inherently a subjective decision, I wonder if it might not have been more informative to consider 'disruptiveness' on a scale and plot it accordingly. This would allow for some range to be imagined for each reform as well; surely reforms might be more or less disruptive depending on how they are implemented. Given that each reform is considered against each model, it is somewhat surprising that this is not presented in a tabular or graphical form.

Beyond the specific suggestions in the preceding paragraphs, my suggestions to improve this article are as follows:

1. Reconceptualize this as an opinion piece. Where systematic evidence can be drawn upon to make points, use that, but don't be afraid to just present a discussion from what is clearly a well-informed author.

2. Reconsider the focus on history and 'evolution' if the point is about the current state of play and evaluation of reforms (much as I would always want to see more studies on the history and evolution of peer review).

3. Consider ways in which the typology might be expanded, even if at subordinate level.

I have no competing interests in the compilation of this review, although I do have specific interests as noted above.
          `),
      },
      {
        name: "Olmo R. van den Akker",
        orcid: "0000-0002-0712-3746",
        review: splitTextIntoParagraphs(`
          missing!!
          `),
      },
      {
        name: "Wendy Leuenberger",
        orcid: "0000-0001-6567-9913",
        review: splitTextIntoParagraphs(`
          Overall thoughts: This is an interesting history piece regarding peer review and the development of review over time. Given the author’s conflict of interest and association with the Centre developing MetaROR, I think that this paper might be a better fit for an information page or introduction to the journal and rationale for the creation of MetaROR, rather than being billed as an independent article. Alternatively, more thorough information about advantages to pre-publication review or more downsides/challenges to post-publication review might make the article seem less affiliated. I appreciate seeing the history and current efforts to change peer review, though I am not comfortable broadly encouraging use of these new approaches based on this article alone.

 

Page 3: It’s hard to get a feel for the timeline given the dates that are described. We have peer review becoming standard after WWII (after 1945), definitively established by the second half of the century, an example of obligatory peer review starting in 1976, and in crisis by the end of the 20th century. I would consider adding examples that better support this timeline – did it become more common in specific journals before 1976? Was the crisis by the end of the 20th century something that happened over time or something that was already intrinsic to the institution? It doesn’t seem like enough time to get established and then enter crisis, but more details/examples could help make the timeline clear. 

Consider discussing the benefits of the traditional model of peer review.

Table 1 – Most of these are self-explanatory to me as a reader, but not all. I don’t know what a registered report refers to, and it stands to reason that not all of these innovations are familiar to all readers. You do go through each of these sections, but that’s not clear when I initially look at the table. Consider having a more informative caption. Additionally, the left column is “Course of changes” here but “Directions” in text. I’d pick one and go with it for consistency.

3.2: Considering mentioning your conflict of interest here where MetaROR is mentioned.

With some of these methods, there’s the ability to also submit to a regular journal. Going to a regular journal presumably would instigate a whole new round of review, which may or may not contradict the previous round of post-publication review and would increase the length of time to publication by going through both types. If someone has a goal to publish in a journal, what benefit would they get by going through the post-publication review first, given this extra time?

There’s a section talking about institutional change (page 14). It mentions that openness requires three conditions – people taking responsibility for scientific communication, authors and reviewers, and infrastructure. I would consider adding some discussion of readers and evaluators. Readers have to be willing to accept these papers as reliable, trustworthy, and respectable to read and use the information in them. Evaluators such as tenure committees and potential employers would need to consider papers submitted through these approaches as evidence of scientific scholarship for the effort to be worthwhile for scientists.

Based on this overview, which seems somewhat skewed towards the merits of these methods (conflict of interest, limited perspective on downsides to new methods/upsides to old methods), I am not quite ready to accept this effort as equivalent of a regular journal and pre-publication peer review process. I look forward to learning more about the approach and seeing this review method in action and as it develops.  
          `),
      },
    ],
    authors: [
      {
        name: "Dmitry Kochetkov",
        affiliations: [],
      },
    ],
    dates: {
      "Curated version": "May 23, 2024",
      "Peer reviewed": "May 23, 2024",
      "Preprint posted": "May 23, 2024",
    },
    sections: article9Sections,
  },
  12: {
    title:
      "Researchers are willing to trade their results for journal prestige: results from a discrete choice experiment",
    doi: "10.31219/osf.io/uwt3b",
    published: "Published on Aug 02, 2024",
    reviews: [
      {
        name: "Stephen Curry",
        orcid: "0000-0002-0552-8870",
        review: `<p class="paragraph">This manuscript reports the results of an interesting discrete choice experiment designed to probe the values and interests that inform researchers’ decisions on where to publish their work.</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">Although I am not an expert in the design of discrete choice experiments, the methodology is well explained and the design of the study comes across as well considered, having been developed in a staged way to identify the most appropriate pairings of journal attributes to include.</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">The principal findings to my mind, well described in the abstract, include the observations that (1) researchers’ strongest preference was for journal impact factor and (2) that they were prepared to remove results from their papers if that would allow publication in a higher impact factor journal. The first of these is hardly surprising –&nbsp;and is consistent with a wide array of literature (and ongoing activism, e.g. through DORA, CoARA). The second is much more striking – and concerning for the research community (and its funders). This is the first time I have seen evidence for such a trade-off.</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">Overall, the manuscript is very clearly written. I have no major issues with the methods or results. However, I think but some minor revisions would enhance the clarity and utility of the paper.
          </p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">First, although it is made clear in Table 1 that the researchers included in the study are all from the medical and clinical sciences, this is not apparent from the title or the abstract. I think both should be modified to reflect the nature of the sample. In my experience researchers in these fields are among those who feel most intensely the pressure to publish in high IF journals. The authors may want also to reflect in a revised manuscript how well their findings may transfer to other disciplines.</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">Second, in several places I felt the discussion of the results could be enriched by reference to papers in the recent literature that are missing from the bibliography. These include (1) Muller and De Rijcke’s 2017 paper on Thinking with Indicators, which discusses how the pressure of metrics impacts the conduct of research (<a href="https://doi.org/10.1093/reseval/rvx023" rel="" target="blank">https://doi.org/10.1093/reseval/rvx023</a>); (2) Bjorn Brembs’ analysis of the reliability of research published in prestige science journals (<a href="https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2018.00376/full" rel="" target="blank">https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2018.00376/full</a>; and (3) McKiernan’s et al.’s examination of the use of the Journal Impact Factor in academic review, promotion, and tenure evaluations (<a href="https://pubmed.ncbi.nlm.nih.gov/31364991/" rel="" target="blank">https://pubmed.ncbi.nlm.nih.gov/31364991/</a>).</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">Third, although the text and figures are nicely laid out, I would recommend using a smaller or different font for the figure legends to more easily distinguish them from body text.</p>

            `,
      },
      {
        name: "Tony Ross-Hellauer",
        orcid: "0000-0003-4470-7027",
        review: `
          <p class="paragraph">Peer Review of Preprint: "Researchers Are Willing to Trade Their Results for Journal Prestige: Results from a Discrete Choice Experiment", <a href="https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Furldefense.com%2Fv3%2F__https%3A%2F%2Fdoi.org%2F10.31219%2Fosf.io%2Fuwt3b__%3B!!NVzLfOphnbDXSw!HdeyeHHei6yWQHFjhN3deSSfp82ur9i9JNOLEVOYZN0BvyslUO2S8DlvjBbautmafJEvlUsxQZbT0JLQX7lO8EcOWleCpgE%24&amp;data=05%7C02%7Ca.l.brasil.varandas.pinto%40cwts.leidenuniv.nl%7C9f47a111adec49d04bb608dd0614ae94%7Cca2a7f76dbd74ec091086b3d524fb7c8%7C0%7C0%7C638673408085227949%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=%2Fk4Kg%2BJVM4lOvkopA0GieRD5h3nDwLeMNsXdg7kcgvE%3D&amp;reserved=0" rel="" target="blank" title="https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Furldefense.com%2Fv3%2F__https%3A%2F%2Fdoi.org%2F10.31219%2Fosf.io%2Fuwt3b__%3B!!NVzLfOphnbDXSw!HdeyeHHei6yWQHFjhN3deSSfp82ur9i9JNOLEVOYZN0BvyslUO2S8DlvjBbautmafJEvlUsxQZbT0JLQX7lO8EcOWleCpgE%24&amp;data=05%7C02%7Ca.l.brasil.varandas.pinto%40cwts.leidenuniv.nl%7C9f47a111adec49d04bb608dd0614ae94%7Cca2a7f76dbd74ec091086b3d524fb7c8%7C0%7C0%7C638673408085227949%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=%2Fk4Kg%2BJVM4lOvkopA0GieRD5h3nDwLeMNsXdg7kcgvE%3D&amp;reserved=0">https://doi.org/10.31219/osf.io/uwt3b</a></p><p class="paragraph">&nbsp;</p><p class="paragraph">Tony Ross-Hellauer, <a href="mailto:tross@know-center.at" rel="" target="blank" title="mailto:tross@know-center.at">tross@know-center.at</a>, 2nd Nov 2024</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">In "<em>Researchers Are Willing to Trade Their Results for Journal Prestige: Results from a Discrete Choice Experiment</em>", the authors investigate researchers’ publication preferences using a discrete choice experiment in a cross-sectional survey of international health and medical researchers. The study investigates publishing decisions in relation to negotiation of trade-offs amongst various factors like journal impact factor, review helpfulness, formatting requirements, and usefulness for promotion in their decisions on where to publish. The research is timely; as the authors point out, reform of research assessment is currently a very active topic. The design and methods of the study are suitable and robust. The use of focus groups and interviews in developing the attributes for study shows care in the design. The survey instrument itself is generally very well-designed, with important tests of survey fatigue, understanding (<em>dominant choice task</em>) and respondent choice consistency (repeat choice task) included. Respondent performance was good or excellent across all these checks. Analysis methods (pMMNL and latent class analysis) are well-suited to the task. Pre-registration and sharing of data and code show commitment to transparency. Limitations are generally well-described.</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">In the below, I give suggestions for clarification/improvement. Except for some clarifications on limitations and one narrower point (reporting of qualitative data analysis methods), my suggestions are only that – the preprint could otherwise stand, as is, as a very robust and interesting piece of scientific work.</p>
          <p class="paragraph">&nbsp;</p>
          <p class="paragraph">1. Respondents come from a broad range of countries (63), with 47 of those countries represented by fewer than 10 respondents. Institutional cultures of evaluation can differ greatly across nations. And we can expect variability in exposure to the messages of DORA (seen, for example, in level of permeation of DORA as measured by signatories in each country, <a href="https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Furldefense.com%2Fv3%2F__https%3A%2F%2Fsfdora.org%2Fsigners%2F).__%3B!!NVzLfOphnbDXSw!HdeyeHHei6yWQHFjhN3deSSfp82ur9i9JNOLEVOYZN0BvyslUO2S8DlvjBbautmafJEvlUsxQZbT0JLQX7lO8EcOYtZsJkA%24&amp;data=05%7C02%7Ca.l.brasil.varandas.pinto%40cwts.leidenuniv.nl%7C9f47a111adec49d04bb608dd0614ae94%7Cca2a7f76dbd74ec091086b3d524fb7c8%7C0%7C0%7C638673408085242099%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=by5mhPfSM0MFFG9LE2iiYjdtSs5IhvpuukqVv%2FLak2s%3D&amp;reserved=0" rel="" target="blank" title="https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Furldefense.com%2Fv3%2F__https%3A%2F%2Fsfdora.org%2Fsigners%2F).__%3B!!NVzLfOphnbDXSw!HdeyeHHei6yWQHFjhN3deSSfp82ur9i9JNOLEVOYZN0BvyslUO2S8DlvjBbautmafJEvlUsxQZbT0JLQX7lO8EcOYtZsJkA%24&amp;data=05%7C02%7Ca.l.brasil.varandas.pinto%40cwts.leidenuniv.nl%7C9f47a111adec49d04bb608dd0614ae94%7Cca2a7f76dbd74ec091086b3d524fb7c8%7C0%7C0%7C638673408085242099%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=by5mhPfSM0MFFG9LE2iiYjdtSs5IhvpuukqVv%2FLak2s%3D&amp;reserved=0">https://sfdora.org/signers/).</a> In addition, some contexts may mandate or incentivise publication in some venues using measures including IF, but also requiring journals to be in certain databases like WoS or Scopus, or having preferred journal lists). I would suggest the authors should include in the Sampling section a rationale for taking this international approach, including any potentially confounding factors it may introduce, and then adding the latter also in the limitations.</p>
          <p class="paragraph">&nbsp;</p>

          <p class="paragraph">2. Reporting of qualitative results: In the introduction and methods, the role of the focus groups and interviews seems to have been just to inform the design of the experiment. But then, results from that qualitative work then appear as direct quotes within the discussion to contextualise or explain results. In this sense though, the qualitative results are being used as new data. Given this, I feel that the methods section should include description of the methods and tools used for qualitative data analysis (currently it does not). But in addition, to my understanding (and this may be a question of disciplinary norms – I’m not a health/medicine researcher), generally new data should not be introduced in the discussion section of a research paper. Rather the discussion is meant to interpret, analyse, and provide context for the results that have already been presented. I personally hence feel that the paper would benefit from the qualitative results being reported separately within the results section.</p>

          <p class="paragraph">&nbsp;</p>

          <p class="paragraph">3. Impact factors – Discussion section: While there is interesting new information on the relative trade-offs amongst other factors, the most emphasised finding, that impact factors still play a prominent role in publication venue decisions, is hardly surprising. More could perhaps be done to compare how the levels of importance reported here differ with previous results from other disciplines or over time (I know a like-for-like comparison is difficult but other studies have investigated these themes, e.g., <a href="https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Furldefense.com%2Fv3%2F__https%3A%2F%2Fdoi.org%2F10.1177%2F0165551520958591__%3B!!NVzLfOphnbDXSw!HdeyeHHei6yWQHFjhN3deSSfp82ur9i9JNOLEVOYZN0BvyslUO2S8DlvjBbautmafJEvlUsxQZbT0JLQX7lO8EcOxpGR1v8%24&amp;data=05%7C02%7Ca.l.brasil.varandas.pinto%40cwts.leidenuniv.nl%7C9f47a111adec49d04bb608dd0614ae94%7Cca2a7f76dbd74ec091086b3d524fb7c8%7C0%7C0%7C638673408085255478%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=w2bB5PB8l9vvCqX14MqsqEClFu2YgzGmb3MQTp3gxj8%3D&amp;reserved=0" rel="" target="blank" title="https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Furldefense.com%2Fv3%2F__https%3A%2F%2Fdoi.org%2F10.1177%2F0165551520958591__%3B!!NVzLfOphnbDXSw!HdeyeHHei6yWQHFjhN3deSSfp82ur9i9JNOLEVOYZN0BvyslUO2S8DlvjBbautmafJEvlUsxQZbT0JLQX7lO8EcOxpGR1v8%24&amp;data=05%7C02%7Ca.l.brasil.varandas.pinto%40cwts.leidenuniv.nl%7C9f47a111adec49d04bb608dd0614ae94%7Cca2a7f76dbd74ec091086b3d524fb7c8%7C0%7C0%7C638673408085255478%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=w2bB5PB8l9vvCqX14MqsqEClFu2YgzGmb3MQTp3gxj8%3D&amp;reserved=0">https://doi.org/10.1177/01655515209585</a>). In addition, beyond the question of whether impact factors are important, a more interesting question in my view is why they still persist. What are they used for and why are they still such important “driver[s] of researchers’ behaviour”? This was not the authors’ question, and they do provide some contextualisation by quoting their participants, but still I think they could do more to contextualise what is known from the literature on that to draw out the implications here. The attribute label in the methods for IF is “ranking”, but ranking according of what and for what? Not just average per-article citations in a journal over a given time frame. Rather, impact factors are used as a proxy indicators of less-tangible desirable qualities – certainly prestige (as the title of this article suggests), but also quality, trust (as reported by one quoted focus group member “I would never select a journal without an impact factor as I always publish in journals that I know and can trust that are not predatory”, p.6), journal visibility, importance to the field, or improved chances of downstream citations or uptake in news media/policy/industry etc. Picking apart the interactions of these various factors in researchers’ choices to make use of IFs (which is not in all cases bogus or unjustified) could add valuable context. I’d especially recommend engaging at least briefly with more work from Science and Technology Studies - especially Müller and de Rijcke’s excellent Thinking with Indicators study (doi: 10.1093/reseval/rvx023), but also those authors other work, as well as work from Ulrike Felt, Alex Rushforth (esp <a href="https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Furldefense.com%2Fv3%2F__https%3A%2F%2Fdoi.org%2F10.1007%2Fs11024-015-9274-5__%3B!!NVzLfOphnbDXSw!HdeyeHHei6yWQHFjhN3deSSfp82ur9i9JNOLEVOYZN0BvyslUO2S8DlvjBbautmafJEvlUsxQZbT0JLQX7lO8EcOF3aJWtI%24&amp;data=05%7C02%7Ca.l.brasil.varandas.pinto%40cwts.leidenuniv.nl%7C9f47a111adec49d04bb608dd0614ae94%7Cca2a7f76dbd74ec091086b3d524fb7c8%7C0%7C0%7C638673408085269138%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=I%2FZBIK6B0cJ0%2B2jm6DK9EiPIbyR0%2BvrivPlkfLqRPAA%3D&amp;reserved=0" rel="" target="blank" title="Original URL:
https://urldefense.com/v3/__https://doi.org/10.1007/s11024-015-9274-5__;!!NVzLfOphnbDXSw!HdeyeHHei6yWQHFjhN3deSSfp82ur9i9JNOLEVOYZN0BvyslUO2S8DlvjBbautmafJEvlUsxQZbT0JLQX7lO8EcOF3aJWtI$

Click to follow link.">https://doi.org/10.1007/s11024-015-9274-5</a>), Björn Hammerfelt and others.</p>
<p class="paragraph">&nbsp;</p>
<p class="paragraph">4. Disciplinary coverage: (1) A lot of the STS work I talk about above emphasises epistemic diversity and the ways cultures of indicator use differ across disciplinary traditions. For this reason, I think it should be pointed out in the limitations that this is research in Health/Med only, with questions on generalisability to other fields. (2) Also, although the abstract and body of the article do make clear the disciplinary focus, the title does not. Hence, I believe the title should be slightly amended (e.g., “Health and Medical Researchers Are Willing to Trade …”)</p>

          `,
      },
    ],
    authors: [
      {
        name: "Natalia Gonzalez Bohorquez",
        affiliations: ["Queensland University of Technology"],
        email: "natalia.gonzalezbohorquez@hdr.qut.edu.au",
      },
      {
        name: "Sucharitha Weerasuriya",
        affiliations: ["Queensland University of Technology"],
        email: "sucharitha.weerasuriya@qut.edu.au",
      },
      {
        name: "David Brain",
        affiliations: ["Queensland University of Technology"],
        email: "david.brain@qut.edu.au",
      },
      {
        name: "Sameera Senanayake",
        affiliations: ["Duke-NUS Medical School"],
        email: "sameera.senanayake@duke-nus.edu.sg",
      },
      {
        name: "Sanjeewa Kularatna",
        affiliations: ["Duke-NUS Medical School"],
        email: "sanjeewa.kularatna@duke-nus.edu.sg",
      },
      {
        name: "Adrian Barnett",
        affiliations: ["Queensland University of Technology"],
        email: "a.barnett@qut.edu.au",
        orcid: "0000-0001-6339-0374",
      },
    ],
    dates: {
      "Curated version": "Sep 12, 2024",
      "Peer reviewed": "Sep 12, 2024",
      "Preprint posted": "Sep 12, 2024",
    },
    sections: {
      Abstract: splitTextIntoParagraphs(`
        The research community’s fixation on journal prestige is harming research quality, as some researchers focus on where to publish instead of what. We examined researchers’ publication preferences using a discrete choice experiment in a cross-sectional survey of international health and medical researchers. We asked researchers to consider two hypothetical journals and decide which they would prefer. The hypothetical journals varied in their impact factor, formatting requirements, speed of peer review, helpfulness of peer review, editor’s request to cut results, and whether the paper would be useful for their next promotion. These attributes were designed using focus groups and interviews with researchers, with the aim of creating a tension between personal and societal benefit. Our survey found that researchers’ strongest preference was for the highest impact factor, and the second strongest for a moderate impact factor. The least important attribute was a preference for making changes in format and wording compared with cutting results. Some respondents were willing to cut results in exchange for a higher impact factor. Despite international efforts to reduce the importance of impact factor, it remains a driver of researchers’ behaviour. The most prestigious journals may have the most partial evidence, as researchers are willing to trade their results for prestige.
        `),
      rest: `
        <p class="paragraph">Peer reviewed publications are academic currency [1]. Having sufficient publications in the bank is important for hiring, promotion and funding [2, 3]. Publications are also a vital record of evidence which can improve policy and practice, and direct future research [4]. Ideally publications could be both useful as academic currency and sources of evidence for scientific progress. However, the value of publications as a currency may be trumping their main purpose to provide reliable evidence [5]. The intrinsic motivation of a “Taste for Science” (described by Merton [6]) may have been superseded by the extrinsic motivation of a “Taste for Publications” [7]. In a “publish or perish” world, researchers may “prefer popularity to intrinsic value” and hence focus on where to publish instead of what to publish [1].</p><p class="paragraph">Most researchers regularly make considered decisions on what journal to submit to and how to navigate peer review. Factors include the journal’s prestige (often defined using the impact factor), the target audience, the article processing charges, the required formatting, and the journal’s rejection rate and turnaround times. The perfect home for a paper is rare [8], and researchers often need to make compromises to be successful [9]. We aimed to study some of the important compromises that researchers make and thus examine how researchers publish their research.<br>We were especially interested in the trade-offs that researchers make concerning personal benefits and the wider benefits for society. We aimed to test trade-offs between earning academic currency and creating an accurate record of the evidence.</p><h2>Results</h2><h3>Sample description</h3><p class="paragraph">The surveys were collected between 26 March 2024 and 30 May 2024 (66 days) (see Supplement S.1). The median time to complete the survey was 7 minutes. We received 616 responses from 7,376 invites giving a response rate of 8.5%; this excludes 170 emails that were no longer active. A classification tree found that the response rate varied by email domain, with a higher response rate of 21% for – amongst others – Australia, Switzerland and the UK, and lower response rate of 3% for – amongst others – China, Germany and Japan (see Supplement S.2). The questions were generally well completed but there was some survey fatigue with under 1% missing the first choice task and 15% missing the tenth and last choice task (Supplement S.3).</p><p class="paragraph">Thirteen percent of respondents found answering the hypothetical choices to be difficult or very difficult. The dominant choice task was selected by over 99% of respondents, indicating an excellent understanding of the attributes and levels. The repeat choice task had the same answer as the original for 79% of respondents, indicating good internal consistency.</p><p class="paragraph">Summary statistics on the sample are in Table 1. Respondents had been working in research for a median of 10 years and had a median of 43 peer reviewed papers. Forty-seven percent were female. The most popular broad research area was Clinical Sciences (57%). Forty percent of respondents had a personal target for their annual number of publications.</p><p class="paragraph">Table 1: Summary statistics on the respondents’ characteristics. Whether researchers had a target number of publications was only asked in the final sample; respondents could tick multiple answers for this question. Q1 = first quartile, Q3 = third quartile.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_t1-1024x835.png" alt="" data-id="c1fbc548-5ea2-4d8e-8919-2e4c664bb105"><figcaption id="c1fbc548-5ea2-4d8e-8919-2e4c664bb105" class="decoration"></figcaption></figure><p class="paragraph">The sample included responses from 63 countries, with the three most common of USA (15%), UK (11%) and Australia (10%) (table of all countries in Supplement S.4).</p><h3>Researchers’ preferences</h3><p class="paragraph">The utilities for each attribute are in Figure 1 and Table 2. The figure also shows the utilities stratified by the respondents’ characteristics and the scenario wording concerning prior rejections.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_1-1024x926.png" alt="" data-id="ca96f181-2f8e-45b6-b3a3-1329346e475a"><figcaption class="wp-element-caption">Figure 1: Utility estimates and 95% confidence intervals for the six attributes. The dotted vertical line at zero is for no difference in utility. Forty-three publications was the sample median. JIF = journal impact factor.</figcaption></figure><p class="paragraph">Table 2: Utilities for the journal preferences and attribute importance. See Table 3 for the full wording of the attributes and levels. JIF = journal impact factor.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_t2-1024x270.png" alt="" data-id="11ab5674-e945-4e88-8313-154cfad55909"><figcaption id="11ab5674-e945-4e88-8313-154cfad55909" class="decoration"></figcaption></figure><p class="paragraph">The strongest preference was for the highest impact factor and the second strongest for the moderate impact factor. The least important attribute was a preference for making changes in format and wording compared with cutting a table and analysis.</p><p class="paragraph">After the impact factor, the next strongest preference was for a helpful review. The utilities for a fast review and minor formatting were similar. Researchers had a clear preference for papers that were useful for their promotion.</p><p class="paragraph">More experienced researchers had a stronger preference for the highest impact factor and minor formatting. Researchers who had more peer reviewed papers had a much stronger preference for the highest and moderate impact factors.</p><p class="paragraph">Female researchers had slightly stronger preferences for helpful reviews and papers that were useful for their promotion.</p><p class="paragraph">There was little difference in researchers’ preferences by whether the paper had been previously rejected or not.</p><p class="paragraph">The latent class results are in Figure 2. The optimal number of groups according to the AIC was four. The largest group had the strongest preferences for impact factor, a relatively small preference for fast results, and a slight preference for cutting results over minor formatting. The second largest group had the strongest preference for a helpful review, with a much reduced – although still positive – preference for journal impact factor. The third group were not concerned about a helpful review, but strongly preferred minor over major formatting and a paper that was useful for their promotion. Ten percent of respondents provided non-informative responses.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_2-1024x964.png" alt="" data-id="13d7cf77-cccd-4be0-b096-fd8073de7101"><figcaption class="wp-element-caption">Figure 2: Utility estimates and 95% confidence intervals for the six attributes using a latent class model. The percents in the panel headers are the group sizes. The dotted vertical line at zero is for no difference in utility. JIF = journal impact factor.</figcaption></figure><h3>Interactions</h3><p class="paragraph">The five planned interactions are plotted in Figure 3 with the estimates in Supplement S.5. When the journal had no impact factor, there was a stronger preference for a faster review. The journal rank had a similar interaction with both the editor’s requests and the style requirements, as there was no difference in utility when the journal had no impact factor. This could indicate an indifference by researchers about their papers in journals without an impact factor.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_3-1024x895.png" alt="" data-id="8aba2156-827f-4b5d-bd4a-df6535fbe41f"><figcaption class="wp-element-caption">Figure 3: Utility estimates for the five planned interactions. The dots are the means and the vertical lines are 95% confidence intervals. The reference group is the left-most level on the x-axis with the green line</figcaption></figure><p class="paragraph">There was an interaction between the editor’s requests and a helpful review, as if the review was not helpful then there was a stronger preference for formatting and wording changes over cutting results. Whereas for helpful reviews, researchers showed little difference between the editor’s requests, which could be because they interpreted all requests as helpful.</p><p class="paragraph">There was a small interaction between a helpful review and speed, as researchers were more willing to wait for a helpful review.</p><h2>Discussion</h2><p class="paragraph">Researchers had the strongest preference for impact factor above any other tested attributes. This was both a desire for high impact factors and an aversion to papers with no impact factor. The importance of impact factor to researchers has been called an “obsession” [10], a “mania” [11], and a “game” that encourages “questionable practices” [12]. Major international initiatives have sought to combat the influence of impact factors, such as DORA in 2012<br>(https://sfdora.org/) and COARA in 2022 (https://coara.eu/). Despite these initiatives and the extensive debate on the negative consequences of using impact factors for evaluating researchers, the highest possible impact factor is a target for many researchers. A focus group participant framed impact factors as useful for “quantifying my academic abilities”, whilst a survey participant commented, “I’ve been told if it isn’t in an impact factor over 10 it doesn’t matter/count”. Journals must be indexed for three years to get an impact factor, but some respondents interpreted a journal without an impact factor as predatory rather than new, as stated by a survey participant, “I would never select a journal without an impact factor as I always publish in journals that I know and can trust that are not predatory.”</p><p class="paragraph">Researchers with more publications and more years of experience had a stronger preference for impact factor (Figure 1). This could be because some early career researchers are yet to understand the importance impact factors. Another explanation is a survivorship effect, as researchers with high impact factor publications have an advantage in employment and promotion [13], whilst researchers with less prestigious papers are out-competed [5].</p><p class="paragraph">Some survey respondents commented that they could not understand how a paper in a high impact factor journal could not be useful for their promotion or fellowship, which was a combination in the discrete choice tasks. This illustrates the power of the impact factor, as it trumps the content of the paper [11]. A recent survey showed how the content of papers is commonly neglected by grant and hiring committees, as over half use journal impact factor to assess credibility [14]. When fellowship and hiring committees make career-changing decisions based on impact factors, this sends a clear signal to researchers to prioritise impact factors over content. A researcher in our interviews appeared comfortable with being assessed based on impact factors: “People have to quantify me by something. So impact factor is a very important way to do that.” However, a focus groups participant recognised that impact factors are usually meaningless when considering real-world impacts: “I’ve been working together with senior executives in the government and federal government. They don’t care about that [journal impact factor], they only want you to give them a half-page summary.”</p><p class="paragraph">A focus group participant gave a perspective on impact factors that was pragmatic and confessional, “Considering and admitting for everybody, for various reasons, usually go for a top ranked journal in its field, and everything, and some of that will be purely mercenary, because that’s what’s required.” Personal values are ceded to the reward systems that use impact factors and/or journal ranking. We aimed to distinguish researchers with a stronger focus on system requirements by asking if they had a target number of publications per year, and 53% had a personal and/or institutional target. However, having a target did not greatly alter researchers’ preferences (Figure 1). Potentially most researchers are “playing the game” and the preference for journal ranking remains high regardless of the desired publication numbers [15].</p><p class="paragraph">A surprising result was the lack of difference in researchers’ preferences for papers that were useful for promotion by experience and publication numbers (Figure 1). This could be because the competition for funding and promotion never ends and researchers are always looking to earn academic currency. Tenured or retired professors may be under less pressure [16] and a professor from the focus groups commented, “I am the least strategic person when it comes to publishing but I think that also comes with seniority as I have no need to ever write a promotion application again!”</p><p class="paragraph">Survey participants were randomised to a scenario where their hypothetical paper had not yet been submitted to a journal or had already been desk-rejected twice (Box 1). This was raised in the focus groups, with comments including: “But then, after many rejections, right? You just want to get it out”. However, in the survey the previous rejections had no effect as researchers’ preferences were remarkably similar (Figure 1). Researchers’ preferences may be impervious to rejection, as the logical approach is to continue to pursue the highest impact factor possible. Preferences may change with more than two rejections or if the rejections were after peer review rather than desk-rejections.</p><p class="paragraph">The lowest utility was for an editor’s request of formatting changes compared with cutting a table and analysis. On average, researchers preferred not to cut their analysis, but this was less of priority than the impact factor, formatting at the submission stage, or the speed of the peer review. In the latent class analysis, the group with the strongest preference for impact factor had a surprising preference for cutting results (Figure 2), showing a willingness to compromise on their evidence to get published in prestigious journals [11]. This compromise was also discussed in our focus groups as a likely trade-off during the peer review process: “I certainly have examples where I have cut things out of papers to try and get something published.” Cutting results has also been discussed in the literature, for example: “Academics who play the ‘publish or perish’ game have a strong incentive to […] accept all ‘suggestions’ by the referees even if one knows that they are misleading or even incorrect” [17], and how during peer review “authors […] remove ideas and insights that they believe in from their work” [18]. To the best of our knowledge, our survey is the first to empirically show this compromise. An important implication is that the journals with the highest impact factors potentially have the most partial evidence, as researchers are more willing to “hold their nose” to satisfy the editors at influential journals [8]. One could argue that the journals were correct, and that the cuts improved the paper. However, in the scenario we told researchers “you believe it [your paper] is good quality” and the cut was 1,000 out of 4,000 words and included a table. Some researchers potentially rationalised this compromise by thinking that the removed results could be included in a supplement, but this relegates their findings at the “whim” of an editor [19].</p><p class="paragraph">An interesting finding from the focus groups and the survey is that researchers showed a relatively strong preference for helpful reviews and were willing to wait longer for helpful reviews. For example, an interview respondent said, “If there’s something that can improve them [my papers], I want them to be improved.” The preference for helpful reviews did not change by the researchers’ experience or number of publications (Figure 1), so it was not restricted to early career researchers. The latent group analysis showed that the second largest group most preferred a helpful review (Figure 2). The relatively strong preference for helpful reviews shows clear support for peer review, as many researchers want the expertise of their peers. Similarly, an international survey on the perception of peer review found that 93% disagree with the claim that peer review is unnecessary and 85% believe that peer review benefits scientific communication [20].</p><h3>Related studies</h3><p class="paragraph">Previous studies have examined researchers’ publication preferences using hypothetical journal choices. Similar to our results, the journal’s impact factor dominated preferences compared with the journal’s editorial board, journal’s standing among peers, quality of reviews, waiting time for reviews, and probability of being accepted [21]. Journal prestige, described using “journal level”, was also the most important attribute to junior authors in a conjoint analysis that compared journal prestige, author numbers, author order, and researchers’ time investment [22]. A choice-set survey found that researchers were willing to trade citations for a more prestigious journal [23].</p><p class="paragraph">A discrete choice experiment examined what metrics academics use when choosing papers to read [24]. There were clear preferences for citation counts, followed by the journal impact factor and download counts.</p><h3>Limitations</h3><p class="paragraph">Our discrete choice experiment was hypothetical and examined stated preferences not revealed preferences.<br>The low response rate (8.5%) reduces our ability to generalise and likely creates a non-response bias. Our approach email included words such as “journal” and “publishing” and so may have appeared similar to the many nuisance journal requests that researchers regularly receive and may have been automatically or manually deleted.</p><p class="paragraph">Respondents to our survey could be more engaged about the publication process than the wider population. We found a difference in response rate by country, hence our results over-represent some countries.</p><h2>Methods</h2><h3>Designing the discrete choice experiment</h3><p class="paragraph">We used a discrete choice experiment to examine researchers’ publication preferences as this is well-suited to testing the multiple trade-offs that researchers make when publishing papers.</p><p class="paragraph">We used multiple stages to design and deploy the discrete choice experiment (see Figure 4 and Supplement S.6 for details). With the aim of considering a wide array of attributes, we started with a literature review of papers that examined one or more potential attributes. The review collected 77 potential attributes about publications, with most concerning the journal (e.g., impact factor), the impact (e.g., social media discussion), and paper’s characteristics (e.g., paper with novel findings).</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_4-1024x915.png" alt="" data-id="5945149d-074a-42a9-9311-44c9dde3c7aa"><figcaption class="wp-element-caption">Figure 4: The stages of designing and deploying the discrete choice experiment to elicit researchers’ publication preferences</figcaption></figure><p class="paragraph">We used focus groups and in-depth interviews with health researchers from Australian academic institutions to explore the most important attributes, collect new attributes, and test potential trade-offs. We recruited participants from our networks and maximised for variation in career stage, gender, and research field. We ran focus groups in clinical sciences (8 participants), public health and health services research (8 participants), and used interviews for the two participants in fundamental science as we did not have enough people for a focus group. We piloted the focus group with 9 participants from health services research. The focus groups and interviews sample sizes were arbitrary, being mostly determined by the number of interested participants.</p><p class="paragraph">We used a semi-structured interview guide with an adapted nominal group technique without consensus [25]. Participants were asked to imagine they had written a paper and were now thinking of submitting it to a journal. They were asked about the attributes they consider most important when submitting to a journal. Each participant talked through up to ten attributes with the group and explained their choices. The attributes mentioned were then added to an online survey and participants voted on their most important attributes, explaining the rationale for their choices. We analysed and selected the attributes using the five steps of attribute development with a distilling approach [26].</p><p class="paragraph">An initial design of eight attributes was tested using a thinking-aloud exercise with ten researchers [27]. Researchers were shown a choice task and were asked to discuss their thoughts aloud on whether: they had any comments on the content or wording; there were any levels that they struggled to understand or that seemed unrealistic; the gaps between any levels were too jarring or obvious; and there was anything missing. This exercise identified that an attribute on journal prestige was sometimes contradictory to an attribute on journal ranking, and hence the prestige attribute was removed.</p><h3>Attribute and level selection</h3><p class="paragraph">The final attributes and levels are in Table 3. In this section we explain the choices behind the attributes and levels, and explain the perceived importance of some attributes and why some attributes were excluded.</p><p class="paragraph">Table 3: The six attributes and their levels for the discrete choice experiment. The first column is a short label used to refer to the attributes.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_t3-1024x345.png" alt="" data-id="6b137755-50e4-4ff7-a513-950a9b7dedbc"><figcaption id="6b137755-50e4-4ff7-a513-950a9b7dedbc" class="decoration"></figcaption></figure><p class="paragraph">Journal impact factor was the most common attribute in the literature review and was also frequently mentioned in the focus groups and interviews. Participants suggested that its importance relied on self-serving purposes like job promotions, grants, and funding, but also it was perceived as a reflection of the excellence of the researcher and a way to quantify the worth of their work. Related to the impact factor was the idea of predatory journals, which raised strong feelings of aversion due to reputational damage (e.g., “I avoid them like the plague”). For the levels, we decided against numeric impact factors because these numbers vary by field [28], hence we used a relative field ranking of the highest, middle, and a journal without an impact factor, which could represent a new journal or a potentially predatory journal.</p><p class="paragraph">Formatting was often considered as a “painful” process. Concerns were mentioned about the time needed to fit a journal’s style requirements, and respondents wanted to avoid onerous systems. We used two simple levels of minor and major formatting.</p><p class="paragraph">Peer review was widely discussed, with researchers interested in the speed and quality of reviews. We framed both these attributes by what their colleagues had told them, as colleagues were an important source of information about prospective journals. We used the relative labels of “slow” and “fast” rather than numeric review times (e.g., 30 days) because average times vary by field [29].</p><p class="paragraph">The focus group discussions uncovered a new issue as some researchers raised experiences of being asked by a journal editor to cut results from their paper at the peer review stage. There were multiple potential reasons including to reduce the word count, to keep a “clean story”, to make the story “digestible”, to remove results that contradicted previous findings, or to remove findings that were not of interest to journals or colleagues. We included this as an attribute as it suited the tension we were aiming to test, being a trade-off between the loss of evidence from presenting an abridged version of the work against the potential benefit of earning a publication. A difference between this attribute and the others is that it occurs post-submission.</p><p class="paragraph">The final attribute was a direct appeal to personal benefit, as it concerned whether the paper was useful or not for their next promotion or fellowship application. An example of a good quality paper that researchers might not use in a fellowship application is a “negative” study, where, for example, a new intervention or treatment did not work (often judged by the arbitrary statistical significance threshold of p &lt; 0.05). “Negative” studies can be less cited and receive less publicity than “positive” studies [30, 31], highlighting their reduced value as academic currency.</p><p class="paragraph">Article processing charges (APCs) were often discussed, but we excluded them as an attribute because they could often not be traded – for example, researchers with no budget to pay APCs. Using charges could have introduced a hypothetical bias, as researchers mostly do not personally pay the APCs and therefore the choices would be not be as meaningful [32].</p><p class="paragraph">Citations were a common attribute in the literature review, but focus group discussions revealed that these were seen as being beyond the control of the researchers and somewhat due to chance. Hence it would not be plausible to use varying citation numbers as attribute levels. Supporting this decision, a prospective study of journal editors found that citation counts were difficult to predict [33].</p><h3>Scenario</h3><p class="paragraph">The scenario in Box 1 was shown at the start of the survey and was repeated under every choice task.</p><p class="paragraph">The scenario framed the choice tasks and included some attributes of journal choice relevant for decision-making that: 1) could not be measured independently as they overlapped with other attributes, or 2) their importance was either relative across participants or deterministic. For example, the scope and readership of the journal were often mentioned in focus groups as one of the most important attributes. However, as researchers were strongly unwilling to submit to journals outside their scope, we added it to the scenario.</p><figure><figcaption id="" class=""><strong>Box 1: Scenario for the discrete choice experiment</strong><br>Imagine you have written a paper and are now trying to get it published in a journal.<br>Your paper contains original research and is around 4,000 words long with tables and figures. Your paper is relevant in your field and you believe it is good quality.<br>You will only consider journals that fit the scope of your paper and are read by your target audience. You have no previous experience with the journals (good or bad). You do not have any personal or professional relationships with the journal editors or publishers.<br>You are the first author and will make all decisions on behalf of your co-authors.<br><strong>Scenario 1 ending:</strong> Your paper has not yet been submitted to any journal.<br><strong>Scenario 2 ending:</strong> Your paper has been submitted and desk-rejected (rejected without peer reviews) by two journals.</figcaption></figure><p class="paragraph">The two scenario endings were created because in the focus groups some researchers mentioned how they might change behaviour after experiencing some rejections. To test this potential difference in the survey, researchers were randomised to view one or the other scenario ending in a 1:1 ratio.</p><p class="paragraph">Focus group participants mentioned that previous experiences with a journal, good or bad, would strongly influence their choices. To avoid this concern, the scenario stated that the researchers did not have any experience with the journal. Similarly we stated that they did not know the editorial staff, as this also influences researchers’ journal choices.</p><h3>Dominant task</h3><p class="paragraph">An example discrete choice task is shown in Figure 5. The choice tasks were unlabelled as the hypothetical journals were “A” and “B”. This example is the dominant choice task where “Journal A” is clearly the most desirable. It was used to examine whether respondents understood the task. It was shown as the first task to warm up respondents and was not used in the data analysis.</p><h3>Survey of discrete choice tasks</h3><p class="paragraph">The online survey started with a link to the participant information sheet (Supplement S.7) and asked researchers to indicate their consent. Fourteen respondents did not consent. Those who consented were shown the scenario (Box 1) and dominant task (Figure 5). Respondents next answered eight choice tasks. A final task was a repeat task of one of the eight. This was used to assess the stability of the participants’ responses based on the percentage of respondents that gave the same answer as the original task [34]. Differing answers could be due to learning effects or fatigue [34]. The repeat task was not used in the analysis.</p><p class="paragraph">The final section of the survey asked respondents if they found the choice tasks easy or difficult. We also gathered the following information from the respondents: their broad research area, gender, years of experience in research, number of published papers, country, and their perceived publication pressure. Lastly, the respondents could add optional comments. Respondents could skip any question. The complete survey is available from Supplement S.7.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_5-1024x539.png" alt="" data-id="62cb9a20-541f-4ed4-b47a-b155ac7ec676"><figcaption class="wp-element-caption">Figure 5: Example discrete choice task showing the attributes and levels. This is the dominant choice task where Journal A has the levels we assumed most respondents would prefer.</figcaption></figure><p class="paragraph">The NGene (version 13.0) software was used to select 24 pairwise choice tasks based on the D-error to give an efficient fractional design. This D-efficient design was developed using the Modified Federov algorithm to estimate a multinomial logit model. For the final D-efficient design, the weights were selected from the pilot test. The 24 choice tasks were divided into three blocks of eight. Using a fractional design maximised the design’s statistical efficiency, whilst giving a manageable number of choice tasks of ten: eight plus the dominant task and re-test.</p><h2>Statistical methods</h2><p class="paragraph">We used the panel mixed multinomial logit (pMMNL) model and the panel Latent Class Model (pLCM) for the main analysis. We also used the pMMNL model to examine whether preferences systematically differed based on respondents’ characteristics. Results are presented as mean utilities with 95% confidence intervals, and the estimated attribute importance [35]. Subgroup analyses were conducted using the following characteristics: years of experience, gender, number of publications, having a publication target, and the hypothetical paper’s prior rejection. The pLCM was used to capture non-systematic heterogeneity in preferences among respondents, assuming that differences in preferences manifest as discrete groups or latent classes [36]. The ideal number of classes was determined using the Akaike Information Criterion (AIC). A pLCM was used to assess task non-attendance, incorporating a “garbage class” to identify respondents who provided non-informative responses [37]. This approach enabled an evaluation of preference heterogeneity that distinguished between attentive and non-attentive participants.</p><p class="paragraph">Our data collection and analyses were preregistered in a study protocol [38]. The only change from our planned design was that we did not use the pre-notification email for most invites, as it did not appear to increase the response rate.</p><h3>Data and code availability</h3><p class="paragraph">Our R code and data are available on GitHub [39].</p><h3>Sample size</h3><p class="paragraph">Sample size formulae are not available for discrete choice experiments and estimates are often made using rules of thumb or simulations [40, 41]. We faced uncertainty in selecting plausible model parameters, with 1 to 2 parameters per discrete choice attribute and no similar prior studies. Hence our final sample size was based on a pilot. Pilot testing has been recommended to inform sample size calculations for complex interventions [42].</p><p class="paragraph">We analysed the pilot data of 51 respondents to inform the final design. The required sample size based on minimising the D-error was 309. Both the pilot and final design had 24 choice tasks in three blocks of eight. The attributes and levels were the same in the pilot and final design, hence we combined respondents from the pilot and final surveys in our analyses.</p><h3>Sampling frame</h3><p class="paragraph">Our target population was current health and medical researchers. We approached this population by creating a sampling frame of researchers extracted from papers on the PubMed database, which is a widely used search engine that contains the MEDLINE database of published papers in life sciences and biomedical topics [43]. To capture current researchers we restricted the search from the year 2022 onwards. We used the “publication type” search field to exclude non-research papers like obituaries. We only extracted researchers who had an email available. The search was conducted on 11 April 2024.</p><p class="paragraph">The search returned over 140,000 papers, which we randomly re-ordered and iteratively extracted no more than one unique email per paper until we had a sample of 9,000 researchers. Randomly selected researchers from the sampling frame were sent an initial email with reminders one and two weeks later.</p><h2>Additional information</h2><h3>Contributions</h3><p class="paragraph">Conceptualization: NGB, SW, DB, SS, SK and AB.<br>Methodology: NGB, SS and AB.<br>Software: SS and AB.<br>Formal analysis: NGB, SS and AB.<br>Investigation: NGB, SW, DB, SS, SK and AB.<br>Data curation: AB.<br>Visualization: AB.<br>Writing—original draft: AB.<br>Writing—review and editing: NGB, SS, DB, SW, SK.<br>Funding acquisition: DB, SS, SK and AB.</p><h3>Ethics declarations</h3><p class="paragraph">The focus groups and interviews were approved by the Queensland University of Technology Human Research Ethics committee (Date: 18 April 2023, number: LR 2023-6685-13695). The survey was approved by the Queensland University of Technology Human Research Ethics committee (Date: 5 March 2024, number: LR 2024-8188-18148).</p><h3>Funding</h3><p class="paragraph">This work received funding from an internal grant from the Centre for Healthcare Transformation at Queensland University of Technology.</p><h2>References</h2><p class="paragraph">[1] Génova, Gonzalo, Astudillo, Hern´an, and Fraga, Anabel. “The Scientometric Bubble Considered Harmful”. In: Science and Engineering Ethics 22.1 (Feb. 2015), pp. 227–235. doi: 10.1007/s11948-015-9632-6.<br>[2] Schimanski, Lesley A. and Alperin, Juan Pablo. “The evaluation of scholarship in academic promotion and tenure processes: Past, present, and future”. In: F1000Research 7 (Oct. 2018), p. 1605. doi: 10.12688/f1000research.16493.1.<br>[3] Rice, Danielle B et al. “Academic criteria for promotion and tenure in biomedical sciences faculties: cross sectional analysis of international sample of universities”. In: BMJ 369 (2020). doi: 10.1136/bmj.m2081.<br>[4] Dawes, Martin et al. “Sicily statement on evidence-based practice”. In: BMC Medical Education 5.1 (Jan. 2005). doi: 10.1186/1472-6920-5-1.<br>[5] Smaldino, Paul E. and McElreath, Richard. “The natural selection of bad science”. In: Royal Society Open Science 3.9 (Sept. 2016), p. 160384. doi: 10.1098/rsos.160384.<br>[6] Merton, R.K. and Storer, N.W. The Sociology of Science: Theoretical and Empirical Investigations. Phoenix books. University of Chicago Press, 1973. isbn: 9780226520926.<br>[7] Binswanger, Mathias. “Excellence by Nonsense: The Competition for Publications in Modern Science”. In: Opening Science. Springer International Publishing, Dec. 2013, pp. 49–72. isbn: 9783319000268. doi: 10.1007/978-3-319-00026-8_3.<br>[8] Maggio, Lauren A. et al. ““The best home for this paper”: A qualitative study of how authors select where to submit manuscripts”. In: (May 2024). doi: 10.1101/2024.05.14.594165.<br>[9] Anderson, Melissa S. et al. “The Perverse Effects of Competition on Scientists’ Work and Relationships”. In: Science and Engineering Ethics 13.4 (Nov. 2007), pp. 437–461. doi: 10.1007/s11948-007-9042-5.<br>[10] Onstad, David W and Sime, Karen R. “The ethical and social effects of the obsession over Journal Impact Factor”. In: Annals of the Entomological Society of America 117.3 (Mar. 2024). Ed. by Matt Hudson, pp. 160–162. doi: 10.1093/aesa/saae013.<br>[11] Casadevall, Arturo and Fang, Ferric C. “Causes for the Persistence of Impact Factor Mania”. In: mBio 5.2 (2014), 10.1128/mbio.00064–14. doi: 10.1128/mbio.00064-14.<br>[12] Falagas, Matthew E. and Alexiou, Vangelis G. “The top-ten in journal impact factor manipulation”. In: Archivum Immunologiae et Therapiae Experimentalis 56.4 (July 2008), pp. 223–226. doi: 10.1007/s00005-008-0024-5.<br>[13] Pitt, Rachael and Mewburn, Inger. “Academic superheroes? A critical analysis of academic job descriptions”. In: Journal of Higher Education Policy and Management 38.1 (2016), pp. 88–101. doi: 10.1080/1360080X.2015.1126896.<br>[14] Hrynaszkiewicz, Iain et al. “A survey of how biology researchers assess credibility when serving on grant and hiring committees”. In: (Mar. 2024). doi: 10.31222/osf.io/ht836.<br>[15] Chapman, Colin A. et al. “Games academics play and their consequences: how authorship, h-index and journal impact factors are shaping the future of academia”. In: Proceedings of the Royal Society B: Biological Sciences 286.1916 (2019), p. 20192047. doi: 10.1098/rspb.2019.2047.<br>[16] Niles, Meredith T. et al. “Why we publish where we do: Faculty publishing values and their relationship to review, promotion and tenure expectations”. In: PLOS ONE 15.3 (Mar. 2020), pp. 1–15. doi: 10.1371/journal.pone.0228914.<br>[17] Frey, Bruno S., Eichenberger, Reiner, and Frey, Ren´e L. “Editorial Ruminations: Publishing Kyklos”. In: Kyklos 62.2 (Apr. 2009), pp. 151–160. doi: 10.1111/j.1467-6435.2009.00428.x.<br>[18] Eisen, Michael B et al. “Peer review without gatekeeping”. In: eLife 11 (Oct. 2022). doi: 10.7554/elife.83889.<br>[19] Schmid, Sandra L. “Five years post-DORA: promoting best practices for research assessment”. In: Molecular Biology of the Cell 28.22 (Nov. 2017). Ed. by Doug Kellogg, pp. 2941–2944. doi: 10.1091/mbc.e17-08-0534.<br>[20] Ware, Mark. “Peer review in scholarly journals: Perspective of the scholarly community–Results from an international study”. In: Information Services &amp; Use 28.2 (2008), pp. 109–112.<br>[21] Rousseau, Sandra and Rousseau, Ronald. “Interactions between journal attributes and authors’ willingness to wait for editorial decisions”. In: Journal of the American Society for Information Science and Technology 63.6 (Mar. 2012), pp. 1213–1225. doi: 10.1002/asi.22637.<br>[22] Krasnova, Hanna et al. “Publication Trade-Offs for Junior Scholars in IS: Conjoint Analysis of Preferences for Quality, First Authorship, Collaboration, and Time”. In: Proceedings of the International Conference on Information Systems (ICIS). 2014.<br>[23] Salandra, Rossella, Salter, Ammon, and Walker, James T. “Are Academics Willing to Forgo Citations to Publish in High-Status Journals? Examining Preferences for 4* and 4-Rated Journal Publication Among UK Business and Management Academics”. In: British Journal of Management 33.3 (May 2021), pp. 1254–1270. doi: 10.1111/1467-8551.12510.<br>[24] Lemke, Steffen, Mazarakis, Athanasios, and Peters, Isabella. “Conjoint analysis of researchers’ hidden preferences for bibliometrics, altmetrics, and usage metrics”. In: Journal of the Association for Information Science and Technology 72.6 (2021), pp. 777–792. doi: https://doi.org/10.1002/asi.24445.<br>[25] Bohorquez, Natalia Gonzalez et al. “Attribute Development in Health-Related Discrete Choice Experiments: A Systematic Review of Qualitative Methods and Techniques to Inform Quantitative Instruments”. In: Value in Health (June 2024). doi: 10.1016/j.jval.2024.05.014.<br>[26] Bohorquez, Natalia Gonzalez et al. “Enhancing Health Preferences Research: Guidelines for Qualitative Attribute Development in Stated Preference Studies”. In: OSF (July 2024). url: https://osf.io/g9jbt.<br>[27] Leighton, J.P. Using Think-Aloud Interviews and Cognitive Labs in Educational Research.<br>Understanding Qualitative Research. Oxford University Press, 2017. isbn: 9780199372911.<br>[28] Althouse, Benjamin M. et al. “Differences in impact factor across fields and over time”. In: Journal of the American Society for Information Science and Technology 60.1 (Dec. 2008), pp. 27–34. issn: 1532-2890. doi: 10.1002/asi.20936. url: http://dx.doi.org/10.1002/asi.20936.<br>[29] Publons. 2018 Global state of peer review series. 2018. doi: 10.14322/publons.GSPR2018. url: https://publons.com/static/Publons-Global-State-Of-Peer-Review-2018.pdf.<br>[30] Greenberg, S. A. “How citation distortions create unfounded authority: analysis of a citation network”. In: BMJ 339.jul20 3 (July 2009), b2680. doi: 10.1136/bmj.b2680.<br>[31] Koren, Gideon. “Bias Against Negative Studies in Newspaper Reports of Medical Research”. In: JAMA: The Journal of the American Medical Association 266.13 (Oct. 1991), p. 1824. doi: 10.1001/jama.1991.03470130104037.<br>[32] Hensher, David A. “Hypothetical bias, choice experiments and willingness to pay”. In: Transportation Research Part B: Methodological 44.6 (July 2010), pp. 735–752. doi: 10.1016/j.trb.2009.12.012.<br>[33] Schroter, Sara et al. “Evaluation of editors’ abilities to predict the citation potential of research manuscripts submitted to The BMJ: a cohort study”. In: BMJ 379 (2022). doi: 10.1136/bmj-2022-073880.<br>[34] Ozdemir, Semra et al. “Who pays attention in stated-choice surveys?” In:¨ Health Economics 19.1 (Mar. 2009), pp. 111–118. doi: 10.1002/hec.1452.<br>[35] Gonzalez, Juan Marcos. “A Guide to Measuring and Interpreting Attribute Importance”. In: The Patient - Patient-Centered Outcomes Research 12.3 (Mar. 2019), pp. 287–295. doi: 10.1007/s40271-019-00360-3.<br>[36] Greene, William H. and Hensher, David A. “A latent class model for discrete choice analysis: contrasts with mixed logit”. In: Transportation Research Part B: Methodological 37.8 (2003), pp. 681–698. doi: https://doi.org/10.1016/S0191-2615(02)00046-2.<br>[37] Gonzalez, Juan Marcos, Johnson, F. Reed, and Finkelstein, Eric. “To pool or not to pool: Accounting for task non-attendance in subgroup analysis”. In: Journal of Choice Modelling 51 (2024), p. 100487. doi: https://doi.org/10.1016/j.jocm.2024.100487.<br>[38] Barnett, Adrian G et al. Study protocol: A discrete choice experiment to examine researchers’ publication preferences: an international cross-sectional survey. Mar. 2024. doi: 10.17605/OSF.IO/P9GUJ. url: https://doi.org/10.17605/OSF.IO/P9GUJ.<br>[39] Barnett, Adrian G. Code and data for a discrete choice experiment of authors’ preferences. July 2024. doi: 10.5281/zenodo.12814359. url:https://github.com/agbarnett/publication_preferences.<br>[40] Lancsar, Emily and Louviere, Jordan. “Conducting Discrete Choice Experiments to Inform Healthcare Decision Making: A User’s Guide”. In: PharmacoEconomics 26.8 (2008), pp. 661–677. doi: 10.2165/00019053-200826080-00004.<br>[41] Reed Johnson, F. et al. “Constructing Experimental Designs for Discrete-Choice Experiments: Report of the ISPOR Conjoint Analysis Experimental Design Good Research Practices Task Force”. In: Value in Health 16.1 (Jan. 2013), pp. 3–13. doi: 10.1016/j.jval.2012.08.2223.<br>[42] Lancaster, GA et al. “Trials in primary care: statistical issues in the design, conduct and evaluation of complex interventions”. In: Statistical Methods in Medical Research 19.4 (May 2010), pp. 349–377. doi: 10.1177/0962280209359883.<br>[43] Sayers, Eric W et al. “Database resources of the national center for biotechnology information”. In: Nucleic Acids Research 50.D1 (Dec. 2021), pp. D20–D26. doi: 10.1093/nar/gkab1112. url: http://dx.doi.org/10.1093/nar/gkab1112.</p><h2>Supplementary material</h2><h3>S.1 Survey responses over time</h3><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_s1-1024x855.png" alt="" data-id="35c10b14-d738-47b8-ba4f-1c921fa2ffc9"><figcaption class="wp-element-caption">Figure S.1: Cumulative number of survey responses over time for the pilot and final design.</figcaption></figure><h3>S.2 Classification tree predicting survey response</h3><p class="paragraph">Table S.1: Results of the classification tree using email domain.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_s2-1024x344.png" alt="" data-id="2cc1239b-562d-41f1-993c-d10febdb1b64"><figcaption id="2cc1239b-562d-41f1-993c-d10febdb1b64" class="decoration"></figcaption></figure><p class="paragraph">We used a classification tree to predict survey response (yes/no) based on the researchers’ email domain (a proxy for country, e.g., au = Australia), and whether the researcher’s affiliation mentioned the words “Hospital”, “Dentist*” or “University”. The classification tree had three leaves with a cross-validated error of 0.990 with a standard error of 0.034. The tree only used the email domain, but found a relatively large difference in response proportions. We present the results as a table instead of a plotted tree as the number of email domains makes the plot cluttered.</p><h3>S.3 Item-missing data</h3><p class="paragraph">The plot below shows item missing data by question number. The missing data patterns are clustered by similarity. The question numbers are presented in order. There is evidence of survey fatigue as the percent missing increases from left-to-right.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_s3-1024x654.png" alt="" data-id="a6751daa-68da-4b49-8298-c42cc1d1bb9e"><figcaption class="wp-element-caption">Figure S.2: Item missing data for the 616 survey responses. The column headings show the question number and percent missing. The panel on the right shows the questions for each question number.</figcaption></figure><h3>S.4 Respondents’ countries</h3><p class="paragraph">Table S.2: Number and percent of responses by country. There were 63 countries in total.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_s4-1017x1024.png" alt="" data-id="05ae2ae9-fc81-4069-8b8b-6b173a95344a"><figcaption id="05ae2ae9-fc81-4069-8b8b-6b173a95344a" class="decoration"></figcaption></figure><h3>S.5 Attribute interactions</h3><p class="paragraph">Table S.3: Utility estimates and 95% confidence intervals for the planned interactions between attributes. The interactions are plotted in Figure 3. This table shows only the interaction terms and not the main effects. These results help judge the null hypothesis of whether there was no interaction.</p><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/12_s5-1024x343.png" alt="" data-id="b2c9d6e8-5814-4595-b3f1-a826050eacbe"><figcaption id="b2c9d6e8-5814-4595-b3f1-a826050eacbe" class="decoration"></figcaption></figure><h3>S.6 Details on the discrete choice experiment design</h3><p class="paragraph">This additional file includes details on the literature review, focus groups and interviews, and thinking aloud exercise. It is available here: https://osf.io/gjch7.</p><h3>S.7 Participant information sheet and survey questions</h3><p class="paragraph">The online version of the participant information sheet is available here https://osf.io/p9guj/wiki/home/.</p><p class="paragraph">A PDF version of the survey is available here https://osf.io/j7mce. The survey was delivered online using Qualtrics.</p><p class="paragraph">The survey questions differed by two questions between the pilot and final survey as we altered the question that aimed to examine researcher’s publishing expectations. This is because for the original question – “My department’s or research group’s expectations with respect to publishing are reasonable” – 81% responded as “Agree” or “Strongly agree” creating limited variance between respondents. Hence in the main survey we asked researchers if they had an annual publication target and what it was.</p>
        `,
    },
  },
  14: {
    authors: [
      {
        name: "Susana Oliveira Henriques",
        affiliations: [
          "Research on Research Institute (RoRI) Centre for Science and Technology Studies (CWTS)",
          "Leiden University",
          "Leiden, the Netherlands",
          "Scientific Research Department",
          "Azerbaijan University of Architecture and Construction",
          "Baku, Azerbaijan",
        ],
        email: "s.oliveira@cwts.leidenuniv.nl",
        orcid: "0000-0002-0947-5083",
        ror: null,
      },
      {
        name: "Narmin Rzayeva",
        affiliations: [
          "Research on Research Institute (RoRI) Information School",
          "University of Sheffield",
          "Sheffield, UK",
        ],
        email: "n.rzayeva@cwts.leidenuniv.nl",
        orcid: "0000-0003-0397-5412",
        ror: "https://ror.org/02shm3a27",
      },
      {
        name: "Stephen Pinfield",
        affiliations: [
          "Research on Research Institute (RoRI) Centre for Science and Technology Studies (CWTS)",
          "Leiden University",
          "Leiden, the Netherlands",
        ],
        email: "s.pinfield@sheffield.ac.uk",
        orcid: "0000-0003-4696-764X",
        ror: "https://ror.org/05krs5044",
      },
      {
        name: "Ludo Waltman",
        affiliations: ["Leiden University"],
        email: "waltmanlr@cwts.leidenuniv.nl",
        orcid: "0000-0001-8249-1752",
        ror: "https://ror.org/027bh9e22",
      },
    ],
    reviews: [
      {
        name: "Ross Mounce",
        orcid: "0000-0002-3520-2046",
        review: `<p class="paragraph">This manuscript examines preprint review services and their role in the scholarly communications ecosystem. &nbsp;It seems quite thorough to me. In Table 1 they list many peer-review services that I was unaware of e.g. SciRate and Sinai Immunology Review Project.</p><p class="paragraph">To help elicit critical &amp; confirmatory responses for this peer review report I am trialling Elsevier’s suggested “structured peer review” core questions, and treating this manuscript as a research article.</p><p class="paragraph">&nbsp;</p><p class="paragraph"><strong>Introduction</strong></p><p class="paragraph"><strong>1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is the background and literature section up to date and appropriate for the topic?</strong></p><p class="paragraph">Yes.</p><p class="paragraph"><strong>2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are the primary (and secondary) objectives clearly stated at the end of the introduction?</strong></p><p class="paragraph">No. Instead the authors have chosen to put the two research questions on page 6 in the methods section. I wonder if they ought to be moved into the introduction – the research questions are not methods in themselves. Might it be better to state the research questions first and then detail the methods one uses to address those questions afterwards? [as Elsevier’s structured template seems implicitly to prefer]</p><p class="paragraph">&nbsp;</p><p class="paragraph"><strong>Methods</strong></p><p class="paragraph"><strong>3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are the study methods (including theory/applicability/modelling) reported in sufficient detail to allow for their replicability or reproducibility?</strong></p><p class="paragraph">I note with approval that the version number of the software they used (ATLAS.ti) was given.</p><p class="paragraph">I note with approval that the underlying data is publicly archived under CC BY at figshare.</p><p class="paragraph">The Atlas.ti report data spreadsheet could do with some small improvement – the column headers are little cryptic e.g. “Nº&nbsp; ST “ and “ST” which I eventually deduced was Number of Schools of Thought and Schools of Thought (?) &nbsp;&nbsp;</p><p class="paragraph">Is there a rawer form of the data that could be deposited with which to evidence the work done? The Atlas.ti report spreadsheet seemed like it was downstream output data from Atlas.ti. What was the rawer input data entered into Atlas.ti? Can this be archived somewhere in case researchers want to reanalyse it using other tools and methods.</p><p class="paragraph">I note with disapproval that Atlas.ti is proprietary software which may hinder the reproducibility of this work. Nonetheless I acknowledge that Atlas.ti usage is somewhat ‘accepted’ in social sciences despite this issue.</p><p class="paragraph">I think the qualitative text analysis is a little vague and/or under-described: “Using ATLAS.ti Windows (version 23.0.8.0), we carried out a qualitative analysis of text from the relevant sites, assigning codes covering what they do and why they have chosen to do it that way.” That’s not enough detail. Perhaps an example or two could be given? Was inter-rater reliability performed when ‘assigning codes’ ? How do we know the ‘codes’ were assigned accurately?</p><p class="paragraph"><strong>4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are statistical analyses, controls, sampling mechanism, and statistical reporting (e.g., P-values, CIs, effect sizes) appropriate and well described?</strong></p><p class="paragraph">This is a descriptive study (and that’s fine) so there aren’t really any statistics on show here other than simple ‘counts’ (of Schools of Thought) in this manuscript. There are probably some statistical processes going on within the proprietary qualitative analysis of text done in ATLAS.ti but it is under described and so hard for me to evaluate.</p><p class="paragraph"><strong>&nbsp;</strong></p><p class="paragraph"><strong>Results</strong></p><p class="paragraph"><strong>5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is the results presentation, including the number of tables and figures, appropriate to best present the study findings?</strong></p><p class="paragraph">Yes. However, I think a canonical URL to each service should be given.&nbsp; A URL is very useful for disambiguation, to confirm e.g. that the authors mean this Hypothesis (www.hypothes.is) and NOT this Hypothesis (<a href="http://www.hyp.io/" rel="" target="blank">www.hyp.io</a>). I know exactly which Hypothesis is the one the authors are referring to but we cannot assume all readers are experts 😊</p><p class="paragraph">Optional suggestion: I wonder if the authors couldn’t present the table data in a slightly more visual and/or compact way? It’s not very visually appealing in its current state. Purely as an optional suggestion, to make the table more compact one could recode the answers given in one or more of the columns 2, 3 and 4 in the table e.g. "all disciplines =&nbsp;⬤ , biomedical and life sciences =&nbsp;▲, social sciences =&nbsp;&nbsp;‡&nbsp; , engineering and technology&nbsp;=&nbsp;† ". I note this would give more space in the table to print the URLs for each service that both reviewers have requested.</p><p class="paragraph"><strong>Service name</strong></p><p class="paragraph"><strong>Developed by</strong></p><p class="paragraph"><strong>Scientific disciplines</strong></p><p class="paragraph"><strong>Types of outputs</strong></p><p class="paragraph">Episciences</p><p class="paragraph">Other</p><p class="paragraph">⬤</p><p class="paragraph">blah blah blah</p><p class="paragraph">Faculty Opinions</p><p class="paragraph">Individual researcher</p><p class="paragraph">▲</p><p class="paragraph">blah blah blah</p><p class="paragraph">Red Team Market</p><p class="paragraph">Individual researcher</p><p class="paragraph">‡</p><p class="paragraph">blah blah blah</p><p class="paragraph">The "Types of outputs" column might even lend themselves to mini-colour-pictograms (?) which could be more concise&nbsp;<em>and</em>&nbsp;more visually appealing? A table just of text, might be scientifically 'correct' but it is incredibly dull for readers, in my opinion.</p><p class="paragraph"><strong>6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Are additional sub-analyses or statistical measures needed (e.g., reporting of CIs, effect sizes, sensitivity analyses)?</strong></p><p class="paragraph">No / Not applicable.</p><p class="paragraph"><strong>&nbsp;</strong></p><p class="paragraph"><strong>Discussion</strong></p><p class="paragraph"><strong>7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Is the interpretation of results and study conclusions supported by the data and the study design?</strong></p><p class="paragraph">Yes.</p><p class="paragraph"><strong>8. Have the authors clearly emphasized the limitations of their study/theory/methods/argument?</strong></p><p class="paragraph">No. Perhaps a discussion of the linguistic/comprehension bias of the authors might be appropriate for this manuscript. What if there are ‘local’ or regional Chinese, Japanese, Indonesian or Arabic language preprint review services out there? Would this authorship team really be able to find them?</p><p class="paragraph">&nbsp;</p><p class="paragraph"><strong>Additional points:</strong></p><p class="paragraph">·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Perhaps the points made in this manuscript about financial sustainability (p24) are a little too pessimistic. I get it, there is merit to this argument, but there is also some significant investment going on there if you know where to look. Perhaps it might be worth citing some recent investments e.g. Gates -&gt; PREreview (2024) <a href="https://content.prereview.org/prereview-welcomes-funding/" rel="" target="blank">https://content.prereview.org/prereview-welcomes-funding/</a>&nbsp; and Arcadia’s $4 million USD to COAR for the Notify Project which supports a range of preprint review communities including Peer Community In, Episciences, PREreview and <a href="https://library.harvard.edu/about/news/2024-04-23/harvard-library-launching-harvard-open-journals-program" rel="" target="blank">Harvard Library</a>.&nbsp; (source: <a href="https://coar-repositories.org/news-updates/coar-welcomes-significant-funding-for-the-notify-project/" rel="" target="blank">https://coar-repositories.org/news-updates/coar-welcomes-significant-funding-for-the-notify-project/</a> )</p><p class="paragraph">&nbsp;</p><p class="paragraph">·&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Although I note they are mentioned, I think more needs to be written about the similarity and overlap between ‘overlay journals’ and preprint review services. Are these arguably not just two different terms for kinda the same thing? If you have Peer Community In which has it’s overlay component in the form of the Peer Community Journal, why not mention other overlay journals like Discrete Analysis and The Open Journal of Astrophysics.&nbsp;&nbsp; I think Peer Community In (&amp; it’s PCJ) is the go-to example of the thin-ness of the line the separates (or doesn’t!) overlay journals and preprint review services. Some more exposition on this would be useful.</p>`,
      },
    ],
    published: "Published on Aug 02, 2024",
    doi: "10.31235/osf.io/8c6xm",
    title:
      "Preprint review services: Disrupting the scholarly communication landscape ",
    sections: {
      Abstract: splitTextIntoParagraphs(`
        Preprinting has gained considerable momentum, and in some fields it has turned into a well-established way to share new scientific findings. The possibility to organise quality control and peer review for preprints is also increasingly highlighted, leading to the development of preprint review services. We report a descriptive study of preprint review services with the aim of developing a systematic understanding of the main characteristics of these services, evaluating how they manage preprint review, and positioning them in the broader scholarly communication landscape. Our study shows that preprint review services have the potential to turn peer review into a more transparent and rewarding experience and to improve publishing and peer review workflows. We are witnessing the growth of a mixed system in which preprint servers, preprint review services and journals operate mostly in complementary ways. In the longer term, however, preprint review services may disrupt the scholarly communication landscape in a more radical way.
        `),
      rest: `
       <h2>Introduction</h2><p class="paragraph">Preprints are well-established in some fields but not in all. Preprint servers make scientific work available rapidly (albeit usually in a form prior to peer review) and also openly, enabling scientific work to be accessed in a timely way not only by scientists but also by policymakers, journalists and others. The COVID-19 pandemic led to an unprecedented rise in the use of preprints by the biomedical research community. Preprints were an essential part of the communication of research about COVID-19, useful in particular as a way of accelerating communication of research results. Another potential benefit of preprints is that they allow authors to receive and incorporate feedback from the wider community prior to journal publication (Fraser et al., 2021). Nevertheless, due to the non-peer-reviewed nature of preprints, concerns about the lack of quality assurance remain (Blatch-Jones et al., 2023; Ni &amp; Waltman, 2023). Partly in response to these concerns, there are a growing number of services that facilitate evaluative peer feedback (e.g., comments, recommendations, reviews) on preprints. Some of these services may even be seen as alternatives to journal-based peer review, pointing towards possible future approaches to overcoming weaknesses of the journal-based peer review system.</p>
        <p class="paragraph">In Figure 1, we summarise some of the potential benefits of services facilitating the review of preprints, as they are commonly presented in sources such as the websites of preprint review services, blog posts and scientific articles. In this paper, we will explore how preprint review services are trying to achieve these benefits and how they may add value to the scholarly communication system, pointing out to a cultural shift in peer review (Avissar-Whiting et al., 2023).&nbsp;</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/10/14_1-1024x553.png" alt="" data-id="41dba906-f1a8-4abd-b1ff-d72ef9f0fcb2" contenteditable="false"><figcaption class="wp-element-caption">Figure 1. Potential benefits of services facilitating the review of preprints. *By users, we mean scientists, policymakers, journalists, and citizens in general.</figcaption></figure><p class="paragraph">Our aim is to develop a systematic understanding of the main characteristics of preprint review services in order to evaluate how these services manage preprint review and to position these services in the wider scholarly communication and peer review environment, including journal publishing. By a preprint review service, we mean a journal-independent peer review service for articles posted on a preprint server, where the peer review process is publicly visible.&nbsp;We report here a descriptive study based on a qualitative analysis of data available in the ReimagineReview registry and other online sources, such as the websites of preprint review services, blog posts and scientific articles. This paper is complementary to a study recently reported by Lutz et al., (2023) of an ongoing survey of Alternative Publishing Platforms. By providing additional information on preprint review services, we are contributing to the work of gaining a better understanding of the landscape formed by these platforms and how they can be placed in the open scholarly communication ecosystem.&nbsp;</p>
        <p class="paragraph">We have opted not to use the term 'peer' when discussing platforms for the review of preprints, adopting instead the designation 'preprint review services', distinguishing them from 'journal-based peer review'. This better reflects the diversity of review approaches facilitated by preprint review services — from the spontaneous posting of feedback by individuals to community-driven review platforms — and considering that feedback on preprints is not bound by the conventions often associated with journal-based peer review — in fact, a variety of forms of preprint feedback are evident, ranging from minimal ‘ratings’ and informal approaches to in-depth formal peer review (Avissar-Whiting et al., 2023).</p>
        <p class="paragraph">In our work, we use as an analytical lens a framework recently proposed by some of us (Waltman et al., 2023) in which a distinction is made between four schools of thought on peer review. The four schools, referred to as the Quality &amp; Reproducibility school, the Democracy &amp; Transparency school, the Equity &amp; Inclusion school, and the Efficiency &amp; Incentives school, provide a framework for organising innovations in the peer review space, grouping them in terms of how they perceive problems of peer review and develop solutions in response to these problems. We discuss these in detail in the analysis below. We suggest that the four schools framework offers a useful way to better understand the complexity of innovation in preprint review. Based on our analysis, we reflect on possible future models of peer review and scholarly communication that may arise from the experimentation we are currently witnessing.</p>
        <p class="paragraph">Following a description of our methods in Section 2, Section 3 provides an overview of services facilitating the review of preprints. Section 4 discusses the contribution of preprint review services in addressing the various issues raised by the four schools of thought on peer review. Section 5 explores how preprint review services are managing apparent tensions between different aims, namely the ones resulting from the anonymisation of reviewers and authors. Section 6 discusses how preprint review services fit into the publishing landscape and how they may influence the development of this landscape, and Section 7 presents some concluding remarks.</p>
        <h2>Methods</h2><p class="paragraph">We carried out a descriptive study using qualitative analysis. We used ReimagineReview (RR) as a starting point to identify services and initiatives to facilitate the review of preprints. RR is a registry of services and experiments around peer review of scientific outputs created by ASAPbio (n.d.). For additional information, we supplemented the directory data with desk research using online sources such as the websites of preprint review services, blog posts and scientific articles. Using ATLAS.ti Windows (version 23.0.8.0), we carried out a qualitative analysis of text from the relevant sites, assigning codes covering what they do and why they have chosen to do it that way. Where there was ambiguity in project descriptions, we contacted the service managers to clarify and confirm some details.&nbsp;</p>
        <p class="paragraph">As of April 2022, the RR directory listed 35 services for preprint review. For our study, we considered only the services where the process of reviewing preprints was visible. ReimagineReview also includes services that do not publish reviews. We did not consider these services. We finalised the selection and analysis of services for our study in November 2022. The web sites of two preprint review services included in our initial set became unavailable after November 2022. These are the CrowdPeer and the Red Team Marked. As CrowdPeer became unavailable before we finished the analysis, we could not check and describe its main characteristics, as we did for the other services, including the Red Team Marked. Therefore, we decided not to include CrowdPeer in our final set of 23 services for review of preprints. Nevertheless, based on our initial analysis and the information available in the RR directory, we have included some comments on CrowdPeer as an illustration of possible future models of peer review and scholarly communication, where relevant. Red Team Marked was taken out of operation later and so we were able to include it in our analysis. On the other hand, some services have evolved and for these services our analyses, as a snapshot, may not be fully up-to-date. This is the case for Rapid Reviews: COVID-19, which has expanded beyond COVID-19. And so, Rapid Reviews: COVID-19 is now Rapid Reviews\Infectious Diseases, an open-access overlay journal that accelerates peer review of important infectious disease-related research preprints (RR\C19, n.d.).</p>
        <p class="paragraph">To provide an overview of the 23 services included in our final set, we first described their main characteristics according to six categories: 1) project name, 2) developed by, 3) scientific disciplines, 4) goals and intentions, 5) peer review approach, and 6) approach to transparency. Under “developed by”, we used three categories: publisher, university or individual researcher or group of researchers. When detailed information was not available online or was available but not sufficient to support our classification, we contacted the service managers, asking them to tell us how they would describe who the project is led by. We used the “other” category for those who did not answer us and where ambiguity remained.</p>
        <p class="paragraph">For the definition of scientific disciplines, we grouped all disciplines identified among our set into five categories considering the usual organisation of the scholarly communication landscape: Physical Sciences, Biomedical and Life Sciences, Engineering and Technology, Social Sciences, and Humanities. We considered the All Disciplines category for services hosting any scientific discipline or having a multidisciplinary approach without a clear definition of the disciplines or sub-disciplines included.</p>
        <p class="paragraph">To discuss how preprint review services fit into the publishing landscape and how they identify possible future models of peer review and scholarly communication, we then analysed the stated goals, review approach and transparency, using previous studies as a baseline to discuss and validate our findings, addressing the following questions:</p>
        <ul><li data-listnumber=""><p class="paragraph">How do these services differ from, and what do they have in common with traditional peer review? How are they linked with the four peer review schools of thought recently proposed by some of us (Waltman et al., 2023), and how do they manage connections between schools when adopting ideas from multiple schools?&nbsp;</p>
        </li></ul><ul><li data-listnumber=""><p class="paragraph">How can preprint review supplement journal-based peer review or offer an alternative for journal-based peer review?</p>
        </li></ul><p class="paragraph">Our use of the four schools of thought framework enabled us to explore how innovation and experimentation promoted by preprint review services can address the journal-based peer review challenges identified by Waltman et al. (2023) and provide possible solutions and future directions. We assessed the contribution of preprint review services against the key issues identified in the four schools of thought framework, considering the stated goals and intentions of each service, their approach to peer review, and their commitment to openness and transparency. As Waltman et al. (2023) state, these schools have different views on the key problems of the peer review system and the innovations needed to address these problems. While the schools may at times complement each other, the differences in their views may also give rise to tensions. Based on this idea, we aim to understand how preprint review services are able to manage possible tensions when bringing together the key issues of different schools.</p>
        <p class="paragraph">Finally, we identified possible future models of peer review and scholarly communication that may arise from the experimentation we are currently witnessing.</p>
        <ul><li data-listnumber=""><p class="paragraph">Overview of preprint review services</p>
        </li></ul><p class="paragraph">Our final set, presented in Table 1, consists of 23 services for review of preprints - some developed by newly-established groups, others by pre-existing organisations, such as publishers or universities. Some of them are focused only on reviewing preprints (7 services), others review preprints and journal articles (14) or privately shared manuscripts (5) or other types of scholarly outputs (11). The most represented category of developers is individual researcher or group of researchers (15 services), followed by others (5), publishers (2) and universities (1). The most represented scientific disciplines are all disciplines (12 services) and biomedical and life sciences (8), followed by physical sciences (3), social sciences (3), engineering and technology (2), and humanities (1).</p>
        <figure><figcaption id="" class=""><strong>Service nameDeveloped by&nbsp;Scientific disciplinesTypes of outputs</strong>Crowd preprint reviewIndividual researcher or group of researchersBiomedical and Life SciencesPreprintsEpisciencesOtherAll disciplinesPreprintsJournal articlesOther scholarly outputsFaculty OpinionsIndividual researcher or group of researchersBiomedical and Life SciencesJournal articlesOther scholarly outputsPreprintsHypothesisIndividual researcher or group of researchersAll disciplinesJournal articlesOther scholarly outputsPreprintsPrivately shared manuscriptsInteractive Public Peer ReviewPublisherAll disciplines&nbsp;Journal articlesPreprintsPeer Community inIndividual researcher or group of researchersAll disciplines&nbsp;PreprintsPrivately shared manuscriptsPeer LibraryIndividual researcher or group of researchersAll disciplinesJournal articlesOther scholarly outputsPreprintsPrivately shared manuscriptsPeeriodicalsOtherAll disciplinesJournal articlesOther scholarly outputsPreprintsPeerRefIndividual researcher or group of researchersAll disciplinesPreprintsPlauditIndividual researcher or group of researchersAll disciplinesJournal articlesOther scholarly outputsPreprintspreLightsIndividual researcher or group of researchersBiomedical and Life SciencesPreprintsPREreview&nbsp;Individual researcher or group of researchers&nbsp;All disciplinesPreprintsPubPeerOtherAll disciplinesJournal articlesPreprintsQeiosIndividual researcher or group of researchersAll disciplinesJournal articlesOther scholarly outputsPreprintsRapid Reviews: COVID-19Publisher&nbsp;Biomedical and Life Sciences Social SciencesJournal articlesPreprintsRed Team MarketIndividual researcher or group of researchersSocial SciencesJournal articlesOther scholarly outputsPreprintsPrivately shared manuscriptsResearchers.OneIndividual researcher or group of researchersAll disciplinesOther scholarly outputsPreprintsPrivately shared manuscriptsReview CommonsIndividual researcher or group of researchersBiomedical and Life SciencesPreprintsScibaseOtherPhysical Sciences;&nbsp;Biomedical and Life SciencesJournal articlesOther scholarly outputsPreprintsScience Open Reviewed&nbsp;UniversityBiomedical and Life SciencesEngineering and Technology Social Sciences&nbsp;HumanitiesJournal articlesPreprintsSciPostOtherPhysical Sciences&nbsp;Journal articlesOther scholarly outputsPreprintsSciRateIndividual researcher or group of researchersPhysical Sciences&nbsp;Engineering and TechnologyPreprintsSinai Immunology Review ProjectIndividual researcher or group of researchersBiomedical and Life Sciences&nbsp;Preprints</figcaption></figure><p class="paragraph">Table 1: Overview of the services for review of preprints</p>
        <p class="paragraph">Table 2 lists the different approaches to peer review transparency and openness taken by the preprint review services. Offering alternative publication options or alternative approaches to quality assessment compared with the traditional journal-based model is a common goal for all services. One of the key motivations in encouraging review of preprints is to enable authors to benefit from having their work reviewed and improved before submission to a journal. This is often seen as having the potential benefit of reducing rounds of journal re-review. Some services also often give credit to reviewers as a way of incentivising participation. Review of preprints is often explicitly said to address the need for quality control of preprints, reinforcing confidence in the use of preprints and making the evaluation of scientific work more efficient and more open and transparent. Transparency is an evident priority in services that implement one or a number of different aspects of open peer review identified by Ross-Hellauer (2017) including “open reports” (where reviewer reports and author responses are published), “open identities” (where the identities of authors and reviewers are known to each other), “open interaction” (direct unmediated exchange between authors and reviewers is enabled), “open participation” (where reviewers can self-nominate and add comments without being invited to do so), amongst others. In some cases, like in Review Commons, transparency applies only to review reports and not the reviewer's identity. In others, like in SciPost and Peer Community In, reviewers have the option to withhold their identity, becoming visible only to editors or editorial staff. There are two services, Peer Community In and Hypothesis, that support double-anonymous peer review, in which not only reviewers but also authors can withhold their identity.</p>
        <p class="paragraph">The services promote new communication patterns regarding quality assessment and reviewer selection (Table 2). Public post-publication reviews, recommendations, comments, free-form commenting or quantitative scores are among the different assessment approaches we identified. When considering the seven services that are</p>
        <p class="paragraph">restricted to reviewing preprints, the most frequent quality assessment approach is the structured review form (where reviews are input using standardised structured headings). This approach is used by 4 services: PREreview, PeerRef, Review Commons and Sinai Immunology Review Project. In respect of the reviewer’s selection, apart from 2 services (Peeriodicals and Peer Community In), selection of reviewers does not depend on the editor’s decision only. We identified 7 services that allow reviewers to be selected by an editor, the editorial staff, or the community and 2 services where reviewers are selected by an automated tool. There are 12 services where reviewers can be self-nominated. Although the use of an automated tool constitutes an innovative approach to the selection of reviewers, it is not limited to preprint review services. When compared to journal-based peer review, the self-nomination of reviewers is an important innovative aspect of preprint review services, enabling reviewers to get involved in an ostensibly more participative and interactive review process. Self-nomination means that basically anyone can&nbsp;serve as a reviewer on a platform, without intervention from an editor, editorial staff or anyone else. In most cases,&nbsp;reviewers&nbsp;just have to be registered on the platform to make a comment or recommendation, or to endorse a preprint based on a pick and choose format (e.g., the Plaudit endorsement tool).</p>
        <p class="paragraph">Some initiatives offer services that&nbsp;go beyond the review process itself.&nbsp;PreLights and PREreview, for instance, invest in reviewer training for early-career researchers and in other initiatives focused on promoting and recognising reviewers.&nbsp;&nbsp;Prelights provides assessments of reviewers (peer review of peer reviews), or recognition for reviewers’ contributions. PREreview organises training workshops centred on issues of equity, diversity, and inclusion.&nbsp;&nbsp;Another focus of some services is reproducibility, a key issue relating to the quality of scientific outputs. The aim of overcoming or mitigating the lack of quality control associated with preprints and of addressing issues of reproducibility is visible in the policies and procedures of some of the services. This is the case for Queios, Scibase and Peer Community In, which request authors to make code and data openly available and then ask reviewers to consider the code and data in their review.</p>
        <figure><figcaption id="" class=""><strong>&nbsp;Service&nbsp;Reviewer selected by&nbsp;Peer review format&nbsp;Reviewer identity&nbsp;&nbsp;Reviewer identity known to&nbsp;Competing interests&nbsp;Transparency</strong>Crowd preprint reviewSelf-nominatedAnnotationsNon-anonymousEditor or editorial staffNot includedOpen identitiesOpen interactionOpen reportsSingle anonymousEpisciencesEditor, editorial staffFree-form commentingQuantitative scoresStructured review formAnonymousNon-anonymousEditor or editorial staff or publicCheckedOpen interactionSingle anonymousOpen identitiesOpen reportsFaculty OpinionsEditor, editorial staff or communityAnnotationsFree-form commentingQuantitative scoresStructured review formNon-anonymousPublicCheckedOpen identitiesOpen reportsHypothesisSelf-nominatedSocial annotationFree-form commentingStructured review formAnonymousNon-anonymousEditorial or editorial staffNot includedDouble anonymousOpen identitiesOpen interactionOpen participationOpen reportsSingle anonymousInteractive Public Peer ReviewEditor, editorial staff or communityStructured review formFree-form commentingAnonymousNon-anonymousEditor or editorial staffCheckedOpen interactionOpen participationOpen reportsPeer Community inEditorFree-form commentingAnonymousNon-anonymousEditor or editorial staffCheckedDouble anonymousOpen identitiesOpen reportsSingle anonymousPeer LibrarySelf-nominatedAnnotationsNon-anonymousPublicNot includeOpen identitiesOpen interactionOpen participationOpen reportsPeeriodicalsEditorFree-form commentingStructured review formAnonymousNon-anonymousn.a.*n.a.*Open interactionPeeRefAutomated toolStructured review formNon-anonymousPublicDisplayedOpen identitiesOpen interactionOpen reportPlauditSelf-nominatedAnnotationsQuantitative scoresStructured review formNon-anonymousPublicNot includedOpen identitiesOpen interactionOpen participationOpen reportspreLightsSelf-nominatedSummaryReasons for selectionSignificance and relevanceFree-form commentingNon-anonymousPublicNot includeOpen identitiesPREreviewSelf-nominatedFree-form commentingStructured review formScoreAnonymousEditor or editorial staffDisplayedOpen identitiesOpen interactionOpen reportsSingle anonymousPubPeerSelf-nominatedFree-form commentingAnonymousNon-anonymousNonePublicNot includedOpen identitiesOpen interactionOpen participationOpen reportsSingle anonymousQeiosAutomated tool or communityStructured review formQuantitative scoresFree-form commentingNon-anonymousPublicDisplayedOpen identitiesOpen interactionOpen participationOpen reportsRapid Reviews: COVID-19Editor, editorial staff or communityStructured review formStrength of evidence scaleAnonymousNon-anonymousEditor or editorial staffCheckedOpen identitiesOpen reportsRed Team MarkedEditor, editorial staff or communityAnnotationsFree-form commentingStructured review formAnonymousNon-anonymousEditor or editorial staffNot includedOpen identitiesOpen interactionOpen reportsSingle anonymousResearchers.OneSelf-nominatedFree-form commentingAnonymousNon-anonymousEditor or editorial staffNot includedOpen interactionOpen participationOpen reportsReview CommonsEditor, editorial staff or communityStructured review formAnonymousNon-anonymousEditor or editorial staffCheckedOpen reportsScibaseSelf-nominatedFree-form commentingStructured review formScale or ratingAnonymousNon-anonymousPublicNot includedOpen participationScience Open ReviewedAuthors, editor, editorial staff or communityReview reportFree-form commentingAnonymousNon-anonymousPublicCheckedOpen identitiesScipostEditor, editorial staff or community,&nbsp;self-nominatedFree-form commentingStructured review form&nbsp;Anonymous&nbsp;Editor or editorial staffCheckedOpen participationOpen reportsSciRateSelf-nominatedFree-form commentingQuantitative scoresAnonymousNon-anonymousPublicCheckedOpen identitiesOpen interactionOpen participationSinai Immunology ReviewProjectSelf-nominatedStructured review formSummary and gradeNon-anonymousEditor or editorial staffPublic&nbsp;Not includedOpen identitiesOpen reports</figcaption></figure><p class="paragraph"><em>*Information not available or not identified&nbsp;</em></p>
        <p class="paragraph">Table 2: Different approaches to peer review transparency and openness</p>
        <h2>Preprint review services through the lens of four peer review schools of thought</h2><p class="paragraph">We use the four peer review schools of thought recently proposed by some of us (Waltman et al., 2023) as a framework to discuss the developments around preprint review services. Waltman et al. (2023) proposed four schools of thought on innovation in peer review, each of which has a different emphasis on what the key problems are within current peer review systems and what the priorities should be for improving these systems. Below, we present the focus of each school of thought, as previously described by Waltman et al. (2023):</p>
        <ul><li data-listnumber=""><p class="paragraph">The Democracy &amp; Transparency school focuses on making the evaluation of scientific research more democratic and transparent. Concerned that peer review systems are often elitist and opaque, this school advocates broader participation in the review process to increase the accountability of editors and peer reviewers and enable information produced in peer review to be reused by others.&nbsp;</p>
        </li><li data-listnumber=""><p class="paragraph">The Quality &amp; Reproducibility school focuses on the role of peer review in evaluating and improving the quality and reproducibility of scientific research. Based on concerns about inconsistent quality associated with peer review processes, this school is interested in innovations in peer review that improve the quality of review reports and published research. Another focal issue for this school is safeguarding research integrity and identifying scientific misconduct.</p>
        </li><li data-listnumber=""><p class="paragraph">The Equity &amp; Inclusion school focuses on making peer review processes more equitable and inclusive. This school is given impetus particularly by concerns about bias in peer review systems. It emphasises the need for a balanced representation of different groups of researchers in the peer review system to reduce or eliminate biases related to gender, geography, race, ethnicity, etc.&nbsp;</p>
        </li><li data-listnumber=""><p class="paragraph">The Efficiency &amp; Incentives school focuses on improving the efficiency of peer review processes and the incentives for peer reviewers. This school is concerned about the pressure on the peer review system, which makes it increasingly difficult to find peer reviewers, emphasising the need to increase the efficiency of peer review and to better incentivise reviewers.</p>
        </li></ul><p class="paragraph">We assessed the characteristics of the 23 preprint review services against the key issues raised by the four schools of thought, considering the stated goals and intentions of each service, their approach to peer review, and their commitment to openness and transparency. Based on those criteria, we identified how each service could be associated with each of the four schools of thought. This gives us insight into key issues, such as where the priorities of the different services lie, how they are managing apparent tensions between different aims, and to what extent they are delivering on their objectives. These designations are discussed in more detail in the following sub-sections.</p>
        <h3>Making peer review more democratic and transparent</h3><p class="paragraph">Calls for more transparent and open peer review approaches are a common feature of discussions about peer review. Recently, Kelly-Ann Allen et al. (2022) presented the lack of transparency as one of the three main challenges of peer review, together with the exploitative nature and the slowness of peer review. Based on a wide-ranging conversation on X (the social-media platform previously known as Twitter), they identified the need to open the “black box” of the current peer review system as the most constructive way to “fix peer review”. According to Fernandez-Llimos (2023) to minimise the peer review crisis, we should start recognising peer reviewers by acknowledging their contribution in every single paper they reviewed, and we can do this most effectively in open peer reviews.</p>
        <p class="paragraph">By assuming a commitment to openness and transparency, preprint review services are already arguably trying to “fix peer review”, challenging authors, reviewers, editors, and publishers to rethink and adjust their practices. Openness and transparency are common goals for all services included in our study, meaning that they all support the ideas of the Democracy &amp; Transparency school. All services in our study provide open review reports and promote open interactions among authors, reviewers and users. Still, Table 2 presents several variations in the peer review approach, selection of reviewers, management of their identities, transparency and openness. It is evident that transparency and openness have different meanings and are applied differently by different services. Managing reviewers’ identities is perhaps one of the biggest challenges the services face. As highlighted by Chloros et al., (2023) open peer review involving open identities enables better recognition of the contribution reviewers make. However, for many researchers, revealing their identity may raise concerns, like the fear of retaliation, and this may result in them providing less critical feedback. In Section 5, we discuss how preprint review services deal with this tension and how they balance transparency and openness with the interests and concerns of reviewers.</p>
        <h3>Improving quality and reproducibility</h3><p class="paragraph">The Quality &amp; Reproducibility school emphasises the crucial role of peer review in improving and certifying the quality of scientific work. A focus on quality is evident in preprint review services that train reviewers, evaluate reviewers' work, check for competing interests, and pay attention to ethics and integrity. As can be seen in Table 2, in addition to unstructured reports, preprint review services also use structured review forms, quantitative scores, and scales or ratings, showing the broad range of approaches taken by preprint review services to assess and improve the quality of scientific work. Many preprint review services have adopted ideas of the Quality &amp; Reproducibility school, thereby showing their commitment to rigorous preprint review and positioning themselves as promoters of high-quality and reproducible research. Faculty Opinions for instance states that the “caliber” of its reviewers and the “rigour” of the validation they provide ensure the quality of the research recommended by them. At Peer Community In, reviewers evaluate preprints in their thematic fields based on “rigorous peer review” and “conflicts of interests are carefully checked at each step of the process”. Plaudit considers that “publisher-independent endorsements” provided by “known and trusted academic community members provide credibility for valuable research”.</p>
        <p class="paragraph">High-quality standards in peer review are often highlighted as essential for improving research quality and reproducibility (e.g., Chambers &amp; Tzavella, 2022).&nbsp;&nbsp;As the attention given to the lack of reproducibility in research grows (e.g., Brendel, 2021; Errington et al., 2021),&nbsp;&nbsp;improving the reproducibility of research is increasingly seen as one of the responsibilities of peer review. This is reflected in the goals and intentions of some preprint review services and their policies and procedures. Examples of services for which this is the case are Scibase, Qeios and Peer Community In.</p>
        <p class="paragraph">SciBase states that “science has a reproducibility problem" and argues that “only through honest public discussion can post-publication review become part of the scientific process, meaning that the reviewer's identity is known to the public.” Reproducibility is one of the dimensions evaluated in SciBase’s review approach, which combines a report and an approval status (scale or rating). The overall rating provided by a review is a weighted average of the ratings of each of the individual dimensions, with the reproducibility dimension having the highest weight (25%). The other dimensions are logic/design, impact, transparency, clarity and versatility (15% each). SciBase also enables users to rate reviews themselves on a 1-5-star scale, with the intention of promoting transparency, rigour, accountability and reviewer recognition.</p>
        <p class="paragraph">With the aim of improving quality and reproducibility, some services, like Qeios and Peer Community In, require authors to make all study data, digital materials, and computer code publicly available at the time of submission to the maximum extent permissible by the relevant legal or ethical restrictions. Scripts, simulation code, software parameters, etc., are prerequisites for submission. Also, reviewers are asked to check that authors provide sufficient details for the methods and analysis to allow replication by other researchers, considering statistical analyses and consistency of raw data and scripts, among other critical issues. As Peer Community In states, with these prerequisites, it aims to “establish a high-quality, free, public system for identifying high-quality preprints, after rigorous peer-review.” In addition to these examples, other services like Hypothesis and Peer Library make guidelines, recommendations or other support documents available to reviewers.</p>
        <h3>Making peer review processes more equitable and inclusive</h3><p class="paragraph">Journal-based peer review is often perceived to be susceptible to disparities, inequities, and bias (e.g.,&nbsp;Bancroft et al., 2022). The need to make peer review processes more equitable and inclusive is the focus of the Equity &amp; Inclusion school. Supporters of this school often emphasise the need for a balanced representation of different groups of researchers in the peer review system to avoid biases related to gender, geography, race, ethnicity, etc.&nbsp;</p>
        <p class="paragraph">An increasing amount of attention is paid to underrepresented groups in publication and peer review processes, aiming to contribute to diversity, equity, and inclusion (Royal Society of Chemistry, n.d.). Increasing diversity amongst editors, reviewers and authors is starting to be reflected in some editorial policies and practices. A key aim is to address biases in peer review against underrepresented groups. Concerns around peer review bias highlight the complexity of social interactions among actors involved in peer review and raise questions about the nature of the various forms of bias discussed in the literature&nbsp;(Lee et al., 2013).&nbsp;&nbsp;</p>
        <p class="paragraph">An important example of a preprint review service explicitly addressing inequities and biases, with special attention to reviewers, is PREreview. By providing opportunities for traditionally marginalised research communities to get involved, train, connect, and be recognised for their contributions to scholarship, PREreview aims “to bring more equity and transparency to scholarly peer review.” At the same time, with the stated aim of helping researchers think deeply about how assumptions or biases may affect their assessment of manuscripts, PREreview developed the Bias Reflection Guide&nbsp;(Foster et al., 2021).&nbsp;&nbsp;This tool reflects the perspective of the Equity &amp; Inclusion school. This guide is provided to help students and researchers to make their approach to peer review more inclusive and less biased.</p>
        <p class="paragraph">PREreview enables reviewers to publish reviews with their public names (connected to ORCIDs) or pseudonyms provided by the platform. This is intended to allow reviewers to critique the work they review without fearing negative consequences, particularly if the author is more senior than the reviewer. In this way, ‘PREreviewers’, particularly those belonging to vulnerable communities, may “contribute to open preprint review without fearing retribution or judgment that may occur when their full name is attached to the feedback, whilst retaining an element of accountability”. From the point of view of reviewers, protecting their identities can be expected to reduce the likelihood of bias based on factors such as gender, ethnicity, or institutional affiliation. Arguably, it protects reviewers against retaliation and encourages rigorous and honest feedback. Preprint review services take several different approaches to handle issues related to the identity of reviewers. Even for services that invite reviewers to embrace “full transparency (i.e., critics' names and their criticisms are made publicly available)”, as Read Team Marked does, revealing one’s identity is not always mandatory (Table 2). Reviewers’ identities may be protected if they wish. Some services enable reviewers to choose whether they want to sign their review reports, others protect the identity of reviewers with a nickname or a pseudonym. Another approach is taken by Crowd preprint reviews, which aggregates the comments of different reviewers in a final synthesised review that is “publicly posted, without linking specific comments to the commenter.” Rapid Reviews: COVID-19 (RR:C19) emphasises global diversity. It works with a “global team, including board members and reviewers from all regions”, and reviewers can be anonymised upon request.</p>
        <p class="paragraph">Another approach promoted by the Equity &amp; Inclusion school is double-anonymous peer review, in which not only reviewers but also authors are anonymous. This approach is hard to implement in the context of preprint review. Nevertheless, some services make an effort to facilitate double-anonymous peer review in special situations. We will discuss this in more detail in Section 5.2.</p>
        <h3>Improving peer review efficiency and reviewer incentives</h3><p class="paragraph">Peer review is often seen as overloaded (e.g.,&nbsp;Nguyen et al., 2015; Okuzaki et al., 2019), and there is a lot of discussion about faster and more efficient approaches to peer review. Reducing inefficiencies in peer review constitutes a big challenge for journals, where the expectation typically is that a separate review process is carried out for each submission to each journal, including submissions previously rejected by other journals. By providing journal-independent portable review, preprint review services may reduce re-reviewing at multiple journals and increase the efficiency of peer review. On the other hand, depending on the extent to which journals are willing to adjust their processes and consider re-using preprint reviews, we might ask whether preprint review initiatives are, in fact, adding to an already overburdened system by creating even more work? In Section 6, we address these questions in more detail.</p>
        <p class="paragraph">With regard to portable peer review, ten years ago,&nbsp;Swoger, (2013)&nbsp;argued that rejection takes time and energy from authors, reviewers and editors, and that portable peer review reduces re-reviewing at multiple journals, increasing the efficiency of the process. Swoger pointed out the role that new "portable peer review" services, independent of specific journals, could play in cutting down on redundant work by separating the review process from the publication process.</p>
        <p class="paragraph">This illustrates one of the key issues emphasised by the Efficiency &amp; Incentives school, which focuses on improving the efficiency of peer review processes and the incentives for peer reviewers. This school is concerned about the pressure on the peer review system, in which it is increasingly difficult to find peer reviewers. Approaches to make review processes more efficient and reduce re-reviewing at multiple journals are facilitated by various preprint review services, like Rapid Reviews: COVID-19, PeerRef and Review Commons, together with initiatives to incentivise researchers by making their work more visible and recognised. Nevertheless, according to the experience reported by the Rapid Reviews: COVID-19 service, so far, there is no hard evidence that journals would want to take into account their reviews. Despite the effort made to balance rapidity and rigour and making peer review more efficient, not all journals are receptive to establishing formal relationships with preprint review services, to have access to a feed of reviews that could supplement their review process.&nbsp;&nbsp;</p>
        <p class="paragraph">Preprint peer review services may also address problems arising from the "reputation economy" for reviewers (Swoger, 2013). CrowdPeer, for instance, allowed reviewers “to build their reputations within the community and benefit professionally.” By implementing a reviewer evaluation system, in which the quality of a reviewer’s work is evaluated, CrowdPeer brought together key issues from the Democracy &amp; Transparency school (accountability) and the Efficiency &amp; Incentives school (recognition). Another example is preLights. By providing a “platform where early-career researchers can practise their scientific writing and reviewing skills," this service aims to raise the profiles of early-career researchers as "trusted preprint selectors and commentators." preLights provides recommendation letters to support early-career researchers.</p>
        <p class="paragraph">The attribution of a DOI (Digital Object Identifier) to public review reports makes the reviewer's work fully citable and claimable as a scientific output, and promotes recognition of reviewers. This is another way to help reviewers to gain recognition for their efforts. It is implemented by several services, like Rapid Reviews: COVID-19, Interactive Public Peer Review, and preLights. In addition, Rapid Reviews: COVID-19 and Plaudit link researchers, identified by their ORCID, to the research they have evaluated, identified by a DOI.</p>
        <p class="paragraph">For Science Open Reviewed, a “paid service [for reviewers] combined with published reviewer acknowledgement” may promote higher quality reviews, provide “fair and motivating compensation” for professional peer review services and “minimise reviewer bias and promote greater reviewer accountability.”</p>
        <h2>Managing tensions between schools of thought: Anonymisation of reviewers and authors</h2><p class="paragraph">In some cases, key priorities of different peer review schools of thought can be achieved alongside each other by the same preprint review service. For example, the Efficiency &amp; Incentives school and the Democracy &amp; Transparency school seem to complement each other in various ways. It may therefore be relatively easy for a preprint review service to embrace the ideas of both of these schools. This is, for instance, illustrated by CrowdPeer, although interestingly, this service is no longer operational. By providing “open review of preprints and engagement of a diverse group of reviewers,” it aligned with the Democracy &amp; Transparency school, making the evaluation of research more democratic and transparent. At the same time, CrowdPeer also aimed to improve the efficiency of review processes by providing “a universal structure for reviews to standardise the review process” and providing an “educational tool for reviewers.” In that way, CrowdPeer aimed to reduce the pressure on the review system, in line with the Efficiency &amp; Incentives school.</p>
        <p class="paragraph">In other cases, however, there are tensions between different peer review schools of thought, reflecting different perspectives on how peer review can best be organised. As we discuss in the next sub-sections, managing the identities of reviewers and authors is one of the biggest challenges preprint review services faces.</p>
        <h3>Anonymisation of reviewers</h3><p class="paragraph">The Equity &amp; Inclusion school promotes anonymity of reviewers, as we have seen. It sees anonymity as a way to protect reviewers against possible negative consequences of critical reviews. Especially researchers who are in an early career stage or who otherwise find themselves in a vulnerable position may fear such negative consequences. Anonymity makes it easier for these researchers to perform peer review and therefore arguably contributes to more inclusive approaches to peer review.</p>
        <p class="paragraph">The Quality &amp; Reproducibility school may also be supportive of anonymity of reviewers, based on the rationale that anonymity enables reviewers to be more frank and therefore results in higher-quality peer review. However, anonymity of reviewers is in tension with the ideas of the Democracy &amp; Transparency school. This school prefers to organise peer review as an open dialogue in which authors and reviewers participate on an equal basis, with each party accountable for their contributions. Organising peer review in this way is difficult or even impossible when reviewers are anonymous.</p>
        <p class="paragraph">As shown in Table 2, with respect to transparency and openness, preprint review services provide different options. To balance transparency and protect reviewers' interests, some services (13) enable reviewers to decide whether to sign their reviews, becoming anonymous or non-anonymous to the public. Others (8) require a non-anonymous review. In addition to the option of signing or not signing reviews, another solution already mentioned is to protect reviewers under a nickname or a pseudonym. This approach is a way in which preprint review services manage tensions between peer review schools of thought, in particular between the Democracy &amp; Transparency and the Equity &amp; Inclusion schools. PREreview is explicit that it aims to bring “more equity and transparency to scholarly peer review by supporting and empowering communities of researchers, particularly those at early stages of their career", but in doing so, it arguably also limits transparency. For Hypothesis, enabling reviewers to use a pseudonym promotes “transparency and credibility without public identity,” although it is evident that they are managing a tension between these two aims. In relation to reviewers' identity, these services attempt to balance the idea of transparency and the accountability it brings with protections for reviewers where they might experience negative consequences if their identities were disclosed.</p>
        <p class="paragraph">Anonymity of reviewers also poses a challenge for the Efficiency &amp; Incentives school. To incentivise reviewers, this school emphasises the importance of giving recognition to reviewers. However, when reviewers are anonymous, it is difficult to publicly recognise them for the efforts they make, other than to say a particular reviewer has reviewed for a particular service without any more detail than that. In particular, associating reviewers with the text of the review, and recognising their contribution at that level of detail, becomes difficult. In addition, anonymity of reviewers may also complicate the reuse of reviews, making it difficult for editors to recognise the competency and credibility of reviewers, thereby undermining the desire of the Efficiency &amp; Incentives school to make peer review more efficient.</p>
        <p class="paragraph">This highlights how complex and challenging it can be for a preprint review service to manage tensions between different priorities, like transparency, rigour, recognition, and equity and inclusion, with tensions related to anonymity in peer review being among the most challenging ones.</p>
        <h3>Anonymisation of authors</h3><p class="paragraph">For the Democracy &amp; Transparency and the Efficiency &amp; Incentives schools, performing peer review after publication, not before, offers an important way to enhance the transparency and efficiency of peer review. Hence, these schools take a critical stance towards double-anonymous approaches to peer review, in which not only reviewers but also authors are anonymous. Anonymity of authors is incompatible with the idea of performing peer review after publication. On the other hand, the Equity &amp; Inclusion school supports double anonymity, arguing that anonymity of authors reduces bias in peer review and therefore contributes to making peer review more equitable and inclusive. As already pointed out, the Quality &amp; Reproducibility school may perceive reducing bias as a way to improve the quality of peer review and may therefore also support double anonymity.</p>
        <p class="paragraph">In our set of preprint review services, there are two, Peer Community In and Hypothesis, that state they offer double-anonymous peer review (Table 2). For Peer Community In, double-anonymous peer review is not a regular practice but may happen exceptionally by request to fulfil the author's needs. To submit an article anonymously to Peer Community In, the authors must use a private web-based interface (e.g., Google Docs, Dropbox or GitHub). Data, scripts and codes must also be made available through a private web-based interface. In this way, the names of the authors do not need to be disclosed to the reviewers evaluating the article, making it possible to have a double-anonymous evaluation. If the article is eventually recommended by the recommender overseeing the peer review process, the authors must deposit a final, non-anonymised version of their article in an open archive. Although Hypothesis also claims to facilitate double-anonymous peer review, it is not clear to us how this is actually done.</p>
        <p class="paragraph">Double anonymity is essentially incompatible with preprint review. Still, although it is less transparent, it may be argued to be more inclusive because it attempts to avoid biases related to gender, geography, race, ethnicity, etc. This is clearly a compromise - an attempt to manage the tensions between the drive for transparency, characteristic of preprinting, with that of inclusiveness. By offering double-anonymous evaluation of an article, Peer Community In and Hypothesis offer a peer review process that is very similar to traditional closed journal-based peer review, making us question if one should refer to this as preprint review.</p>
        <h2>How do preprint review services fit into the publishing landscape?</h2><p class="paragraph">Preprint review services typically position themselves as complementary to journal-based peer review. The idea of being a “complementary service with no intention of competitiveness” (Peer Community In) is present in the stated aims of almost all preprint review services, which for instance, make the argument that they enable researchers to “innovate and explore new approaches to scientific dissemination, in parallel to the traditional publishing industry” (Peeriodicals). PeerRef states its aim to “provide researchers with greater choice in how their research is shared and evaluated, and eliminate the need for repeated peer review in successive journals” is a key way in which this complementarity works. Some services implement their complementary role in the system by working directly with affiliate, friendly or partner journals, including the emerging overlay journals model&nbsp;(Rousi &amp; Laakso, 2022), such as the ones hosted by Episcences, where open-peer review reports must be available on an open repository, data repository or software heritage. Other services support journals in more indirect ways, for instance by helping editors find “new research they may not have otherwise discovered” (Rapid Reviews: COVID-19) and reducing re-reviewing. In this way, preprint review services arguably help to organise peer review in more efficient ways.</p>
        <p class="paragraph">However, the success of preprint review initiatives as a complementary service to journal-based peer review depends on the extent to which journals are going to adjust their processes and are going to make use of preprint reviews. According to&nbsp;Saderi and Greaves (2021), editors may be open to these new developments only under certain conditions. To better understand if and how preprint reviews may help make journal-based peer review processes faster and more efficient, Saderi and Greaves asked editors of the journals involved in the COVID-19 Rapid Review&nbsp;(OASPA, 2020)&nbsp;initiative to fill out a short survey. Based on a small number of responses, they concluded that any progress towards an integrated workflow between preprint reviews and journal-based peer review would take a coordinated effort by different stakeholders and that this effort must focus on building mutual trust: on the one hand, the trust by editors in preprint reviews and, on the other hand, the trust by authors and reviewers that their contributions will be valued and recognised.</p>
        <p class="paragraph">If journals are not interested in making use of preprint reviews, preprint review services arguably leave themselves open to the criticism that they put even more pressure on an already overburdened system by adding another layer of review on top of journal-based peer review. Rather than developing ways to improve publishing and peer review workflows, the services might be seen as unnecessarily adding further complexity to the system. The viability of preprint review services may depend on how they address this criticism - for example by arguing that they offer alternative publication options to authors and help to turn peer review into a more rewarding experience for authors and reviewers.</p>

        <p class="paragraph">Nevertheless, while almost all preprint review services we examined ostensibly aim to supplement the current journal-based publishing system, some also identify the possibility of more radical change, aiming for more ambitious reforms of scientific publishing, considering that “the quality of published work must stand on its own, without the crutch of impact factors, journal prestige, 'likes', 'thumbs up', or the artificial stamp of approval signalled by the label “peer review”&nbsp;&nbsp;(Researchers.One), or to relieve the pressure on authors to publish their work in ’top-tier’, but often paywalled journals" (Plaudit).&nbsp;Despite some resistance from editors to partner with preprint review services, some journals are already exploring review of preprints by themselves, or partnering with preprint review services.&nbsp;This raises a key question: Should preprint review services just be seen as a complement to journal-based publishing, or could they potentially offer an alternative to journal-based publishing?</p>
        <p class="paragraph">At present, preprint review services seem to opt for a cautious approach, avoiding a radically disruptive position that could compromise their future development. However, after this first stage of experimentation, the increasing maturity of preprint review services and the involvement of and recognition from the research community may possibly enable these services to position themselves as an alternative, rather than a complement, to journal-based publishing. As discussed by Kramer et al. (2022), the dissemination function of journals may be taken over by preprint servers (although the term ‘preprint’ may no longer be appropriate), while preprint review services may take over the evaluation function of journals. In the most extreme outcome, conventional journals may cease to exist, and scientific publishing may take place entirely on preprint servers and preprint review platforms.&nbsp;</p>
        <p class="paragraph">Edmunds (2022), editor in chief at GigaScience Press, argues that as standalone movements, the several models and experiments developed to improve the journal system, “were a harder sell, but now, working together, they interact and support each other synergistically”. Like Edmunds, we believe that preprint review services offer important opportunities to improve the publishing process by improving transparency, accountability, efficiency and speed. Moreover, Edmunds highlights that “research culture has also been changing, with younger researchers seeing transparency and openness as a norm, and funders and publishers endorsing and promoting such efforts”. Edmunds is probably right that “time seems to have finally arrived” for preprinting combined with open and portable peer review.</p>
        <h2>Conclusions</h2><p class="paragraph">To provide a systematic understanding of the main characteristics of preprint review services, we carried out a descriptive study of 23 services. We described how the services have been set up to manage preprint review and how they fit into the publishing landscape. Based on this, we identified possible future models of peer review and scholarly communication.</p>
        <p class="paragraph">We used the four peer review schools of thought framework (Waltman et al., 2023) to explore how innovation and experimentation promoted by preprint review services can address the challenges of traditional journal-based peer review and provide possible solutions and future directions. Our analysis gave us insight into key issues, such as where the priorities of the different services lie and how they manage tensions between schools, reflecting different perspectives on how peer review can best be organised.</p>
        <p class="paragraph">The most important tension that we identified relates to anonymisation of reviewers and authors. In line with the ideas of the Democracy &amp; Transparency school, preprint review services promote more open forms of peer review in which authors and reviewers participate on a more equal basis. However, from the perspective of the Equity &amp; Inclusion school, this raises concerns. To make peer review processes more equitable and inclusive, this school emphasises the importance of enabling anonymisation of reviewers and possibly also authors, which is in tension with the focus on openness and transparency of preprint review services.</p>
        <p class="paragraph">Preprint review services have the potential to address a number of key issues in scholarly communication and peer review. By providing quality control, preprint review services can enrich the value of preprints, reinforce confidence in their use, and make the evaluation of research more efficient and more open and transparent. In addition, preprint review services may improve accountability and recognition of reviewers, supplement journal-based peer review, reduce the need for re-submissions, and review at multiple journals.</p>
        <p class="paragraph">The scholarly communication landscape seems to be moving toward a mixed system in which preprint servers, preprint review services and journals operate mostly in complementary ways. Preprint review services currently seem to avoid a radically disruptive position. Still, after a period of experimentation, their increasing maturity may enable these services to position themselves as an alternative, rather than a complement, to traditional journal-based publishing. In the longer term, preprint review services may, therefore, disrupt the scholarly communication landscape in a more radical way.</p>
        <p class="paragraph">Despite the potential to either complement existing scholarly communication services or even replace them, it is apparent that at present many preprint review services face problems of sustainability. During the period of our analysis, two services, CrowdPeer and Red Team Marked, were taken out of service. Whilst the reasons for their closure are not entirely clear, it is obvious that many of the services we studied are built with small budgets and have uncertain funding streams. They typically rely on the enthusiasm and commitment of volunteers. Achieving financial stability will be a challenge for many of these services, particularly if they wish to maintain the independence from large commercial interests, which many of them currently have.</p>
        <h2>Acknowledgements</h2><p class="paragraph">We are grateful to&nbsp;the managers of&nbsp;preprint review services that kindly contributed to our work by clarifying and confirming some details and adding relevant additional information. We thank Jessica Polka from Crowd preprint review, Raphaël Tournoy from Episciences,&nbsp;Natascha Töpfer from Interactive Public Peer Review, Denis Bourguet from Peer Community In, Mitar from Peer Library, Elliott Lumb from PeerRef, Reinier Prosée from preLights, Daniela Saderi from PREreview, Gabriele Marinello from Qeios, Nick Lindsay from Rapid Reviews: COVID-19, Ryan Martin from Researchers.One and Aram Harrow from SciRate.&nbsp;We also extend our gratitude to Stephen Gabrielson, Dibyendu Roy Chowdhury, Ashley Farley, and Gary McDowell of the ASAPbio Meta-Research Crowd for their insightful comments on our work (<a href="https://prereview.org/reviews/10210714" rel="" target="blank">https://prereview.org/reviews/10210714</a>).&nbsp;&nbsp;</p>
        <h2>Author contributions</h2><p class="paragraph">Conceptualization: SOH, NR, SP, LW; Data curation: SOH; Formal analysis: SOH; Funding acquisition: SP, LW; Investigation: SOH; Methodology: SOH, SP, LW; Project administration: SOH, SP, LW; Supervision: SP, LW; Validation: SOH, NR, SP, LW; Visualization: SOH; Writing – original draft: SOH; Writing – review &amp; editing: SOH, NR, SP, LW&nbsp;</p>
        <h2>Competing interests</h2><p class="paragraph">Stephen Pinfield and Ludo Waltman are involved in MetaROR (MetaResearch Open Review), a platform for open peer review of preprints in the field of metaresearch.</p>
        <h2>Funding information</h2><p class="paragraph">Stephen Pinfield and Ludo Waltman were supported by Wellcome [221297/Z/20/Z] as part of its core funding of the Research on Research Institute (RoRI).</p>
        <h2>Data availability</h2><p class="paragraph">The data that support the findings of this study are openly available in figshare.https://doi.org/10.6084/m9.figshare.24307312.v1</p>
        <h2>References</h2><p class="paragraph">ASAPbio. (n.d.). ReimagineReview – A registry of platforms and experiments innovating around peer review<em>.</em>&nbsp;<em>ASAPbio.&nbsp;</em>https://reimaginereview.asapbio.org/</p>
        <p class="paragraph">Avissar-Whiting, M., Belliard, F., Brand, A., Brown, K., Clément-Stoneham, G., Dawson, S., Dey, G., Ecer, D., Edmunds, S. C., Fischer, T. D., Farley, A., Franko, M., Fraser, J., Funk, K., Ganier, C., Harrison, M., Hatch, A., Hazlett, H., Hindle, S., … Williams, M. (2023). Advancing the culture of peer review with preprints.&nbsp;<em>Open Science Framework.</em>https://doi.org/10.31219/osf.io/cht8p</p>
        <p class="paragraph">Bancroft, S. F., Ryoo, K., &amp; Miles, M. (2022). Promoting equity in the peer review process of journal publication.&nbsp;<em>Science Education</em>, 106(5), 1232–1248. https://doi.org/10.1002/sce.21733</p>
        <p class="paragraph">Blatch-Jones, A., Saucedo, A. R., &amp; Giddins, B. (2023). The use and acceptability of preprints in health and social care settings: A scoping review.&nbsp;<em>SocArXiv</em>. https://doi.org/10.31235/osf.io/nug4p</p>
        <p class="paragraph">Brendel, K. (2021, July 5). Time to assume that health research is fraudulent until proven otherwise?&nbsp;<em>BMJ</em>. https://blogs.bmj.com/bmj/2021/07/05/time-to-assume-that-health-research-is-fraudulent-until-proved-otherwise/</p>
        <p class="paragraph">Chambers, C. D., &amp; Tzavella, L. (2022). The past, present and future of Registered Reports.&nbsp;<em>Nature Human Behaviour</em>,&nbsp;<em>6</em>(1), 29–42. https://doi.org/10.1038/s41562-021-01193-7</p>
        <p class="paragraph">Chloros, G. D., Konstantinidis, C. I., Vasilopoulou, A., &amp; Giannoudis, P. V. (2023). Peer review practices in academic medicine: How the example of orthopaedic surgery may help shift the paradigm?&nbsp;<em>International Orthopaedics</em>,&nbsp;<em>47</em>(5), 1137–1145. https://doi.org/10.1007/s00264-023-05729-6</p>
        <p class="paragraph">Edmunds, S. (2022). Preprints and open peer review come of age.&nbsp;<em>Research Information.&nbsp;</em>https://www.researchinformation.info/analysis-opinion/preprints-and-open-peer-review-come-age</p>
        <p class="paragraph">Errington, T. M., Mathur, M., Soderberg, C. K., Denis, A., Perfito, N., Iorns, E., &amp; Nosek, B. A. (2021). Investigating the replicability of preclinical cancer biology.&nbsp;<em>eLife</em>,&nbsp;<em>10</em>, e71601. https://doi.org/10.7554/eLife.71601</p>
        <p class="paragraph">Fernandez-Llimos, F. (2023). Comment on the article: “Peer review practices in academic medicine: how the example of orthopaedic surgery may help shift the paradigm?”&nbsp;<em>International Orthopaedics,</em>&nbsp;<em>47</em>(5), 1391–1392. https://doi.org/10.1007/s00264-023-05768-z</p>
        <p class="paragraph">Foster, A., Hindle, S., Murphy, K. M., &amp; Saderi, D. (2021). Open Reviewers Bias Reflection Guide.&nbsp;<em>Zenodo</em>. https://doi.org/10.5281/zenodo.5484052</p>
        <p class="paragraph">Fraser, N., Brierley, L., Dey, G., Polka, J. K., Pálfy, M., Nanni, F., &amp; Coates, J. A. (2021). The evolving role of preprints in the dissemination of COVID-19 research and their impact on the science communication landscape.&nbsp;<em>PLOS Biology</em>,&nbsp;<em>19</em>(4), e3000959. https://doi.org/10.1371/journal.pbio.3000959</p>
        <p class="paragraph">Henriques, Susana Oliveira; Rzayeva, Narmin; Pinfield, Stephen; Waltman, Ludo (2023). Data Set - Preprint review services: Disrupting the scholarly communication landscape?. figshare. Dataset. https://doi.org/10.6084/m9.figshare.24307312.v1</p>
        <p class="paragraph">Lee, C. J., Sugimoto, C. R., Zhang, G., &amp; Cronin, B. (2013). Bias in peer review.&nbsp;<em>Journal of the American Society for Information Science and Technology</em>,&nbsp;<em>64</em>(1), 2–17. https://doi.org/10.1002/asi.22784</p>
        <p class="paragraph">Lutz, J. F., Sondervan, J., Edig, X. van, Freeman, A., Kramer, B., &amp; Rosenkrantz, C. H. (2023). Knowledge Exchange Analysis Report on Alternative Publishing&nbsp;Platforms.&nbsp;<em>Alternative Publishing Platforms.&nbsp;</em>https://doi.org/10.21428/996e2e37.3ebdc864</p>
        <p class="paragraph">Nguyen, V. M., Haddaway, N. R., Gutowsky, L. F. G., Wilson, A. D. M., Gallagher, A. J., Donaldson, M. R., Hammerschlag, N., &amp; Cooke, S. J. (2015). How long is too long in contemporary peer review? Perspectives from authors publishing in conservation biology journals.&nbsp;<em>PLOS ONE</em>,&nbsp;<em>10</em>(8), e0132557. https://doi.org/10.1371/journal.pone.0132557</p>
        <p class="paragraph">Ni, R., &amp; Waltman, L. (2023). To preprint or not to preprint: A global researcher survey.&nbsp;<em>SocArXiv</em>. https://doi.org/10.31235/osf.io/k7reb</p>
        <p class="paragraph">OASPA. (2020).&nbsp;<em>COVID-19&nbsp;</em>Publishers Open Letter of Intent -&nbsp;Rapid Review.&nbsp;<em>OASPA.&nbsp;</em>https://oaspa.org/covid-19-publishers-open-letter-of-intent-rapid-review/</p>
        <p class="paragraph">Okuzaki, Y., Nakamura, S., &amp; Nakaoka, M. (2019).&nbsp;Toward more rapid and efficient peer review: A case study on publication in Ecological Research.&nbsp;<em>Ecological Research</em>,&nbsp;<em>34</em>(5), 563–574. https://doi.org/10.1111/1440-1703.12037</p>
        <p class="paragraph">Reardon, J., Crawford, J., Allen, K.-A., &amp; Walsh, L. (2022, July 25). The peer review system is broken. We asked academics how to fix it.&nbsp;<em>The Conversation.&nbsp;</em>http://theconversation.com/the-peer-review-system-is-broken-we-asked-academics-how-to-fix-it-187034</p><p class="paragraph">Ross-Hellauer, T. (2017). What is open peer review? A systematic review.&nbsp;<em>F1000Research</em>,&nbsp;<em>6</em>, 588. https://doi.org/10.12688/f1000research.11369.2</p><p class="paragraph">Rousi, A. M., &amp; Laakso, M. (2022). Overlay journals: A study of the current landscape.&nbsp;<em>Journal of Librarianship and Information Science</em>, 096100062211252. https://doi.org/10.1177/09610006221125208</p><p class="paragraph">Royal Society of Chemistry. (n.d.). Joint commitment for action on inclusion and diversity in publishing.&nbsp;
        <em>Royal Society of Chemistry</em>.&nbsp;<a href="https://www.rsc.org/policy-evidence-campaigns/inclusion-diversity/joint-commitment-for-action-inclusion-and-diversity-in-publishing/" rel="" target="blank">https://www.rsc.org/policy-evidence-campaigns/inclusion-diversity/joint-commitment-for-action-inclusion-and-diversity-in-publishing/</a></p><p class="paragraph">RR\C19. (n.d.). Rapid Reviews Infectious Diseases. Retrieved 19 July 2024, from https://rrid.mitpress.mit.edu/rrc19</p><p class="paragraph">Saderi, D., &amp; Greaves, S. (2021, July 7).<em>&nbsp;</em>Using preprint reviews to drive journal peer review.&nbsp;<em>ASAPbio</em>. https://asapbio.org/using-preprint-reviews-to-drive-journal-peer-review</p><p class="paragraph">Swoger, B. (2013). Can you take it with you when you go? Portable peer review.&nbsp;<em>Scientific American Blog Network.</em>https://blogs.scientificamerican.com/information-culture/can-you-take-it-with-you-when-you-go-portable-peer-review/</p><p class="paragraph">Waltman, L., Kaltenbrunner, W., Pinfield, S., &amp; Woods, H. B. (2023). How to improve scientific peer review: Four schools of thought.&nbsp;<em>Learned Publishing</em>,&nbsp;<em>36</em>(3), 334–347. https://doi.org/10.1002/leap.1544</p>
        `,
    },
    dates: {
      "Curated version": "Sep 12, 2024",
      "Peer reviewed": "Sep 12, 2024",
      "Preprint posted": "Sep 12, 2024",
    },
  },
  15: {
    doi: "10.48550/arXiv.2404.06500",
    title: "The Rise and Fall of the Initial Era",
    authors: [
      {
        name: "Simon Porter",
        email: "s.porter@digital-science.com",
      },
      {
        name: "Daniel Hook",
        email: "d.hook@digital-science.com",
      },
    ],
    reviews: [
      {
        name: "Dmitry Kochetkov",
        orcid: "0000-0001-7890-7532",
        review: `
            <p class="paragraph">The presented preprint is a well-researched study on a relevant topic that could be of interest to a broad audience. The study's strengths include a well-structured and clearly presented methodology. The code and data used in the research are openly available on Figshare, in line with best practices for transparency. Furthermore, the findings are presented in a clear and organized manner, with visualization that aid understanding.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">At the same time, I would like to draw your attention to a few points that could potentially improve the work.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I think it would be beneficial to expand the annotation to approximately 250 words.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The introduction starts with a very broad context, but the connection between this context and the object of the research is not immediately clear. There are few references in this section, making it difficult to determine whether the authors are citing others or their own findings.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The transition to the main topic of the study is not well-defined, and there is no description of the gap in the literature regarding the object of study. Additionally, "bibliometric archaeology" appears at the end of the introduction but is only mentioned again later in the discussion, which may cause confusion for the reader.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; It would be helpful to clearly state the purpose and objectives of the study both in the Introduction and in the abstract as well.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Besides, it is important to elaborate on the contribution of this study in the introduction section.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The same applies to the background - a very broad context, but the connection with the object of the research is not entirely clear.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Page 4 - as far as I understand, these are conclusions from a literature review, while point 3 (Reflective Richness of Data) does not follow from the previous analysis.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The overall impression of the introduction and background is that it is an interesting text, but it is not well related to the objectives of the study. I would recommend shortening these sections by making the introduction and literature review more pragmatic and structured. At the same time, this text could be published as a standalone contribution.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; As I mentioned above, the methodology refers to the strengths of the study. However, in this section, it would be helpful to introduce and justify the structure of presenting the results.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">10.&nbsp;&nbsp; In the methodology section, the authors could also provide a footnote with a link to the code and dataset (currently, it is only given at the end).</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">11.&nbsp;&nbsp; With regard to the discussion, I would like to encourage the authors to place their results more clearly in the academic context. Ideally, references from the introduction and/or literature review would reappear in this section to help clarify the research contribution.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">12.&nbsp;&nbsp; Although Discussion C is an interesting read, it seems more related to the introduction than the results. Again, the text itself is rather interesting, but it would benefit from a more thorough justification.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">Remarks on the images:</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; At least the data source for the images should be specified in the background, because it is not obvious to the reader before describing the methodology.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The color distinction between China and Russia in Figure 8 is not very clear.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The gray lines in Figures 9-11 make the figures difficult to read. Additionally, the meaning of these lines is not clearly indicated in the legends of Figures 10 and 11. These issues should be addressed.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">All comments and suggestions are intended to improve the article. Overall, I have a very positive impression of the work.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">Sincere,</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">Dmitry Kochetkov</p>
            `,
      },
      {
        name: "Erjia Yan",
        orcid: "0000-0002-0365-9340",
        review: `
            <p class="paragraph"><strong>Overview</strong></p>
            <p class="paragraph">This manuscript provides an in-depth examination of the use of initials versus full names in academic publications over time, identifying what the authors term the "Initial Era" (1945-1980) as a period during which initials were predominantly used. The authors contextualize this within broader technological, cultural, and societal changes, leveraging a large dataset from the Dimensions database. This study contributes to the understanding of how bibliographic metadata reflects shifts in research culture.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph"><strong>Strengths</strong></p>
            <p class="paragraph"><strong>+ Novel concept and historical depth</strong></p>
            <p class="paragraph">The paper introduces a unique angle on the evolution of scholarly communication by focusing on the use of initials in author names. The concept of the "Initial Era" is original and well- defined, adding a historical dimension to the study of metadata that is often overlooked. The manuscript provides a compelling narrative that connects technological changes with shifts in academic culture.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph"><strong>+ Comprehensive dataset</strong></p>
            <p class="paragraph">The use of the Dimensions database, which includes over 144 million publications, lends significant weight to the findings. The authors effectively utilize this resource to provide both anecdotal and statistical analyses, giving the paper a broad scope. The differentiation between the anecdotal and statistical epochs helps clarify the limitations of the dataset and strengthens the authors' conclusions.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph"><strong>+ Cross-disciplinary relevance</strong></p>
            <p class="paragraph">The study's insights into the sociology of research, particularly the implications of name usage for gender and cultural representation, are highly relevant across multiple disciplines. The paper touches on issues of diversity, bias, and the visibility of researchers from different backgrounds, making it an important contribution to ongoing discussions about equity in academia.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph"><strong>+ Technological impact</strong></p>
            <p class="paragraph">The authors successfully connect the decline of the "Initial Era" to the rise of digital publishing technologies, such as Crossref, PubMed, and ORCID. This link between technological infrastructure and shifts in scholarly norms is a critical insight, showing how the adoption of new tools has real-world implications for academic practices.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph"><strong>Weaknesses</strong></p>
            <p class="paragraph">-&nbsp; <strong>Lack of clarity and readability</strong></p>
            <p class="paragraph">While the manuscript is rich in data and analysis, it can be dense and challenging to follow for readers not familiar with the technical details of bibliometric studies. The text occasionally delves into highly specific discussions that may be difficult for a broader audience to grasp while other concepts are introduced in cursory. Consider condensing the introduction section, removing unrelated historical accounts, and leading the audience to the key objectives of this research much earlier.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">-&nbsp; <strong>Missing empirical case studies</strong></p>
            <p class="paragraph">The manuscript remains largely theoretical, relying heavily on data analysis without providing concrete case studies or empirical examples of how the "Initial Era" affected individual disciplines or researchers. A more detailed exploration of specific instances where the use of initials had significant consequences would make the findings more tangible. Incorporating case studies or anecdotes from the history of science that illustrate the real-world impacts of the trends identified in the data would enrich the paper. These examples could help ground the analysis in practical outcomes and demonstrate the relevance of the "Initial Era" to contemporary debates.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">-&nbsp; <strong>Half-baked comparative analysis</strong></p>
            <p class="paragraph">Although the paper presents interesting data about different countries and disciplines, the comparative analysis between these groups could be further developed. For example, the reasons behind the differences in initial use between countries with different writing systems or academic cultures are not fully explored. A more in-depth comparative analysis that explains the cultural, linguistic, or institutional factors driving the observed differences in initial use would add nuance to the findings. This could involve a more detailed discussion of how non-Roman writing systems influence name formatting or how specific national academic policies shape author metadata.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph">-&nbsp; <strong>Limited discussion of alternative explanations</strong></p>
            <p class="paragraph">While the authors link the decline of the "Initial Era" to technological advancements, other potential explanations, such as changing editorial policies (“technological harmonisation”), shifts in academic prestige, or the influence of global collaboration, are not fully explored. The paper could benefit from a broader discussion of these factors. Expanding the discussion to include alternative explanations for the decline of initial use, and how these might interact with technological changes, would provide a more comprehensive view. Engaging with literature on academic publishing practices, editorial decisions, and global research trends could help contextualize the findings within a wider framework.</p>
            <p class="paragraph">&nbsp;</p>
            <p class="paragraph"><strong>Conclusion</strong></p>
            <p class="paragraph">This manuscript offers a novel and insightful analysis of the evolution of name usage in academic publications, providing valuable contributions to the fields of bibliometrics, science studies, and research culture. With improvements in clarity, comparative analysis, and the incorporation of case studies, this paper has the potential to make a significant impact on our understanding of how metadata reflects broader societal and technological changes in academia. The authors are encouraged to refine their discussion and expand on the implications of their findings to make the manuscript more accessible and applicable to a wider audience.</p>
            </div>
            `,
      },
      {
        name: "Laurel L Haak",
        orcid: "0000-0001-5109-3700",
        review: `
            <p class="paragraph">I started reading this paper with great interest, which flagged over time. As someone with extensive experience both publishing peer-reviewed research articles and working with publication data (Web of Science, Scopus, PubMed, PubMedCentral) I understand there are vagaries in the data because of how and when it was collected, and when certain policies and processes were implemented. For example, as an author starting in the late 1980s, we were instructed by the journal “guide to authors” to use only initials. My early papers were all only using initials. This changed in the mid-late 1990s. Another example, when working with NIH publications data, one knows dates like 1946 (how far back MedLine data go), 1996 (when PubMed was launched), and 2000 (when PubMedCentral was launched) and 2008 (when NIH Open Access policy enacted). There are also intermediate dates for changes in curation policy…. that underlie a transition from initials to full name in the biomedical literature.</p>
            <p class="paragraph">I realize that the study covers all research disciplines, but still I am surprised that the authors of this paper don’t start with an examination of the policies underlying publications data, and only get to this at the end of a fairly torturous study.</p>
            <p class="paragraph">As a reader, this reviewer felt pulled all over the place in this article and increasingly frustrated that this is a paper that explores the Dimensions database vagaries only and not really the core overall challenges of bibliometric data, irrespective of data source. Dimensions ingests data from multiple sources — so any analysis of its contents needs to examine those sources first.</p>
            <p class="paragraph">A few specific comments:</p>
            <ul><li data-listnumber=""><p class="paragraph">The “history of science” portion of the paper focuses on English learned societies in the 17th century. There were many other learned societies across Europe, and also “papers” (books, treatises) from long before the 17th century in Middle-eastern and Asian countries (e.g, see history of mathematics, engineering, governance and policy, etc.). These other histories were not acknowledged by the authors. Research didn’t just spring full-formed out of Zeus’ head.</p>
            </li><li data-listnumber=""><p class="paragraph">It is unclear throughout if the authors are referring to science, research, which disciplines are or are not included. The first chart on discipinary coverage is Fig 13 and goes back to 1940ish. Also, which languages are included in the analysis? For example, Figure 2 says “academic output” but from which academies? What countries? What languages? Disciplines? Also, in Figure 2, this reviewer would have like to see discussion about the variability in the noisiness of the data over time.</p>
            </li><li data-listnumber=""><p class="paragraph">The inclusion of gender in the paper misses the mark for this reviewer. When dealing with initials, how can one identify gender? And when working in times/societies where women had to hide their identity to be published…. how can a name-based analysis of gender be applied? If this paper remains a study of the “initial era”, this reviewer recommends removing the gender analysis.</p>
            </li><li data-listnumber=""><p class="paragraph">Reference needed for “It is just as important to see ourselves reflected in the outputs of the research careers…” (section B).</p>
            </li><li data-listnumber=""><p class="paragraph">Reference needed for “This period marked the emergence of “Big Science” (Section B). How do we know this is Big Science? What is the relationship with the nature of science careers? Here it would be useful perhaps to mention that postdocs were virtually unheard of before Sputnik.</p>
            </li><li data-listnumber=""><p class="paragraph">Fig 3. This would be more effective as a % total papers than absolute #.</p>
            </li><li data-listnumber=""><p class="paragraph">Gradual Evolution of the Scholarly Record. This reviewer would like to see proportion of papers without authors. A lot of history of science research is available for this period, and a few references here would be welcome, as well as a by-country analysis (or acknowledgement that the data are largely from Europe and/or English-speaking countries).</p>
            </li><li data-listnumber=""><p class="paragraph">Accelerated Changes in Recent Times. Again, this reviewer would like to see reference to scholarship on the history of science. One of the things happening in the post WW2 timeframe is the increase in government spending (in the US particularly) on R&amp;D and academic research. So, is the academy changing or is it responding to “market forces”.</p>
            </li><li data-listnumber=""><p class="paragraph">Reflective richness of data. “Evolution of the research community” is not described in the text, not is collaborative networks.</p>
            </li><li data-listnumber=""><p class="paragraph">In the following paragraph, one could argue that evaluation was a driver of change, not a response to it. This reviewer would like to see references here.</p>
            </li><li data-listnumber=""><p class="paragraph">II. Methodology. (i) 2nd sentence missing “to” “… and full form to refer to an author name…”. (ii) 2nd para the authors talk about epochs, but the data could be (are) discontinuous because of (a) curation policy, (b) curation technology, (c) data sources (e.g., Medline rolled out in the 1960s and back-populated to 1946). (iii) 4th para referes to Figs 3 and 4 showing a marked change between 1940 and 1950, but Fig 3 goes back only to 1960, and Fig 4 is so compressed it is hard to see anything in that time range. (iv) Para 7. “the active publishing community is a reasonable proxy for the global research population”. We need a reference here and more analysis. Is this Europe? English language? Which disciplines? All academia? Dimensions data? (v) Para 12 “In exploring the issue of gender…” see comments above. Gender is an important consideration but is out of scope, in this reviewer’s opinion, for this paper focused on use of initials vs. full name.</p>
            </li><li data-listnumber=""><p class="paragraph">Listing 1. Is there a resolvable URL/DOI for this query?</p>
            </li><li data-listnumber=""><p class="paragraph">Figs 9-11, 14, 15. This reviewer would like to see a more fulsome examination / discussion of data discontinuities. Particularly around ~1985-2000.</p>
            </li></ul><p class="paragraph">Discussion.</p>
            <ul><li data-listnumber=""><p class="paragraph">The country-level discussion suggests the data (publications included) are only those that have been translated into English. Please clarify. Also, please add references in this section. There are a lot of bold statements, such as “A characteristic of these countries was the establishment of strong national academies.” Is this different from other places in the world? How? In the para before this statement, there is a phrase “picking out Slavonic stages” that is not clear to this reviewer.</p>
            </li><li data-listnumber=""><p class="paragraph">The authors seem to get ahead of themselves talking about “formal” and “informal” in relation to whether initials or full names are used. And then discuss the “Power Distance” and end up arguing that it isn’t formal/informal … but rather publisher policies and curation practices driving the initial era and its end.</p>
            </li><li data-listnumber=""><p class="paragraph">And then the authors come full circle on research articles being a technology, akin to a contract. Which is neat and useful. But all the intermediate data analysis is focused on the Dimensions data base and this reviewer would argue should be a part of the database documentation rather than a scholarly article.</p>
            </li><li data-listnumber=""><p class="paragraph">This reviewer would prefer this paper be focused much more tightly on how publishing technology can and has driven the sociology of science. Dig more into the E. Journal Analysis and F. Technological analysis. Stick with what you have deep data for, and provide us readers with a practical and useful paper that maybe, just maybe, publishers will read and be incentivized to up their game with respect to adoption of “new” technologies like ORCID, DOIs for data, etc. Because these papers are not just expositions on a disciplinary discourse, they are also a window into how science (research) works and is done.</p></li></ul></div>
            `,
      },
    ],
    published: "Sep 12, 2024",
    dates: {
      "Curated version": "Sep 30, 2024",
      "Peer reviewed": "Sep 30, 2024",
      "Preprint posted": "Sep 30, 2024",
    },
    sections: {
      Abstract: `
        Bibliographic data is a rich source of information that goes beyond the use cases of location and citation -- it also encodes both cultural and technological context. For most of its existence, the scholarly record has changed slowly and hence provides an opportunity to gain insight through its reflection of the cultural norms of the research community over the last four centuries. While it is often difficult to distinguish the originating driver of change, it is still valuable to consider the motivating influences that have led to changes in the structure of the scholarly record. An "initial era" is identified during which initials were used in preference to full names by authors on scholarly communications. Causes of the emergence and demise of this era are considered as well as the implications of this era on research culture and practice.
        `,
      rest: `
        <h2>Introduction</h2><p class="paragraph">In the contemporary discourse on technology, dominated by references to digital and electronic innovations, the notion of the research article as a form of technology may appear incongruous. However, an exploration of the research article through the lens of technology is critical for a comprehensive understanding of its role, interactions with other technological forms, and its consequent impact on society. The research article, viewed technologically, is a significant construct, with a long-standing history of shaping social norms and establishing institutions that extend their influence across the research community, irrespective of disciplinary boundaries, geographical locations, or historical periods&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib1" rel="" target="blank">1</a>]. This technology has achieved ubiquity on three distinct levels: spatial, with researchers globally engaging with and understanding research articles under shared assumptions; temporal, allowing for the contextual comprehension of older articles through a slow evolution of the format; and disciplinary, with a cross-disciplinary recognition of the rigorous scrutiny and scientific methodology underpinning the work. These characteristics are critical for research to function, as an incremental activity that builds on prior results and knowledge.</p>
        <p class="paragraph">The intrinsic characteristics of research papers have rendered them the foundational elements of research communication and, crucially, the conduits of trust among researchers, transcending spatial, temporal, and disciplinary divides. This established trust facilitates incremental research and underpins the development and cohesion of the global research community. Analogous to economic institutions, the norms surrounding research papers enable researchers to make assumptions similar to the reliability of contracts in legally robust countries, thus enabling international academic transactions. Beyond facilitating trust, the structured format of a research paper—detailing the research’s specifics, the researchers, the location and timing of the research, funding sources, and relevant previous work—supports the provenance and contextualisation essential for the credibility of its communicated results.</p>
        <p class="paragraph">The interaction between technology and its consequent influence over its users and communities is a well-documented phenomenon; however, possibly due to the long-lived and slow-changing nature of its underlying format, the research paper stands out for its persistence over the centuries. Over approximately the first 300 years of formalised research communication, dating back to the 1660s, the pace of change in research publication formats has been gradual. Only in the last half-century has the rapid transformation of research practices necessitated a quicker evolution of this technology.</p>
        <p class="paragraph">At its core, the research paper remains a rare example of a 17<sup>th</sup>-Century technology in current use. It is a technology that originally emerged from a very different time to serve a very different context. It was the Age of Empire, where research communications were characterised by brief correspondences, often containing hand-drawn diagrams or concise result tables, exchanged among a small, affluent elite. Originating in the club culture of coffee houses in cities like London and Paris, this technology now underpins a vast, global research ecosystem, supporting millions of contributors and a burgeoning diversity of cultures, geographies, and subjects&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib2" rel="" target="blank">2</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib3" rel="" target="blank">3</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib4" rel="" target="blank">4</a>].</p>
        <p class="paragraph">The study of the representation of author names provides us with an insight into the structure of research culture and the sociology of research. This has been well-studied with the implications of gender bias in various aspects of the scholarly record being examined in detail&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib5" rel="" target="blank">5</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib6" rel="" target="blank">6</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib7" rel="" target="blank">7</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib8" rel="" target="blank">8</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib9" rel="" target="blank">9</a>]. It is not the focus of this article to rehash these arguments but rather to add to the data supporting these arguments. In a period where there are high proportions of researchers using full-form names, we are able potentially to determine more about the gender and background of the participants in the research ecosystem and thus learn about the geographical and discipline-based diversity of the participants in the research ecosystem. When we enter periods where initial-form is used, data become less rich and we are able to determine less.</p>
        <p class="paragraph">This study leverages the slow evolution of the research paper to conduct a form of “bibliometric archaeology”&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib10" rel="" target="blank">10</a>]&nbsp;over the digital research record&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib11" rel="" target="blank">11</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib12" rel="" target="blank">12</a>], examining the interplay between technological and societal changes as encapsulated in the history of the research paper’s format. By focusing on seemingly minor details, such as the presentation of author names, from full given names (“full form”) to initials only (“initial form”), this paper seeks to uncover broader narratives, correlating shifts in norms with technological, geographic, disciplinary, and societal factors.</p>
        <p class="paragraph">The paper is organised as follows: In the remainder of the current section we set a historical backdrop that gives an insight into the consistency of the data that we study. In the methodology section, we give an overview of the data sources and technology used with exemplar code, and we describe the approach that we have taken to classify authors and papers into the different cohorts required for the analyses described above. In the results section we present several analyses and interpret these. Finally, we conclude the paper with a brief discussion of the results and suggest future directions of research.</p>
        <h3>A brief history of metadata norms</h3><p class="paragraph">The prevailing norm within contemporary academic discourse mandates the association of scholarly outputs—be it journal articles, conference proceedings, books, or other forms—with one or more authors. This practice, however, was not always a staple of scholarly communication. The explicit linkage of authors to their scholarly contributions, particularly when introducing novel results or claims, has evolved into a cornerstone of the modern research paradigm. This evolution is driven largely by the critical role of provenance in establishing the reliability of claims, facilitating subsequent research built upon these foundations, and the necessity for researchers to be credited with their findings as a means of securing funding and institutional support.</p>
        <p class="paragraph">Despite the ubiquity of this norm, contemporary scholarly practices do witness exceptions, particularly in types of outputs that do not introduce novel findings, such as editorial articles, where the omission of author names remains more common.</p>
        <p class="paragraph">In the nascent stages of formal scholarly communication, particularly within the “club” culture that characterised the early operations of the Philosophical Transactions of the Royal Society—regarded as the first formal research journal—the practice of attributing author names to scholarly works was not deemed essential. An analysis of the journal’s publications from its initial five years (1665-1669) reveals that only 171 out of 357 articles listed in the&nbsp;<em>Dimensions</em>&nbsp;database from Digital Science feature identifiable authors. Furthermore, during this period, not a single contribution to the journal provided the address or institutional affiliation of the author, underscoring a significant departure from contemporary standards of authorship and attribution.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F1-658x1024.png" alt="" data-id="860c39c7-b2fc-462f-b80c-3c75dd215283" contenteditable="false"><figcaption class="wp-element-caption">Figure 1:&nbsp;A page from the Philosophical Transactions of the Royal Society from December 1669 demonstrating at once the similarity to a modern research article and the significant differences in the level and detail of metadata present in the article&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib13" rel="" target="blank">13</a>].</figcaption></figure><p class="paragraph">In the&nbsp;17t⁢h&nbsp;Century, at the advent of formalised scholarly communication, there was a relatively small community and communications to the journal either arrived directly from a member of a scholarly society or were submitted via a member. Thus, names were often either considered immaterial, would have been known from context and are now lost to time, or were elegantly, and seemingly purposefully not mentioned as in the example from 1669 shown in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S1.F1" rel="" target="blank">1</a>, which reads</p>
        <blockquote><p class="paragraph"><em>“A Letter Written by an Intelligent and Worthy English Man from Paris, to a Considerable Member of the R. Society in London, concerning some Transactions there, relating to the Experiment of the Transfusion of Blood.”</em></p>
        </blockquote><p class="paragraph">The letter is dated but not signed and the author is not explicitly named&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib13" rel="" target="blank">13</a>].</p>
        <p class="paragraph">The genesis of any novel form of discourse inherently involves a period of adjustment, wherein norms and social contracts gradually emerge and solidify. This evolutionary process has led to a myriad of reasons for the inclusion or exclusion of specific details in formal communications, some of which mirror broader societal norms. Reflecting on the origins of scholarly communication, primarily rooted in British and European traditions, one can draw parallels with the literary conventions of the time. For instance, in Jane Austen’s ”Pride and Prejudice” (published in 1797), the first names of Mr. and Mrs. Bennet are never revealed. Similarly, Arthur Conan Doyle’s iconic characters, Holmes and Watson, introduced in 1887, seldom use first names in their interactions, reflecting the written norms of their times.</p>
        <p class="paragraph">The reasons for omitting author identities can also stem from more problematic motives, such as the desire to conceal an author’s gender. Now famously, the Brontë sisters initially published under male pseudonyms as writing was not an appropriate activity for women in that period. More recently J.K.Rowling opted to use her initials to conceal her gender for commercial reasons, anticipating potential bias in her readership, before she became too well known and successful to be able to conceal her identity&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib14" rel="" target="blank">14</a>].</p>
        <p class="paragraph">Anonymity or pseudonymity can be a choice not only to protect from persecution, it can also serve to protect authors from repercussions related to the voicing of controversial opinions or work that is not in step with the political regime where it is being carried out. The revolutionary ideas shared on the future of currency are an example in the seminal white paper on Bitcoin by Satoshi Nakamoto, where the identity, gender, nationality, or even the individuality or plurality of the author(s) remains unknown&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib15" rel="" target="blank">15</a>].</p>
        <h3>The evolution of the research article as technology</h3><p class="paragraph">The role of technology in facilitating or restricting the disclosure of author information has pivotal import. For example, while current practices do not mandate the encoding of gender data in author identities, this omission has been significant in fields where research outcomes vary with the researcher’s gender&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib16" rel="" target="blank">16</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib17" rel="" target="blank">17</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib18" rel="" target="blank">18</a>]. The decision not to maintain detailed records of authors’ gender or ethnicity is influenced by contemporary societal norms, yet future scholars might view this lack of information as a critical oversight, akin to how the absence of affiliation details in early scholarly works is perceived today.</p>
        <p class="paragraph">Perhaps more importantly than the needs of future scholars, the increase in transparency afforded by first author names plays a daily role in how we experience research. First names, in the ethnicities and genders that they suggest, provide an, albeit imperfect, high-level reflection of the diversity of experiences that are brought to research. It is just as important to see ourselves reflected in the outputs of the research careers that we choose to pursue, as the voices that represent us on panels at conferences. This highlights the non-neutral role of technology in the representation of individual-related information.</p>
        <p class="paragraph">The interplay between technological and societal changes, each influencing the other in complex ways, is also evident in the shifting patterns of co-authorship in scholarly papers. Historical analysis (see Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S1.F2" rel="" target="blank">2</a>) reveals a shift in the modal average number of authors from one to gradually increasing numbers of participants over time, reflecting the changing sociology of research. Analysis of the locations of researchers shows the fundamental nature of research community&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib19" rel="" target="blank">19</a>].</p>
        <p class="paragraph">The slow and steady evolution shown in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S1.F2" rel="" target="blank">2</a>&nbsp;in collaboration may also be considered to be a societal parallel for the slow evolution in the nature of the technologies that underpin scholarly communication. The increase in authorship of a paper from a single author (modal average) from the genesis of scholarly communication until around 1960 followed by a shift, relatively rapidly, to five co-authors per paper as a modal average in the present day, gives us a sense of when technological and social changes started to facilitate and change norms. A more detailed analysis of evolution of co-authorship and its importance in addressing more complex problems is dealt with by Thelwall and Maflahi&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib20" rel="" target="blank">20</a>].</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F2-1024x1003.png" alt="" data-id="847510be-6695-4819-9ec2-90e1158ccff2" contenteditable="false"><figcaption class="wp-element-caption">Figure 2:&nbsp;Development of averages of co-authorship numbers on academic output since 1665. The red line shows the modal number of co-authors, the yellow line shows the median number of co-authors, the blue line shows the mean average number of co-authors. The grey shaded area shows the standard deviation from the mean cutoff by the zero axis.</figcaption></figure><p class="paragraph">The 20<sup>th</sup>&nbsp;century witnessed a transformative shift in research practices, fundamentally driven by technological advancements. This period marked the emergence of “Big Science”, a paradigm characterised by large-scale scientific endeavours that required extensive collaboration across disciplines and often, across national borders. The sociological landscape of certain research fields underwent significant changes due to this shift, influencing and reshaping established norms within the scientific community. The impact of Big Science is particularly evident in the evolving patterns of authorship, as collaborations expanded to include hundreds or even thousands of contributors.</p>
        <p class="paragraph">Illustrative of this trend, the growth in the number of research papers featuring extensive co-authorship is striking (see Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S1.F3" rel="" target="blank">3</a>). Papers with more than 100 authors, indicative of large collaborative projects, began to appear with greater frequency, reflecting the complex, interdisciplinary nature of modern scientific inquiries (yellow region) starts in earnest in the late 1970s&nbsp;<sup>1</sup>. This trend is further accentuated in research requiring substantial resources, such as particle physics experiments, where the collective expertise and effort of hundreds of scientists are essential to the project’s success. Papers with over 500 co-authors (blue) and, notably, more than 1000 co-authors (red) have become increasingly common, underscoring the scale of collaboration in certain areas of research.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F3.png" alt="" data-id="b7a14ab3-7d5f-49c6-a981-b1d74127ce41" contenteditable="false"><figcaption class="wp-element-caption">Figure 3:&nbsp;Number of papers in each year with more than 100 (yellow area), 500 (blue area) and 1000 (red area) co-authors respectively.</figcaption></figure><p class="paragraph">From the overarching observations and detailed examples discussed, three key insights emerge that elucidate the evolution of scholarly communication:</p>
        <ol><li data-listnumber=""><p class="paragraph">1.&nbsp;<strong>Gradual Evolution of the Scholarly Record</strong>: The development of conventions around the scholarly record, such as the norm of naming authors on papers, reveals a slow evolutionary process. It highlights that it took nearly two centuries to establish a general expectation for author attribution on scholarly works. This gradual change underscores the inertia inherent in academic traditions and the time it takes for new norms to solidify across the research community.</p>
        </li><li data-listnumber=""><p class="paragraph">2.&nbsp;<strong>Accelerated Changes in Recent Times</strong>: The pace at which the scholarly record has been transforming has markedly increased in recent years. This acceleration is evident in the rapid shift from a median of two authors to five authors per paper across a span of 60 years, a change driven by the increasing frequency, volume, and data size of scholarly outputs. This contrasts sharply with the nearly 300-year period it previously took for such a demographic shift in authorship, highlighting how technological advancements and the expanding scope of collaborative research have expedited changes in scholarly communication practices.</p>
        </li><li data-listnumber=""><p class="paragraph">3.&nbsp;<strong>Reflective Richness of Data</strong>: The data surrounding scholarly communication not only mirrors the evolution of the research community but has, in recent times, also become indicative of the technologies that facilitate this communication. The growing complexity and interconnectedness of research outputs, evidenced by the increasing number of co-authors and the expansion of collaborative networks, reflect broader shifts in the technologies available for research and dissemination. This richness in data offers profound insights into the dynamics of scholarly practices and their evolution over time.</p>
        </li></ol><p class="paragraph">On a practical level, the significance of author names extends beyond mere attribution. In the context of evaluation for promotions, tenure, grant assessments, and many other forms academic recognition, the identity of authors, corroborated by associated identifiers, plays a crucial role. Indeed, research is becoming increasingly centred around quantification&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib22" rel="" target="blank">22</a>]. The trust placed in a particular paper, and by extension, in communities of researchers, is increasingly dependent on the clarity and reliability of authorship information. Furthermore, this information is pivotal in selecting referees, assessing potential conflicts of interest, and facilitating a myriad of other critical academic processes. The evolution of scholarly communication, therefore, not only reflects changes in the academic landscape but also emphasises the growing importance of clear and reliable attribution in maintaining the integrity and trustworthiness of scholarly work.</p>
        <h2>Methodology</h2><p class="paragraph">In the study presented here, we explore the change in the representation of author names in scholarly research articles. As established in the Introduction, we define the terms&nbsp;<em>initial form</em>&nbsp;to refer to an author name where only the initials are supplied and no given name is supplied, and&nbsp;<em>full form</em>&nbsp;to refer an author name that includes at least one fully stated given name. We use&nbsp;<em>Dimensions</em>&nbsp;from Digital Science as the data source for our exploration. At the time of writing&nbsp;<em>Dimensions</em>&nbsp;contains over 144 million publications, of which approximately 112 million are journal articles. Much of the analysis shown in this paper is based on queries that can be run directly on the&nbsp;<em>Dimensions</em>&nbsp;database in the Google BigQuery environment without the need for further coding in, for example, a Python environment and hence are accessible to a broad audience&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib11" rel="" target="blank">11</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib23" rel="" target="blank">23</a>].</p>
        <p class="paragraph">Although we have included several graphs and examples regarding scholarly output before 1945 we recognise that publications volumes are smaller further back in time. Thus, we define two epochs in publication, one before 1945 which we refer to as the&nbsp;<em>anecdotal epoch</em>&nbsp;and the other after 1945 that we call the&nbsp;<em>statistical epoch</em>. It is noteworthy that&nbsp;<em>Dimensions</em>, our primary data source, is limited in the validity of its data for earlier times in the anecdotal epoch. The data source was not constructed with the current use case in mind and hence coverage of content in the 17<sup>th</sup>, 18<sup>th</sup>&nbsp;and even 19<sup>th</sup>&nbsp;centuries is reliant on publisher and community interest in awarding persistent identifiers to research outputs of these periods in order for them to be included in&nbsp;<em>Dimensions</em>. In addition, mappings to institutions that have disappeared or changed their name over the earlier part of the last 350 years may not be fully represented in geographical analyses - similarly, country references will be to modern countries not countries that have existed in the past - we take no account of moving boundaries, merely imposing the current world view back in time for simplicity of analysis.</p>
        <p class="paragraph">Broadly speaking we have set out not to use statistical methods or to take inferences based on data in the anecdotal epoch - such figures that depict these data are intended to be illustrative, sometimes highlighting the limitations of&nbsp;<em>Dimensions</em>&nbsp;for the period, and at other times to add anecdotal colour and context to our discussions and arguments. Data concerning the statistical epoch, is more trustworthy both from a structural perspective (systems existed to more completely capture the scholarly record and it is a period during which there remains significant academic interest in reading and making reference to the material, thus the literature of this period has much better coverage of persistent identifiers) as well as from a volume perspective. Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S1.F2" rel="" target="blank">2</a>&nbsp;shows the growth in volume of publications (using the restrictions specified in Listing&nbsp;1).</p>
        <p class="paragraph">In terms of both the quality and volume of the data, the transition from the anecdotal to the statistical epoch is not sharply defined but happens gradually. However, our choice of 1945 is not completely arbitrary. The current authors have previously noted that 1945 was the year in which the centre of mass of global research stopped its march toward North America and began to move back toward Europe&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib11" rel="" target="blank">11</a>]. It was also the year in which Vannevar Bush published the Endless Frontier&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib24" rel="" target="blank">24</a>]&nbsp;and, as such, is a good point in time from which to date the modern approach to research. Indeed, both Figs&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S1.F3" rel="" target="blank">3</a>&nbsp;and&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S2.F4" rel="" target="blank">4</a>&nbsp;show a marked change in behaviour between 1940 and 1950.</p>
        <p class="paragraph">On a technical note, during the&nbsp;<em>Dimensions</em>&nbsp;data ingestion process author details are gathered from across the available sources that&nbsp;<em>Dimensions</em>&nbsp;draws on. Each author has a last name and a first name, a unique identifier assigned within&nbsp;<em>Dimensions</em>&nbsp;(where it is possible to determine), a list of institutional affiliations mapped to unique identifiers (GRIDs, where resolvable), an ORCID (again, where resolvable) that have been asserted in relation to a given output, and a marker of whether the author is listed as a corresponding author. For the majority of publishers this is delivered via a JATS XML feed, but smaller publishers or those without the technical capability to deliver JATS XML, Crossref data is used or, with the permission of the publisher, data is crawled directly from their website. Once cleaned and enhanced in the&nbsp;<em>Dimensions</em>&nbsp;data pipeline, the data are parsed into the&nbsp;<em>Dimensions</em>&nbsp;database and loaded into Google BigQuery.</p>
        <p class="paragraph">Each analysis that we present takes either an author-centric or paper-centric view. That is to say we either examine the proportion of papers published in a year with certain characteristics or we examine the characteristics of authors that have published in the year directly. As a side note, we observe that the active publishing community is a reasonable proxy for the global research population but at each point in the timeline that we engage with, there are good reasons why this cannot be taken to be more than a representative sample. For example, in the early years of the scholarly record (circa 1665) publication was a new activity that was not engaged with by every academic and social structures of the time tended to exclude women. On the other hand, in the present day, a significant proportion of global research takes place in proprietary settings and hence is not published in the scholarly record. Thus, all the comments that we make in the paper need to be taken within these constraints in mind.</p>
        <p class="paragraph"><code>--&nbsp;RAW&nbsp;table:&nbsp;Split&nbsp;the&nbsp;first&nbsp;name&nbsp;string&nbsp;into&nbsp;sections&nbsp;and&nbsp;prepare&nbsp;the&nbsp;first&nbsp;three&nbsp;for&nbsp;analysis,&nbsp;gather&nbsp;other&nbsp;key&nbsp;locationalinformation&nbsp;that&nbsp;we&nbsp;might&nbsp;need&nbsp;later. SELECT&nbsp;p.id, p.year, author.researcher_id, ady.country_code, ady.grid_id, author.first_name,&nbsp;author.last_name, LENGTH(author.first_name)&nbsp;total_length, LENGTH(SPLIT(author.first_name,’&nbsp;’)[SAFE_OFFSET(0)])&nbsp;var1_one_length, STRPOS(SPLIT(author.first_name,’&nbsp;’)[SAFE_OFFSET(0)],".")&nbsp;var1_one_dot, LENGTH(SPLIT(author.first_name,’&nbsp;’)[SAFE_OFFSET(1)])&nbsp;var1_two_length, STRPOS(SPLIT(author.first_name,’&nbsp;’)[SAFE_OFFSET(1)],".")&nbsp;var1_two_dot, LENGTH(SPLIT(author.first_name,’.’)[SAFE_OFFSET(0)])&nbsp;var2_one_length, STRPOS(SPLIT(author.first_name,’.’)[SAFE_OFFSET(0)],".")&nbsp;var2_one_dot, LENGTH(SPLIT(author.first_name,’.’)[SAFE_OFFSET(1)])&nbsp;var2_two_length, STRPOS(SPLIT(author.first_name,’.’)[SAFE_OFFSET(1)],".")&nbsp;var2_two_dot, LENGTH(SPLIT(author.first_name,’’)[SAFE_OFFSET(2)])&nbsp;var1_three_length, LENGTH(SPLIT(author.first_name,’.’)[SAFE_OFFSET(2)])&nbsp;var2_three_length, FROM&nbsp;dimensions-ai.data_analytics.publications&nbsp;p, UNNEST(authors)&nbsp;author LEFT&nbsp;OUTER&nbsp;JOIN&nbsp;UNNEST(author.affiliations_address)&nbsp;ady WHERE&nbsp;ARRAY_LENGTH(authors)&nbsp;&lt;&nbsp;50&nbsp;and&nbsp;p.type=’article’</code></p>
        <p class="paragraph">Listing&nbsp;1:&nbsp;Core SQL Query for Dimensions on Google BigQuery that takes the name string and begins the process of allowing the classification of names into initial form or full form.</p>
        <p class="paragraph">It would be a simple matter to test the first name to see if it is of length one or two characters (either an initial or an initial and a “.”). However, this approach would miss people with multiple initials or with a two character first name such as “Bo”. Hence, we use a slightly more complex approach were we examine the first three segmented tokens in the first name field in&nbsp;<em>Dimensions</em>. We test for the length of the field, the position of the character “.” . This allows us to correctly classify edge cases such as authors that are known principally, and hence who publish by, their second name such as J. Robert Oppenheimer (see, for example&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib25" rel="" target="blank">25</a>]).</p>
        <p class="paragraph">A further complexity is that, in many cases, names are preserved in both their original and transliterated forms in&nbsp;<em>Dimensions</em>. In these cases a whole name is a single character, for example in Kanji or Katakana (see, for example&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib26" rel="" target="blank">26</a>]) and hence a whole name can be mistaken for an initial. Fortunately, this problem is a small one in the context of the current work since, as of the date of writing, only 250,991 of 8,456,140 publications in Chinese (or around 3%) have the potential to contain Kanji characters, that might have have been transliterated into English. There is a potentially bigger issue for Japan in that 1,152,602 of 4,460,000 Japanese language publications (or around 25%) have the potential to contain Kanji or Katakana characters without transliteration. However, deeper examination suggests that just 176,177 of 2,741,338 authors (or around 6.5%) with a Japanese address have a single character first name - which may legitimately be a transliterated first initial or a Japanese character that could be misclassified as an initial. While this number is not insignificant, we only wish to pick out high-level trends in the data in this article and hence this is negligible. We have not commented on how the percentage of Chinese language and Japanese language publications requiring translation has changed with time, however, this point is currently moot as global research has been dominated by countries with roman alphabets in the past and, again, for an analysis that requires only a high-level trend, it is reasonable to assume that the variance introduced by this effect does not change the overall appearance of the line.</p>
        <p class="paragraph">We have limited the query in Listing&nbsp;1 to include only outputs that are classed as papers, since books and conference proceedings have different qualities and characteristics that we don’t wish to cloud the current discussion. We have also removed papers with more than 50 co-authors as these papers also have different characteristics.</p>
        <p class="paragraph">We have built further aggregations on top of the core table provided by Listing&nbsp;1 to support the other analyses presented in this paper. Once the data are presented in the format from Listing&nbsp;1 we perform aggregations that give a determination of the names of authors at both an author and a paper level. In the case of an author we classify their name as short form or long form and we classify a paper as initial form (for all co-authors), full form (for all co-authors), “Mixed” for mixtures of the prior two types or “Undetermined” when we can’t map names to either short or long form categories. The structure of Listing&nbsp;1 allows us then to study how the data change with time, with subject category and with country.</p>
        <p class="paragraph">In exploring the issue of gender we used genderize.io under licence. The genderize.io tool provides a dataset with probabilities that a given name is statistically associated with either men or women, or “undetermined” on both a nation-neutral and national basis. The tool is well-used in the study of gender in academia&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib27" rel="" target="blank">27</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib28" rel="" target="blank">28</a>]. This is a statistical analysis and hence cannot attempt to access the subtle and complex issues of gender identity, but rather it focuses on how gender is represented in the historical scholarly record. We have employed a methodology where we matched name variations in author names in&nbsp;<em>Dimensions</em>&nbsp;to the results obtained for that name variation in genderize.io. The results were stored in a private table in Google BigQuery. We then matched author names, where given, to data in this table. We used the current university of the academic to provide nationality information and, where data was either unavailable or indeterminate, we defaulted to using data without a national context. Obviously, this approach is open to errors in that, especially in more recent years, researchers are much more likely to travel during their careers and hence their current university may not be culturally relevant for such an analysis. Many names are also used much more internationally now and hence those that are traditionally nationally associated may not be used in context. It is also the case that some researchers will originate from countries where a Western name is adopted for business purposes. In this final case, the names are often more central to common usage as they are designed to mimic Western-style naming conventions and may even improve the data. The point of this paper is not, however, to do a detailed gender analysis and we believe that our approach is sufficiently robust to be serviceable in the current context.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F4-1024x869.png" alt="" data-id="9c1bbfc3-3fdc-4172-aeac-3d264e0181f5" contenteditable="false"><figcaption class="wp-element-caption">Figure 4:&nbsp;Volume of scholarly publication using the restrictions specified in Listing&nbsp;1 for comparability with other plots in the paper. This plot shows a clear cutoff around 1940 at which point there is sufficient data for the reasonable interpretation of statistical approaches.</figcaption></figure><h2>Results</h2><h3>Paper-based analysis</h3><p class="paragraph">As indicated in the introduction, the historical use of names in the scholarly record has been uneven and may, to a certain extent, reflect societal norms. While, in the early years of&nbsp;<em>Dimensions</em>’ coverage, the volumes of data are small, Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F5" rel="" target="blank">5</a>shows that until around 1800 it appeared to be normal to use one’s full name in scholarly correspondence (despite the fascinating cases discussed in the introduction). It is also important to acknowledge the lens through which we are examining the past is imperfect. The work of Lefanu&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib29" rel="" target="blank">29</a>]&nbsp;makes it evident that there was a lively and thriving research discussion prior to 1800, however, records have either not been preserved or not been transferred into digital realms, meaning that they are invisible to our study.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F5-1024x852.png" alt="" data-id="0108483f-90ff-4faa-aad8-a2859c9c9e88" contenteditable="false"><figcaption class="wp-element-caption">Figure 5:&nbsp;Proportion of papers in which all authors state their names using initials (“Initial form %” - red line), versus all authors stating their full names (“Full form %” - blue line) versus some authors stating their full name while others state their initials (“Mixed %” - green line. “Undetermined %” includes edge cases that we have not programmed for including no first name or no name at all. We note four distinct eras of behaviour: The Developmental Era tracks from the genesis of scholarly publication until 1945; The Initial Era from 1945 to 1980; and the Modern Era from 1980 to present day.</figcaption></figure><p class="paragraph">Figure&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F5" rel="" target="blank">5</a>&nbsp;can be broken into three eras, with one phase aligned with the anecdotal epoch, and the second and third phases in the statistical epoch:</p>
        <ul><li data-listnumber=""><p class="paragraph">We define the “Developmental Era” as the period from 1665 to 1950, that appears to be split into two sections, separated by a sharp transition that occurs in 1798. As we will explain, the sharpness of this distinction is a result of the data in&nbsp;<em>Dimensions</em>&nbsp;(and of the systems on which&nbsp;<em>Dimensions</em>&nbsp;is based–specifically, systems that are based around the attribution of DOIs to academic articles&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib30" rel="" target="blank">30</a>]). Thus, we must regard the data reported until 1798 as unrepresentative of the actual development of scholarly publication and only anecdotal in nature.<br><br>In the period until 1798 the Dimensions data contains just a few regularly publishing journals with total global number of articles being published reaching 288 in 1798. Up until that year, the journals with the largest publication volumes were Philosophical Transactions of the Royal Society of London, Transactions of the Linnean Society of London and Transactions of the Royal Society of Edinburgh, each of which were publishing around 30 articles per year and all of which had adopted a standardised title page, typically printed the names of authors in full form. But, in 1798,&nbsp;<em>The Philosophical Magazine: A Journal of Theoretical, Experimental and Applied Physics</em>&nbsp;was established by Alexander Tilloch of the London Philosophical Society, publishing 142 articles; the following year, in 1799, an even more voluminous journal launched as&nbsp;<em>The British and foreign medical review</em>&nbsp;was launched by the Royal College of Physicians, publishing 327 articles. Hence, there was a sharp and significant increase in the number of articles in&nbsp;<em>Dimensions</em>. These are the articles in publications that scholarly societies and other participants in the scholarly ecosystem decided were sufficiently valuable (and where means were sufficient) to clean up metadata and make DOIs or PubMed identifiers available. In the two significant journals mentioned here, author name forms are mixed in both journals with some articles being editorial (without author names), but while full form names still appear regularly, initial form becomes much more acceptable and this becomes established as a dominant signal entering the 1800s. Lefanu provides a for more detailed treatment of the rise of medical journals&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib29" rel="" target="blank">29</a>].<br><br>In the years following 1800 the numbers of articles increase in volume to a level where some types of statistical analysis are appropriate, so long as the data are not filtered and segmented too finely. By 1823, the total, global number of journal articles tracked in&nbsp;<em>Dimensions</em>&nbsp;is consistently above 1000 (the level of a mid-sized research university today).<br><br>It is worthy of note that throughout the whole of the Developmental Era there a low level of co-authorship and hence, we remain in Adams’ first age&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib19" rel="" target="blank">19</a>]. As we see in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S1.F2" rel="" target="blank">2</a>&nbsp;it is not until 1958 that the modal average of authors on a paper moves from single-authorship to two co-authors. <br><br>During this period scholarly communications are published in early versions of scholarly journals operated by early scholarly societies, with an intended audience of members, almost as we would treat newsletters today. Personal connections and members would often have been known to each other due to the small size of these early communities. Several innovations took place in this period, however, including the introduction of a somewhat standard title page to communications.<br><br>In 1798, at the transition point, only 179 papers were published (as tracked by&nbsp;<em>Dimensions</em>&nbsp;under the constraint of Listing&nbsp;1), but in 1799, 517 papers were published - a tripling of output. This significant increase in output appears to be coupled to the beginning of the proliferation of scientific journals. Although it is difficult to see the detail of this development in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F6" rel="" target="blank">6</a>, which shows that there are currently around 70,000 actively publishing journals, in 1797 just four journals are recorded as active in publication in&nbsp;<em>Dimensions</em>&nbsp;(until just 1780, more than a century after the beginning of this movement, there had only been one or two active regularly publishing journals at any point in time, with 1781 being the first year when three journals published). From 1799, seven journals were regularly publishing. By 1827 this had become 19 and by 1837 it was 30. This part of significant growth began to see the development of norms and standards that needed to reach across journals and the community started to become large enough that the prior “gentlemanly” approach was no longer sufficient to handle the level of publication volumes and the community which needed to share in the knowledge being generated.<br><br>During this period, the level of formality in the use of initial form versus full form in papers appears to have increased spontaneously and significantly.</p>
        </li><li data-listnumber=""><p class="paragraph">The “Initial Era” began in the wake of the second world war, the dawn of Bush’s Endless Frontier&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib24" rel="" target="blank">24</a>]&nbsp;in 1945, and continues until 1980. It is the singular period in the history of the scholarly record in which the initial form dominated the full form of author names. The initial era is entirely contained in the statistical epoch and hence there are sufficiently many journals and papers that a statistical approach to analysis is valid.</p>
        </li><li data-listnumber=""><p class="paragraph">The “Modern Era” has persisted from 1980 to the present day. It is an era in which we have seen the norm move steadily and rapidly toward the dominance of the use of full form names.</p>
        </li></ul><figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F6-1024x694.png" alt="" data-id="415b9db1-3b16-4de0-84e6-143998df5aa1" contenteditable="false"><figcaption class="wp-element-caption">Figure 6:&nbsp;Development in the number of actively publishing journals.</figcaption></figure><h3>Researcher-based analysis</h3><p class="paragraph">In Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F5" rel="" target="blank">5</a>&nbsp;we see the proportion of papers in which authors used initial form. In this section we ask a slightly different but related question, which is: How many authors used initial form? While one might expect this to be an identical question, and while it is related, there is a fundamental difference. Papers as objects exist in a world of journals, subject norms, publisher house styles and technological constraints. While each of these affects the authors, they have strong ties to national, cultural and linguistic effects. Thus, analyses of people rather than papers do add to our understanding of the developments shown above.</p>
        <p class="paragraph">Figure&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F7" rel="" target="blank">7</a>&nbsp;shows the globally conglomerated author view of the use of initial form on journal articles—it averages across national, cultural, gender, technological and disciplinary boundaries. We will decompose this plot in a variety of ways through the rest of the Results section, so it is important to gain familiarity with its overall shape and its key features. It is worthy of note that there is a an initial steep incline from 1945, marking the beginning of the Initial Era. The norm of initial-form dominance is maintained around 50% until around 1975 when the form starts being replaced by full form names. There is a rapid decrease in popularity of initial form between 1985 and 2000 and then, in around 2002 a precipitous drop, followed by a continued decline to around the 6% level is seen. It looks likely that the current decline will continue until initial-form is expunged entirely (or almost entirely).</p>
        <p class="paragraph">Had we plotted Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F7" rel="" target="blank">7</a>&nbsp;into the past, we would see that long-form names not only dominated the paper-based view but also the researcher-based view of publication in the anecdotal epoch.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F7-1024x595.png" alt="" data-id="da1b5d77-42a5-450c-9845-a00c48484858" contenteditable="false"><figcaption class="wp-element-caption">Figure 7:&nbsp;Proportion of authors publishing with initial form names from 1945 to 2023.</figcaption></figure><h3>Geographical analysis</h3><p class="paragraph">The data in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F7" rel="" target="blank">7</a>&nbsp;are averaged across all countries but, over the time period that we are examining, not all countries participate in the research enterprise equally. Thus there is an implicit weighting in the global picture toward specific countries. If we speak about cultural, political, linguistic or geographic trends then it is important to understand the level of participation of different countries to an international average. As such, Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F8" rel="" target="blank">8</a>&nbsp;reveals the contribution of each country to global paper production rates in&nbsp;<em>Dimensions</em>. The graph is produced by taking each paper and partitioning it by the countries of the affiliations of each author on the paper and then summing these contributions over all journal articles published in each year. In the plot, we have only plotted the top 13 countries and have conglomerated under the title ”Rest of World” as a 14<sup>th</sup>&nbsp;participant. As we can see from the Figure, during the period from 1945 to 1990, an extremely high percentage of global research output emanated from the US and the UK. While Japan and Germany make a significant and sustained contribution from the mid-1950s, between 50% and 60% of global output in the 1970s and 1980s came from the US and the UK - English-speaking countries with a somewhat shared cultural base. Thus, the behaviours summarised in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F7" rel="" target="blank">7</a>&nbsp;are dominated by an Anglo-American societal behaviours. However, as we see in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>&nbsp;cultural alignment between the UK and the US may be more divergent than expected.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F8-988x1024.png" alt="" data-id="e9e95928-7c73-4b45-96cc-0c5dbe06b67b" contenteditable="false"><figcaption class="wp-element-caption">Figure 8:&nbsp;Percentage annual contribution to research journal publications by country of affiliation of author from 1945 to 2023.</figcaption></figure><p class="paragraph">Nonetheless, each country has its own social norms and traditions. Some countries are sufficiently large or diverse that there are different norms within the country while other countries have kinship with neighbours and there is some homogenisation between cultures. In our analysis here, we see some of these facets emerge naturally from the data.</p>
        <p class="paragraph">The following three figures (Figs&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F10" rel="" target="blank">10</a>&nbsp;and&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F11" rel="" target="blank">11</a>) show the proportion of authors publishing their name using initial form rather than full form—in each case around 25 countries are plotted (countries are selected to appear in the plot when they consistently participate in more than 500 papers per year) in grey with specific subsets picked out in colour.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F9-810x1024.png" alt="" data-id="4d302510-4ba5-4807-933e-1be3738f5eec" contenteditable="false"><figcaption class="wp-element-caption">Figure 9:&nbsp;Proportion of papers only displaying initial-form names broken down by countries of author affiliations from 1945 until present day. The grey dotted lines show all countries in the list for context, selected Western countries are picked out in colour. Fractional apportionment of papers to countries has been applied.</figcaption></figure><p class="paragraph">In Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>&nbsp;Australia, France, Germany, Spain, the United Kingdom and the United States are picked out in colours. We see that the United States is the country that most consistently leads on use of full form rather than initial form name forms on papers. Interestingly it shows a sharp jump in formality during the late 1980s followed by a slow reversion to its prior behaviour, ended by an almost symmetric sharp drop in the early 2000s. However, at all points, the US is the “familiar” country of the Western world. Australian authors have been the second most familiar since the late 1970s, having previously been slightly closer to their cousins in the United Kingdom, who having originally led on formality in this group in the 1940s through to the 1970s, started to be much more central in the group. While the Germans were highly informal in the 1940s, their increase in use of initial form rose significantly in the 1950s and 60s but has since settled close to the middle of this group. Finally, French authors have been probably more consistent than others in their embrace of the initial form, resulting in their moving from being one of the least formal to the most formal in the group while staying at more or less the same level of initial form usage between 1945 to 1990. They have then been consistently the highest user of initial form since the mid 1970s to present day. Of course, for all members of this group there has been a significant harmonisation over the last 20 years with extremely low levels of initial form usage in place today.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F10-810x1024.png" alt="" data-id="fccc0df8-11d9-4461-a1cd-20e34e960d15" contenteditable="false"><figcaption class="wp-element-caption">Figure 10:&nbsp;As Figure&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>&nbsp;with countries in the Asia-Pacific region and Brazil selected.</figcaption></figure><p class="paragraph">In Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F10" rel="" target="blank">10</a>&nbsp;we have selected countries from the Asia-Pacific region and Brazil. In this case we see clearly the formalism (initial form alignment) of India from 1945 to present day with it being consistently (and in much of this time period significantly) above its comparators. Brazil passed the production threshold for this analysis in the late 1960s, at which point it was already more aligned to full name usage than most of the rest of the world (below most lines in the grey background), and continues to be even today. China, Japan and South Korea have all consistently been amongst the countries least likely to use initial form for the longest period of time. The facet in common for these countries is their use of non-Roman writing systems. Thus, in native language, author names are often single character, but when translated into English as choice is made to list the name as as a full name rather than just as a single initial. This may, in part, be due to cultural and linguistic factors in many Asian countries that give rise to common last names, the lack of a second or third given name, and hence the need to state full first names in order have the ability to disambiguate authors in a pre-ORCID environment&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib31" rel="" target="blank">31</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib32" rel="" target="blank">32</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib33" rel="" target="blank">33</a>]. All these issues are compounded by transliteration in the scholarly record.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F11-857x1024.png" alt="" data-id="a0e5d532-293c-49a0-9a95-196930dd5e4a" contenteditable="false"><figcaption class="wp-element-caption">Figure 11:&nbsp;As Figures&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>&nbsp;and&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F10" rel="" target="blank">10</a>&nbsp;with selected Slavonic-language countries picked out.</figcaption></figure><p class="paragraph">In Figs&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>&nbsp;and&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F10" rel="" target="blank">10</a>, it is notable that most countries that we reviewed have, at least in recent times, been below the modal behaviour regarding initial form usage. Thus, in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F11" rel="" target="blank">11</a>&nbsp;we explore the more formal part of this diagram, picking out Slavonic stages. Not all countries in this group speak a strictly Slavonic language, specifically, Romanian is an Eastern Romance language. Some countries in the group use the Roman alphabet while others use the Cyrillic alphabet, however, both of these alphabets are structured in a way so as to allow the delineation of first names and initials in a way that some of the Asian writing systems do not. However, the shared history of these countries (with the exception of Croatia) is that they formed part of the the orbit of the USSR before its fall in 1991.</p>
        <p class="paragraph">A characteristic of these countries was the establishment of strong national academies, which may have led to greater harmonisation of norms of research culture at a national level. What is less clear is whether this is the core effect as this could also be due to pre-existing (i.e. non-research-specific) cultural and linguistic alignments. The data in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F11" rel="" target="blank">11</a>&nbsp;suggests that these countries have tended to be more formal in their use of initial form rather than full form in research discourse. Isolating the effect of national academies in research culture agenda-setting is beyond the scope of the current work. However, we conjecture that this may be a factor in determining behaviour. In the UK, the diffuse nature of the national academy (being split between the British Academy, the Royal Society and the Royal Academy of Engineering) has tended to mean that universities are more independent and powerful in their own right; in the US a similar diffuse system exists with a much more powerful university sector; and, in Germany the Max Planck Society, the Leibniz Association, Fraunhofer Society, Helmholtz Association and others provide a diverse setting for research culture to develop. Whether France’s more formal preference is due to linguistic structure or administrative structure is, again, hard to disinter—one may, after all, be a consequence of the other.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F12-924x1024.png" alt="" data-id="b57988ff-e6d7-47e1-81ae-a1fd54725f0f" contenteditable="false"><figcaption class="wp-element-caption">Figure 12:&nbsp;Proportion of papers published displaying only initial-form authors in 2020 versus power distance for a selection of countries. The size of each disk represents the cumulative volume of publications associated with the country between 1945 and 2023. Each disk is coloured according to the linguistic grouping of the principal language spoken in the country of the author’s affiliation.</figcaption></figure><p class="paragraph">Overall, over the last 20-year period we see a similar rate of harmonisation toward a more familiar norm. However, it is interesting to note a few of the journeys that we see in these data. For instance, it is noteworthy that Ukraine’s curve mirrors that of Russia throughout the period that we studied until circa 2014. Likewise, it is interesting to note that Belarus has continued to be consistently formal in its name format.</p>
        <p class="paragraph">Europe is a particularly challenging setting for analysis of this nature due to the richness of the history of the region with significant changes in borders and definitions of countries over the period in which we are interested. In the Initial Era, some stability prevails but this is set against a backdrop of multinational history in which many countries in Europe have sub-populations of differing language and ethnicities - in some sense countries are an artificial construct. And yet, they are also the basis of national academies, evaluation systems, and systems of funding. Yet, at the same time, the Initial Era finds its genesis coincidentally at the time when the Marshall Plan was rebuilding Europe. A time of austerity for many countries in Europe, when there may have been a propensity toward more formal styles.</p>
        <p class="paragraph">Beyond this, countries are also the places of education and the level of formality is instilled through institutions of learning and higher learning. Another confounding factor in our analysis is that in a more globalised world researchers are more mobile. Thus, the academic affiliation on their papers may not be indicative of their cultural, linguistic or educational background. Even more challenging, with the rise of international collaboration, it is unclear whether researchers will naturally import the style of other researchers with whom they have worked, who may not share their cultural background but who may convince them to adopt a different cultural norm.</p>
        <p class="paragraph">Our data do seem to suggest some level of cultural, historical, linguistic or political coupling. For example, we see the close mirroring of practices between Ukraine and Russia until 2014, when political developments may have had cultural consequences, on the other hand we see a close relationship between Russia and Belarus seemingly not altering Belarus’s more conservative approach to publishing with a preference to initial form. Indeed, Belarusian-affiliated authors appear to be the sole example that do not follow the overwhelming international norm toward the use of full name on publications. However, it is very difficult to draw any solid conclusions that are not purely anecdotally-motivated from these data.</p>
        <p class="paragraph">In an attempt to gain further insight, we explore the interplay of linguistic and cultural effects further by turning to the work of the cultural theorist Hofstede from the late 1960s. Through his work at IBM, Hofstede developed Cultural Dimension theory&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib34" rel="" target="blank">34</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib35" rel="" target="blank">35</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib36" rel="" target="blank">36</a>]&nbsp;in an attempt to understand how cultural considerations affected organisational structures. One of the cultural dimensions that Hofstede developed, called the Power Distance Index, attempts to quantify the extent to which the less powerful members of organisations and institutions accept and expect power to be distributed unequally. As such, Power Distance may be thought of as a proxy for how formal a country is culturally in a professional setting. Of course, such things change with time and the Power Distance shown here is a snapshot at a particular point in time. We argue, however, that these things do not change as quickly from a cultural perspective as they do from a technological perspective.</p>
        <p class="paragraph">Figure&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F12" rel="" target="blank">12</a>&nbsp;plots the proportion of authors using an initial form aggregated at a country level against Hofstede’s Power Distance for that country. If there were to be a direct positive correlation between Power Distance and use of the initial form of the name on papers, then we would see country disks clustered close to a line that starts low and which gradually slopes upward to the right. However, in our plot we see little to no correlation. We see that most countries (including US, Canada, Japan, China et al.) in our selection are less formal (lower incidence of initial form) than the power distance in their country; while a few countries are just above the line (Australia, Germany, Hungary et al.), showing that they are slightly more formal than the power distance in their country suggests. Indeed, no country is significantly more formal in its usage of initial form then their power distance would indicate.</p>
        <p class="paragraph">Countries with similar languages have similar power distances but there is little correlation with the proportion of authors using initial form. Indeed, countries with a first language of English have a Power Distance of around 0.4 but spread from 25% to 50% in their use of initial form; on the other hand Romance-language countries are associated with a wide range of power distances (50 for Italy to 90 for Romania) and yet there is a fairly consistent initial form percentage around 40%. However, Belarus, Russia and Ukraine are notably more formal (high power distance) and exhibit higher percentages of initial form usage (c. 70%) than other countries.</p>
        <p class="paragraph">This suggests that formality of the cultural in which a piece of research is carried out is not a dominant factor in the adoption of initial form as a publication form. Indeed, since research is typically a highly collaborative endeavour we see people from different nationalities and cultures doing their work in different cultures; furthermore we see collaboration on single papers between different cultures and different locations. Nor does language or size of research output appear to have a particular effect. Nonetheless, the curves in Figs&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F10" rel="" target="blank">10</a>, and&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F11" rel="" target="blank">11</a>, share similar features and overall shapes in many cases - all the curves trend upward to a more formal style in the mid-1980s, all the curves fall back to less-formal levels after a peak around 1990 and many countries see a precipitous drop in formality in 2000. This similarity in shape suggests that there is a more fundamental underlying driver than a sociological one.</p>
        <h3>Disciplinary Analysis</h3><p class="paragraph">Of course, sociology may differ by subject area more powerfully than by geography. When we grow up, we are taught certain cultural norms and practices, but the practices that we have for engaging in a research communication context are acquired at university either during undergraduate or postgraduate studies, depending on the field. In the post-1945 that we are investigating, university education had already become quite harmonised globally. With the world’s “first-mover” research nations had controlled and established a set of norms that new entrants have sought to emulate in order to engage on the same footing. (Figure&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F8" rel="" target="blank">8</a>&nbsp;demonstrates the extent to which research would, de facto, take place in the English language and in the format chosen by the US, which in and of itself have been heavily influenced by its UK/European ancestry.) This included adoption of Bacon’s scientific method, the concept of the Humbolt institution and the format of scholarly communication in terms of the book and the journal article. In light of this harmonisation it is less surprising that we see correlations with national attitudes in publication practices and that it is more likely that we should see correlations elsewhere.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F13-811x1024.png" alt="" data-id="e7c7fe92-abac-45bb-94a0-0b1895cb80b8" contenteditable="false"><figcaption class="wp-element-caption">Figure 13:&nbsp;Evolution of contribution of different fields to the overall academic corpus with time. Attributions of papers to fields as per ANZSRC Field of Research Codes from&nbsp;<em>Dimensions</em>.</figcaption></figure><p class="paragraph">Figures&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F14" rel="" target="blank">14</a>&nbsp;and&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F15" rel="" target="blank">15</a>&nbsp;show analogous plots to Figs&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F10" rel="" target="blank">10</a>, and&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F11" rel="" target="blank">11</a>&nbsp;but instead of breaking down the initial form percentage into country contributions, the basis has been changed to examine subject areas. In this case we make use of the 2020 ANZSRC Field of Research Codes that are automatically assigned to publications in&nbsp;<em>Dimensions</em>&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib37" rel="" target="blank">37</a>]. The plots must still, in aggregated form, reduce to Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F7" rel="" target="blank">7</a>&nbsp;as was the case for the sum over all paths in the country-based plots and hence Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F13" rel="" target="blank">13</a>&nbsp;shows the proportion of papers written in each of the subject areas and hence shows us the contribution level of each subject to the aggregate (or, the effective weighting of each line in each plot).</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F14-810x1024.png" alt="" data-id="5341fe7b-4eff-4129-bce1-87e7645ab49a" contenteditable="false"><figcaption class="wp-element-caption">Figure 14:&nbsp;Proportion of journal articles in which authors only use initial form assigned to ANZSRC FoR-subject by year from 1945 to 2022. Selected Science, Technology, Engineering and Medicine (STEM) subjects are picked out in colour. Fractional apportionment is not applied - there is duplication of counting if papers cross 2-digit-FoR classifications. Fields are determined via&nbsp;<em>Dimensions</em>automated attribution to ANZSRC 2020 coding.</figcaption></figure><p class="paragraph">In the first subject-based Fig.&nbsp;&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F14" rel="" target="blank">14</a>, we have chosen to highlight STEM subjects. It is instantly noteworthy that these subjects cluster to the top of the graph, indicating a greater use of the initial form in the publications. The physical sciences appear to have the highest adoption of this format with information and computing sciences have the lowest level—both areas consistently hold these positions. Again, all fields have a similar overall shape, peaking in the late 1970s and early 1980s, since which the use of the initial form standard has declined. Interestingly, while the precipitous drop in 2000 still exists for some subjects the way that it is in the country data, a variety of fields do not show this feature, including&nbsp;<em>Physical Sciences</em>,&nbsp;<em>Agricultural, Veterinary and Food Sciences</em>, and&nbsp;<em>Engineering</em>.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F15-810x1024.png" alt="" data-id="d30554e7-cb0b-4b59-8a46-233aa396ee73" contenteditable="false"><figcaption class="wp-element-caption">Figure 15:&nbsp;Proportion of journal articles in which authors only use initial form assigned to ANZSRC FoR-subject by year from 1945 to 2022. Selected Social Sciences, Humanities and the Arts for People and the Economy (SHAPE) subjects are picked out in colour. Fractional apportionment is not applied - there is duplication of counting if papers cross 2-digit-FoR classifications. Fields are determined via&nbsp;<em>Dimensions</em>&nbsp;automated attribution to ANZSRC 2020 FoR coding.</figcaption></figure><p class="paragraph">The second subject-based figure focuses on the SHAPE disciplines. With the exception of Built Environment and Design and, in a brief period form 1990-2000, for Philosophy and Religious Studies, all these fields exhibit lower levels of use of the initial form. As with the STEM-subject view, we see the 2020 precipitous change in the use of the initial form format is only significantly visible in Philosophy and Religious Studies, with Psychology showing this change to a lesser extent.</p>
        <p class="paragraph">In C. P. Snow’s Tale of Two Cultures&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib38" rel="" target="blank">38</a>], he argues that Social Sciences, Arts and Humanities (which we will refer to as the SHAPE disciplines) have a different culture to Science, Technology, Engineering and Medicine (which we will refer to as the STEM disciplines), and this appears to be born out in our data.</p>
        <p class="paragraph">With the exception of&nbsp;<em>Built Environment</em>&nbsp;and,&nbsp;<em>Philosophy and Religious Studies</em>&nbsp;for a brief period in the 1990s, the SHAPE disciplines lie below the STEM disciplines consistently in their use of initial form. When we take account of the weighting of volumes in different disciplines shown in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F13" rel="" target="blank">13</a>&nbsp;then it is clear that the average of disciplines will be weighted toward the disciplines of&nbsp;<em>Biomedical and Clinical Sciences</em>&nbsp;and&nbsp;<em>Health Sciences</em>&nbsp;and&nbsp;<em>Biological Sciences</em>. All three areas are aligned with medicine and hold a relationship to the formality associated with healthcare professionalism. At the same time they are three of the largest disciplines by number of outputs, accounting for 40%-50% of global publications.</p>
        <h3>Journal analysis</h3><p class="paragraph">It is of course impossible to consider the evolution of articles, without also considering the evolution of practices at the journal level. Journals have historically been associated with scholarly societies that are disciplinary in nature and have embodied communities and have been reflections of their practice. Not only this, journals are the wrapper for article publications with journal management teams having responsibility for technological choices such as how the move from print to online was accomplished and when it took place. In the digital era, they have also held the choice of specific platforms and hence have had their options defined for them by suppliers of these technologies. Not limited to the digital era of publishing, journal editors have made choices such as the adoption of policies such as house styles which opine on the use of grammar, and of author name styling. Understanding these effects in situ is a key input to further enhance our picture of the landscape.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F16-1024x585.png" alt="" data-id="8cd57ccc-d1cf-41f3-a8b3-2419033ac5aa" contenteditable="false"><figcaption class="wp-element-caption">Figure 16:&nbsp;Proportion of initial form usage in three journals, Journal of Biological Chemistry, Tetrahedron and The BMJ from 1950 to 2022.</figcaption></figure><p class="paragraph">Figure&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F16" rel="" target="blank">16</a>&nbsp;shows the changes that we see in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F12" rel="" target="blank">12</a>&nbsp;at the micro level. The examples chosen in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F16" rel="" target="blank">16</a>&nbsp;are well documented and hence we can see some of the changes that took place through the Initial Era with more of the detail.</p>
        <p class="paragraph">In the mid nineties, Highwire press emerged as a significant player in the online hosting of Journal content. Starting with the Journal of Biological Chemistry, Highwire was at the forefront of re-imagining the definition of a paper from a digital ‘photocopy’ of a physical paper, Highwire’s technology allowed the paper to be a fully interactive digital object in a contemporary sense—text was textual rather than graphical and hence was searchable, elements of the body of the document were broken down into component parts so that images and their captions were distinct objects in the paper&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib39" rel="" target="blank">39</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib40" rel="" target="blank">40</a>].</p>
        <p class="paragraph">By 2000, the Journal of Biological Chemistry noted that, following the lead of Highwire press, many journals now considered the online version of the publication to be the version of record.[2] As Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F16" rel="" target="blank">16</a>&nbsp;shows, the precipitous switch from initial-form to full-form names in 1995 coincides precisely with the launch of the journal online, suggests that the new system made first names mandatory.</p>
        <p class="paragraph">The BMJ makes a similar shift, also in 1995, although its profile suggests that that first names are strongly preferred but not enforced&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib41" rel="" target="blank">41</a>]. Prior to this, in around 1975, it appears that all three journals, updated their editorial policies as there are sharp changes of behaviour. Both the BMJ and the Journal of Biological Chemistry appear to have put journal policies in place that mandate (or close to mandate) initial form, while Tetrahedron appears to have either removed a mandate for initial form or even actively promoted full name form given the initial rapid drop off from close to 100% to 60% initial-form name usage within just five years from 1975-1980.</p>
        <p class="paragraph">By 2016, the BMJ had switched to using ScholarOne for manuscript submission. As demonstrated in the BMJ tutorial, within this workflow author names are not keyed in individually, but like a CRM system, selected from a database of previously recorded identities. If a person cannot be found (by email address), a new record can be created, and given names and last names are required. The system is normative in the sense that no indication is given that a given name could just be an initial. Authors are not so much recorded against the article as people are linked. A single representation of an author is now used across all journals that use the ScholarOne service. The BMJ sees no usage of initial-form names from 2016.</p>
        <p class="paragraph">The effect of technology as a driver of change is clearly delineated in this plot as Tetrahedron declines gradually indicating a change in social preference or weaker forms of change such as journal policy and house style changes. Tetrahedron did not go live with its first online submission system until 1995, so it is assumed that this change was managed offline by typesetters.</p>
        <h3>Technological Analysis</h3><p class="paragraph">We may reasonably ask what the impetus was for the change in behaviour in the medicine-related fields seen in a previous section (the significant, sharp discontinuity that we see for Biomedical and Clinical Sciences, and Health Sciences in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F14" rel="" target="blank">14</a>). The speed of change suggests that this was a technological rather than a cultural driver, since technological changes tend to be implemented with greater speed. Indeed, since the medical fields share infrastructures such as MedLine and PubMed, which are funder orientated, there could be a strong funding-aligned impetus that we can pinpoint in this case.</p>
        <p class="paragraph">In order to explore PubMed we need to find differentiating characteristics of the PubMed data to be able to track effects. Helpfully, PubMed and its anticedents such as MedLine have conferred unique identifiers to articles prior to the advent the generalised use of Crossref DOIs. This means that some articles on PubMed do not have DOIs and only have a PubMed identifier.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F17-1024x623.png" alt="" data-id="68892a1e-eafb-4aae-8400-7bb19062cabd" contenteditable="false"><figcaption class="wp-element-caption">Figure 17:&nbsp;Percentage incidence of authors using initial form on PubMed articles with (blue) and without DOIs (red).</figcaption></figure><p class="paragraph">Figure&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F17" rel="" target="blank">17</a>&nbsp;shows distinct and different behaviours between authors associated with articles appearing on PubMed that are associated with DOIs and those which aren’t. The blue line shows the percentage of authors associated with a DOI where initial form is used; the red line shows the same percentage for authors associated with articles without a DOI. For articles with a DOI, we see a gradual change to lower rates of initial form being used with authors migrating to full form. We note a small drop in the blue line around 2002 which, we speculate, would be associated with the launch of PubMed’s new platform. This new platform supported given name metadata fields and hence naturally encouraged the use of full form names&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib42" rel="" target="blank">42</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib43" rel="" target="blank">43</a>].</p>
        <p class="paragraph">Articles without a DOI (only having a PubMed identifier), shown in the red line of Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F17" rel="" target="blank">17</a>, have a distinctly different shape. The difference in behaviour of the red line is explained by a systemic development: When Crossref introduced the opportunity to implement DOIs for academic articles, their metadata format was sufficiently fully formed and modern to allow for a field to include given name and publishers adopting the new standard were able to update their back catalogues with the additional data. Thus, the blue line is a representation of the actual metadata included on articles as they were published, whereas the red line is the result of the legacy metadata standard that existed prior to the advent of Crossref’s metadata schema&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib44" rel="" target="blank">44</a>]. Even though articles without a DOI may, in fact, have full form names on the publication this is not represented in the metadata.</p>
        <p class="paragraph">Similar parallels in behaviour can be seen with other major bibliographic and bibliometric databases that existed through this period, with Web of Science making collecting first names from records processed after 2007, and allowing them to be searched in 2011&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib45" rel="" target="blank">45</a>], which Scopus had done in 2004&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib46" rel="" target="blank">46</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib47" rel="" target="blank">47</a>].</p>
        <p class="paragraph">The ongoing decline in the use of initial form from 2002 to present day, may not be due only to technology changes but rather may be motivated by changes to another scholarly institution—research evaluation. In the period from 2002, technological developments took place to introduce of (current research information systems) CRISes&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib48" rel="" target="blank">48</a>], to address the need for reporting data in research economies that introduced research evaluation such as the UK and Australia. It was not until 2010 the ORCID launched formally to tackle the disambiguation problem at a deeper systemic level - perhaps surprisingly the launch of ORCID does not seem to have a particularly direct effect on these data&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib49" rel="" target="blank">49</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib50" rel="" target="blank">50</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib51" rel="" target="blank">51</a>].</p>
        <h3>Gender Analysis</h3><p class="paragraph">Finally, we turn our attention towards a gender-focused analyses. We have pointed out reasons that gender may be hidden by choice in certain contexts in the introduction to this paper. However, it is clear that the reasons for the obscuring of gender in research publication are considerably more subtle&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib52" rel="" target="blank">52</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib53" rel="" target="blank">53</a>]. Countless studies demonstrate differences in outcome for women in peer review in both grant and publication contexts, and citations of their work is lower than for male counterparts&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib54" rel="" target="blank">54</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib55" rel="" target="blank">55</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib56" rel="" target="blank">56</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib28" rel="" target="blank">28</a>]. Further work seeks to understand the causal nature of these effects&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib57" rel="" target="blank">57</a>]. What we show here is that our data analysis as a further dimension for consideration namely that, perhaps unsurprisingly, technology can be a significant modifier to gender visibility in publication. We note that the results shown in this section rely on statistical methods and hence have known limitations, however, we do believe them to be sufficiently robust for the purposes of a high-level indicative analysis, as presented here. The work of Lockhart et al.[<a href="https://arxiv.org/html/2404.06500v2#bib.bib58" rel="" target="blank">58</a>]&nbsp;provides a critical understanding of some of the technical issues with this type of analysis.</p>
        <figure><img src="https://cms.metaror.org/wp-content/uploads/2024/11/F18-974x1024.png" alt="" data-id="1f14b590-60fb-4446-889a-82180bfde99d" contenteditable="false"><figcaption class="wp-element-caption">Figure 18:&nbsp;The development of perceived gender of authors on PubMed publications from 1980 to 2022. Blue line: Authors with names statistically associated with men; Red line: Authors with names statistically associated with women; Grey line: Authors with initials only; Green line: Authors with statistically indeterminate gendered-names.</figcaption></figure><p class="paragraph">Figure&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F18" rel="" target="blank">18</a>&nbsp;illustrates the development of the perception of gender in authorship from 1980 to 2022 in the context of PubMed. As we have seen in Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F17" rel="" target="blank">17</a>&nbsp;there is a distinct technological event in 2002 that caused a shift away from initial form and toward full form names. We see that both the blue line (names statistically associated with men) and the red line (names statistically associated with women) jump upward around 2002, at which point there is a steep decline in initial form usage (grey line). Increased representation of full names has given rise to an increase in indeterminacy (green line), which has steadied at around 10% of output.</p>
        <p class="paragraph">While the step in the blue line in 2002 draws the eye it is important to take this change as a proportion of the population—a move from circa 38% names statistically associated with men to circa 44% is a 15.7% increase. However, for names statistically associated with women, the move is from 12% to more than 14% - closer to a 20% increase. In addition the gradient of the increase in names statistically associated with men in the 20 years from 2002 to 2022 is at most 8% (from 44% to 52%) compared with names statistically associated with women which has moved by more than 10%.</p>
        <p class="paragraph">While the reporting of gender participation in research is still relatively nascent with systematised aggregation of reporting only available since around 1996. Already in many regions of the world that reported in 2002, the date of the change in PubMed systems, women regularly accounted for between 30% and 50% of researchers&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib59" rel="" target="blank">59</a>]. Yet, PubMed authorships at the time attribute only 15% of output to those with names statistically associated with women, while around 32% of output was either of indeterminate or unknown gender. This, together with the steeper rise in participation of women in using full form names, suggests that the initial form convention proportionally over represented women over men, making the contributions of women researchers systematically more difficult to identify.</p>
        <h2>Discussion</h2><p class="paragraph">We have argued that the period from 1945 until 1980 defined a period that we call the “Initial Era”. Rather than being a normal period, we believe that it is exceptional in the scholarly record, being the only era that we can identify in which there is an extended period where initial form was used instead of full form names on academic manuscripts.</p>
        <p class="paragraph">Each of the subsections in the Results section of this paper examines the data from a different aspect to allow us to, firstly, assess the robustness of the data, and secondly, to draw together a picture of the causes and effects of the Initial Era. In this section we will use our results to speculate on the likely genesis of the Initial Era and to examine the effects. We will finish by considering the future of the scholarly record and the potential for future bibliometric archaeologists to perform similar studies.</p>
        <h3>The rise of the Initial Era</h3><p class="paragraph">We speculate that the adoption of initial form during this exceptional 40-year period is far from Vannevar Bush’s Endless Frontier&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib24" rel="" target="blank">24</a>]&nbsp;being accidental, as it was a highly influential document that appeared at the dawn of US-preeminence, a time when science was being established as “serious business” with a more formal cultural norm associated with it. This may have been influenced by the rise in importance of science and medicine and the view that technology could solve all problems. Similarly, the Marshall Plan rebuilding of Europe could have led to the import of norms the US, but this seems less likely since the US is continually one of the lower users of initial-form (see Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>).</p>
        <p class="paragraph">However, during this period, research output was dominated by the US, the UK and Japan—all countries that were predisposed to publish in English and to reflect the cultural norms in the US for political reasons. The Bretton-Woods Era came to an end in the late 1970s and the Cold War in the late 1980s, and with the rise of globalisation greater international collaboration has been possible, leading to Adam’s Fourth Age&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib19" rel="" target="blank">19</a>]. Yet, as we see from Figs.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F9" rel="" target="blank">9</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F10" rel="" target="blank">10</a>&nbsp;and&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F11" rel="" target="blank">11</a>, the use of initial form persisted (and even increased, possibly due to the constraints of paper media and their interaction with the era of Big Science) from 1980 to 2000. As demonstrated in our technological analysis, it was a combination of house style decisions, technology changes in the form of PubMed and Crossref, and the switch to online journal submission systems that fundamentally changed research culture back to full name usage. It remains a “chicken and egg” issue as to whether technology evolved to meet cultural needs or whether technological change gave cultural norms an opportunity to assert themselves.</p>
        <p class="paragraph">The methodology behind our analysis in Sec.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.SS3" rel="" target="blank">III.3</a>&nbsp;is prone to reflecting international trends. By construction, the unit of analysis is the paper and is classified as an initial-form paper only in the situation that all authors use that form. There are normative trends implicit in a global research community—publishers have house styles that are applied regardless of origin; if the authors who brought funding for the work, or who are in some other way senior, choose a style of publishing their name on the paper perhaps there is a psychological bias that goes on that we cannot track in the data; if a majority of authors choose a specific style then how many authors are comfortable breaking step and choosing a different form for their name? International collaboration is another anchor to which our methodology is particularly sensitive as only papers that are entirely initial-based factor in analysis such as Sec.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.SS3" rel="" target="blank">III.3</a>—hence, a core assumption of the analysis is that the kinds of effects discussed here were sufficiently strong as to normalise behaviour across a paper regardless of geographic background.</p>
        <p class="paragraph">As collaboration has become more of a norm in research (see Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S1.F3" rel="" target="blank">3</a>) and as many countries have invested in and developed their own advanced research economies (see Fig.&nbsp;<a href="https://arxiv.org/html/2404.06500v2#S3.F13" rel="" target="blank">13</a>), a much more diverse community has emerged. Hence, once “senior partners” are now just simple partners. On the one hand, this will necessarily require the representation of newly evolved norms. Yet, the norms are already set and new norms take time to evolve.</p>
        <h3>The fall of the Initial Era</h3><p class="paragraph">As with any era in history, a critical understanding the outcomes and impacts of the period are critical in understanding ourselves and improving current and future practices.</p>
        <p class="paragraph">The fall of the Initial Era is fairly clear cut from a technological perspective. We posit that the current trend is a mix three major factors:</p>
        <ol><li data-listnumber=""><p class="paragraph">1.&nbsp;Technological harmonisation: Allowing the choice of capturing and using full names across many different types of system from research information management within institutions to evaluation, and from publishing to grant application and administration;</p>
        </li><li data-listnumber=""><p class="paragraph">2.&nbsp;Globalisation of research: The natural harmonisation of norms as more and more researchers collaborate outside a narrow context;</p>
        </li><li data-listnumber=""><p class="paragraph">3.&nbsp;Trust: The ongoing need to propagate trust in the sense of understanding the identity of participants in the research ecosystem.</p>
        </li></ol><p class="paragraph">All three of these points play into the greater narrative of how research is changing around us. There is a greater need for transparency as research has an increasingly significant effect on everyday lives and as governments investment more in research, it is critical to make knowledge common. The move to greater transparency is multifaceted—making the knowledge itself more transparently available is just one stream; making its production and provenance transparent is another. Without transparency of production and provenance, we cannot expect the trust, which is needed for research to progress.</p>
        <p class="paragraph">The articulation of this need for openness is often found in evaluation mechanisms, either national or funder based. All the factors that we identify above relate to evaluation. Technological developments, especially around name disambiguation, relate to the rise of ORCID as a standard&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib60" rel="" target="blank">60</a>,&nbsp;<a href="https://arxiv.org/html/2404.06500v2#bib.bib61" rel="" target="blank">61</a>], the use research information management systems and research profiling tools&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib62" rel="" target="blank">62</a>]. The globalisation of research has led to increased researcher movement and for researchers to be able to move freely, they need to be able to assert their credentials—which involves asserting identity in relation to their research work. And, in a world where nefarious actors are on the rise&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib63" rel="" target="blank">63</a>], understanding the provenance of research often relies on understanding identity.</p>
        <p class="paragraph">While one may think that the study of the use of names in academic work is perhaps facile, our study shows that it is anything but trivial. Ultimately, we believe that it is possible to justify the argument that the effect of a period in which the usage of given names is suppressed, during the Initial Era, has been to hide and marginalise the contribution of women to research; suppress data about social norms and increase the vulnerability of the research system to abuse by weakening provenance.</p>
        <h3>The future of bibliometric archaeology</h3><p class="paragraph">If we have learned anything in the writing of this paper, we must reflect that the lens through which we look is as important as the analysis itself. While&nbsp;<em>Dimensions</em>&nbsp;is a useful tool in understanding demographics and evolution of certain types of literature, it is a product of its world and of its construction&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib30" rel="" target="blank">30</a>]–specifically the requirement of a DOI or other mainstream identifier limits Dimensions’ coverage at this time. Data coverage pre-1800 shows low volumes of output and, in part, this is because output levels are lower but it is also because strategic choices have had to be made by infrastructure organisations in what should be afforded, the cost and effort of creating digital versions and digital identifiers for past material that is not likely to be of current research value in a non-sociohistorical setting. This mirrors the more hidden lack of data that we have identified in the Initial Era, where the contribution of women is hidden through the technical implementation of a social construction.</p>
        <p class="paragraph">In the current contemporary setting, capturing and retaining information that describes an individual is changing: GDPR regulations in Europe and their equivalents around the world are designed to allow people to be forgotten if they wish. Yet understanding the identity of those who carried out a piece of research seems natural to many of us and even essential information in order to understand biases that may be present. If one is thinking purely scientifically about record keeping, then it makes sense to gather as much data as possible. However, this shows a profound lack of subtlety and even respect when considering issues of identity and ways of knowing—cultural approaches that come from a different tradition than that of Western research but which, as research diversifies its community, need to be included.</p>
        <p class="paragraph">It is apposite to ask whether we live in a special time in bibliometric analysis where name data continues to be freely available and whether this period will persist. ORCID now assigns unique identifiers to any researcher who would like to have one. We live in a time of increasing polarisation and researchers who wish to carry out certain types of research or express particular opinions can face censure or removal of funding. If we take the logic of using research identifiers further, some might argue that the adoption of cryptographic identities, like Satoshi Nakamoto, would allow for researchers to regain intellectual freedom. Zero-knowledge proofs could be used to authenticate claims for career progression, double-blind peer review would be built in to such a system, potentially increasing the overall fairness of both publication and funding practices. Others might argue that this undermines deeply the trust that we have in the scholarly record and permits abusive expression or propagation of fake research or support of political or commercial interests from “behind a mask”. This is, perhaps, the modern-day equivalent of the binaries pointed out by Evans et al.&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib64" rel="" target="blank">64</a>]&nbsp;in their commentary on the role of the algorithm.</p>
        <p class="paragraph">From a purely analytical standpoint, it would be sad to see the scholarly record lose an aspect that is reflective of current trends. But, perhaps one role for blockchain-like technologies may be in the detailed capture of demographic information of researchers in a way that protects it for future generations in much the same way that detailed census results are not revealed at the time but which are held in trust for a hundred years. While statistical information on gender, culture and background might be helpful in workforce planning and in helping to ensure that the research system carries on working toward greater levels of inclusion and representation, detailed information may be preserved for future historians and bibliometric archaeologists.</p>
        <p class="paragraph">More positively, it could be argued that the transformation away from name formalism should not stop at author bylines. Name formalism is also embraced in reference formats. It could be argued that even within a paper, this formalism suppresses the diversity signal in the research that we encounter. Reference styles were defined in a different era with physical space constraints. Is it time to reconsider these conventions - establishing new paths of analysis in the bibliographic record?</p>
        <p class="paragraph">Within contribution statements that use the Credit Ontology&nbsp;[<a href="https://arxiv.org/html/2404.06500v2#bib.bib65" rel="" target="blank">65</a>], initials are also commonly employed to refer to authors although this is not part of the standard. This convention also creates disambiguation issues when two authors share the same surname and first initials. Here too, as the Digital Structure of a paper continues to evolve, we should be careful not to unquestioningly embed the naming conventions of a different era into our evolving metadata standards.</p>
        <h2>Acknowledgements</h2><p class="paragraph">The authors wish to thank Briony Fane for her careful reading and commenting on the manuscript, and to Kathryn Weber Boer for her suggestions to improve discussions on issues of gender.</p>
        <h2>Data Availability</h2><p class="paragraph">The data and code for this paper is available from Figshare at:&nbsp;<a href="https://doi.org/10.6084/m9.figshare.25664154" rel="" target="blank">https://doi.org/10.6084/m9.figshare.25664154</a></p>
        <h2>Author Contribution</h2><p class="paragraph">Simon J Porter: Conceptualization; Formal Analysis; Methodology; Visualisation; Writing - original draft; Writing - review &amp; editing.</p>
        <p class="paragraph">Daniel W Hook: Formal Analysis; Methodology; Visualisation; Writing - original draft; Writing - review &amp; editing.</p>
        <h2>Conflict of interests</h2><p class="paragraph">The authors of this paper are both employees of Digital Science, the owner and operator of Dimensions.</p>
        <h2>References</h2><ol><li data-listnumber=""><p class="paragraph">P. A. Hall, ed., Varieties Of Capitalism: The Institutional Foundations of Comparative Advantage, illustrated edition ed. (Oxford University Press, U.S.A., Oxford England ; New York, 2001).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">J. Jarvis, The Gutenberg Parenthesis: The Age of Print and Its Lessons for the Age of the Internet (Bloomsbury Academic, New York, 2023).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">A. Johns, The Science of Reading: Information, Media, and Mind in Modern America (University of Chicago Press, Chicago; London, 2023).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">M. Taylor, "How do publications find their audience?" (2024), publisher: figshare.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">B. Macaluso, V. Larivière, T. Sugimoto, and C. R. Sugimoto, Academic Medicine 91, 1136 (2016).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">C. Ni, E. Smith, H. Yuan, V. Larivière, and C. R. Sugimoto, Science Advances 7, , eabe4639 (2021), publisher: American Association for the Advancement of Science.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">C. R. Sugimoto, Y.-Y. Ahn, E. Smith, B. Macaluso, and V. Larivière, The Lancet 393, 550 (2019), publisher: Elsevier.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">D. Kozlowski, D. S. Murray, A. Bell, W. Hulsey, V. Larivière, T. Monroe-White, and C. R. Sugimoto, PLOS ONE 17, e0264270 (2022a), publisher: Public Library of Science.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">D. Kozlowski, V. Larivière, C. R. Sugimoto, and T. Monroe-White, Proceedings of the National Academy of Sciences 119, e2113067119 (2022b), publisher: Proceedings of the National Academy of Sciences.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">A. Meadows, Journal of Information Science 11, 27 (1985).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">D. W. Hook and S. J. Porter, Frontiers in Research Metrics and Analytics 6 (2021).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">L. Bornmann, R. Haunschild, and R. Mutz, Humanities and Social Sciences Communications 8, 1 (2021), number: 1 Publisher: Palgrave.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">Philosophical Transactions of the Royal Society of London 4, 1075 (1997), publisher: Royal Society.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">L. A. Whited, ed., The Ivory Tower and Harry Potter: Perspectives on a Literary Phenomenon (University of Missouri Press, Columbia, Mo., 2004).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">S. Nakamoto, "Bitcoin: A Peer-to-Peer Electronic Cash System," (2008).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">R. E. Sorge, L. J. Martin, K. A. Isbester, S. G. Sotocinal, S. Rosen, A. H. Tuttle, J. S. Wieskopf, E. L. Acland, A. Dokova, B. Kadoura, P. Leger, J. C. S. Mapplebeck, M. McPhail, A. Delaney, G. Wigerblad, A. P. Schumann, T. Quinn, J. Frasnelli, C. I. Svensson, W. F. Sternberg, and J. S. Mogil, Nature Methods 11, 629 (2014), publisher: Nature Publishing Group.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">A. Katsnelson, Nature (2014), 10.1038/nature.2014.15106, publisher: Nature Publishing Group.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">S. Reardon, Nature (2017), 10.1038/nature.2017.23022, publisher: Nature Publishing Group.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">J. Adams, Nature 497, 557 (2013), number: 7451 Publisher: Nature Publishing Group.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">M. Thelwall and N. Maflahi, Quantitative Science Studies 3, 331 (2022).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">The first paper in Dimensions with co-authorship of more than 100 authors is in Chemistry from 1928 |66.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">J. P. Pardo-Guerra, The Quantified Scholar: How Research Evaluations Transformed the British Social Sciences (Columbia University Press, 2022).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">S. J. Porter and D. W. Hook, Frontiers in Research Metrics and Analytics 7 (2022).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">V. Bush, The Endless Frontier, Report to the President on a Program for Postwar Scientific Research, Tech. Rep. (OFFICE OF SCIENTIFIC RESEARCH AND DEVELOPMENT WASHINGTON DC, 1945).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">J. R. Oppenheimer, Proceedings of the National Academy of Sciences 50, 1194 (1963), publisher: Proceedings of the National Academy of Sciences.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">D. Yan, Z. Hai-qing, D. Guo-bao, and T. Cheng, Chinese Journal of Integrative Medicine 11, 229 (2005).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">O. Brück, Communications Medicine 3, 1 (2023), publisher: Nature Publishing Group.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">A. Holmes and S. Hardy, "Gender bias in peer review — opening up the black box," (2019).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">W. R. Lefanu, Bulletin of the Institute of the History of Medicine 5, 735 (1937), publisher: The Johns Hopkins University Press.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">D. W. Hook, S. J. Porter, and C. Herzog, Frontiers in Research Metrics and Analytics 3 (2018).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">W. A. Kretzschmar, in Language in the USA: Themes for the Twenty-first Century, edited by E. Finegan and J. R. Rickford (Cambridge University Press, Cambridge, 2004) pp. 39-57.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">T. A. Velden, A.-u. Haque, and C. Lagoze, in Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries, JCDL '11 (Association for Computing Machinery, New York, NY, USA, 2011) pp. 241-250.</p>
        </li>
        <li data-listnumber=""><p class="paragraph">Y. Liu, L. Chen, Y. Yuan, and J. Chen, American Journal of Physical Anthropology 148, 341 (2012).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">G. Hofstede, Journal of International Business Studies 14, 75 (1983).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">G. Hofstede, Culture's Consequences: Comparing Values, Behaviors, Institutions and Organizations Across Nations, second edition ed. (SAGE Publications, Inc, Thousand Oaks, Calif., 2003).</p>
        </li>
        <li data-listnumber=""><p class="paragraph">G. Hofstede, Online Readings in Psychology and Culture 2 (2011), 10.9707/2307-0919.1014.</p></li>
        <li data-listnumber=""><p class="paragraph">S. J. Porter, L. Hawizy, and D. W. Hook, Quantitative Science Studies 4, 127 (2023).</p></li>
        <li data-listnumber=""><p class="paragraph">C. P. Snow, The Two Cultures, reissue edition ed. (Cambridge University Press, Cambridge, 2012).</p></li>
        <li data-listnumber=""><p class="paragraph">Journal of Biological Chemistry 275, 12361 (2000).</p></li>
        <li data-listnumber=""><p class="paragraph">A. B. Watson, Journal of Vision 11, ii (2011).</p></li>
        <li data-listnumber=""><p class="paragraph">J. Smith and R. Smith, BMJ 312, 1626 (1996).</p></li>
        <li data-listnumber=""><p class="paragraph">NLM, "MEDLINE Data Changes - 2002. NLM Technical Bulletin. Nov-Dec 2001," (2002).</p></li>
        <li data-listnumber=""><p class="paragraph">NLM, "Skill Kit: Searching Full Author Names in PubMed®," (2009), publisher: U.S. National Library of Medicine.</p></li>
        <li data-listnumber=""><p class="paragraph">Crossref, The Formation of CrossRef: A Short History, Tech. Rep. (CrossRef, 2009).</p></li>
        <li data-listnumber=""><p class="paragraph">Clarivate, "Web of Science Core Collection: Explanation on Full Author Names," (2022).</p></li>
        <li data-listnumber=""><p class="paragraph">P. J. Hane, "Elsevier Announces Scopus Service," (2004).</p></li>
        <li data-listnumber=""><p class="paragraph">E. Sullo, Journal of the Medical Library Association 95, 367 (2007).</p></li>
        <li data-listnumber=""><p class="paragraph">B. Jörg, Data Science Journal 9 (2010), 10.2481/dsj.CRIS4.</p></li>
        <li data-listnumber=""><p class="paragraph">D. Butler, Nature 485, 564 (2012), number: 7400 Publisher: Nature Publishing Group.</p></li>
        <li data-listnumber=""><p class="paragraph">Q. Schiermeier, Nature 526, 281 (2015), number: 7572 Publisher: Nature Publishing Group.</p></li>
        <li data-listnumber=""><p class="paragraph">A. Meadows, L. L. Haak, and J. Brown, 32, 9 (2019), number: 1 Publisher: UKSG in association with Ubiquity Press.</p></li>
        <li data-listnumber=""><p class="paragraph">C. López Lloreda, (2022), 10.1126/science.caredit.adf3063.</p></li>
        <li data-listnumber=""><p class="paragraph">E. G. Teich, J. Z. Kim, C. W. Lynn, S. C. Simon, A. A. Klishin, K. P. Szymula, P. Srivastava, L. C. Bassett, P. Zurn, J. D. Dworkin, and D. S. Bassett, Nature Physics 18, 1161 (2022), publisher: Nature Publishing Group.</p></li>
        <li data-listnumber=""><p class="paragraph">C. D. Zhou, M. G. Head, D. C. Marshall, B. J. Gilbert, M. A. El-Harasis, R. Raine, H. O'Connor, R. Atun, and M. Maruthappu, BM) Open 8, e018625 (2018), publisher: British Medical Journal Publishing Group Section: Health policy.</p></li>
        <li data-listnumber=""><p class="paragraph">H. Draux and S. Kundu, Gender Imbalance in Cancer Research Grants, Tech. Rep. ([object Object], 2018) artwork Size: 2013631 Bytes.</p></li>
        <li data-listnumber=""><p class="paragraph">H. Draux, S. Kundu, and S. Porter, Gender Representation in UK Research Institutions, Tech. Rep. ([object Object], 2019) artwork Size: 3057372 Bytes.</p></li>
        <li data-listnumber=""><p class="paragraph">V. A. Traag and L. Waltman, (2022), arXiv:2207.13665 [cs.DL]</p></li>
        <li data-listnumber=""><p class="paragraph">J. W. Lockhart, M. M. King, and C. Munsch, Nature Human Behaviour 7, 1084 (2023).</p></li>
        <li data-listnumber=""><p class="paragraph">U. I. for Statistics, "Share of women among total researchers by country, 1996-2018,"</p></li>
        <li data-listnumber=""><p class="paragraph">L. L. Haak, A. Meadows, and J. Brown, Frontiers in Research Metrics and Analytics 3 (2018).</p></li>
        <li data-listnumber=""><p class="paragraph">S. J. Porter, Frontiers in Research Metrics and Analytics 7, 779097 (2022).</p></li>
        <li data-listnumber=""><p class="paragraph">V. Ilik, M. Conlon, G. Triggs, M. White, M. Javed, M. Brush, K. Gutzman, S. Essaid, P. Friedman, S. Porter, M. Szomszor, M. A. Haendel, D. Eichmann,<br>and K. L. Holmes, Frontiers in Research Metrics and Analytics 2 (2018), 10.3389/frma.2017.00012, publisher: Frontiers.</p></li>
        <li data-listnumber=""><p class="paragraph">S. J. Porter and L. D. McIntosh, "Identifying Fabricated Networks within Authorship-for-Sale Enterprises," (2024), arXiv:2401.04022 [cs].</p></li>
        <li data-listnumber=""><p class="paragraph">J. Evans, T. Reigeluth, and A. Johns, Osiris 38, 19 (2023).</p></li>
        <li data-listnumber=""><p class="paragraph">L. Allen, J. Scott, A. Brand, M. Hlava, and M. Altman, Nature 508, 312 (2014).</p></li>
        <li data-listnumber=""><p class="paragraph">F. Emich, A. Benedetti-Pichler, F. Henrich, L. Moser, R. Strebinger, F. Pregl, F. Zaribnicky, L. Rosenthaler, E. M. Chamot, H. Herbst, H. Fitting, H. Ambronn, A. Frey, L. Wright, K. John, F. K. Reinsch, A. Zimmern, M. Coutin, E. Lehmann, J. L. Pech, R. Harder, H. Siedentopf, C. Spierer, Lieberkühn, W. Kaiser, Rheinberger, W. Kraemer, C. Kern, H. Pohle, S. v. Wachenfeldt, F. Lossen, A. Köhler, Proell, O. Linde, E. Saxl, W. Schäffer, J. Kisser, F. K. Studnióka, F. Roll, T. Huzella, R. Chambers, T. Péterfi, C. V. Taylor, G. de Mottoni, D. L. Parkhurst, H. Utermöhl, Goring, E. Naumann, Volk, G. Lunde, E. A. Hill, E. Q. Adams, G. Linzenmeier, E. Kaufmann, P. Kirkpatrick, M. C. Magarian, R. Wolff, K. Schuhecker, H. C. Hagedorn, B. N. Jensen, W. Geilmann, R. Höltie, L. Dienes, L. Pincussen, P. B. Rehberg, S. Wermuth, M. Shepherd, H. B. Rasmussen, C. E. Christensen, R. Mellet, M. A. Bischoff, H. Hiller, J. J. Hopfield, F. Paneth, K. Peters, P. Günther, H. M. Elsey, M. Crespi, E. Moles, W. Kliefoth, O. E. Frivold, R. E. Burk, B. Noyes, W. Ewald, R. Whytlaw-Gray, H. Whitaker, H. Figour, F. Sautier, F. Verzár, J. Barcroft, L. Condorelli, E. P. Poulton, W. R. Spurrell, E. C. Warner, R. Suhrmann, K. Clusius, J. J. Manley, W. A. Roth, G. Naeser, O. Döpke, H. Leontjew, H. S. Patterson, R. W. Gray, R. A. Millikan, H. G. Barbour, W. F. Hamilton, Dickinson, E. A. Vuilleumier, W. Klemm, W. Biltz, A. Stock, and G. Ritter, Zeitschrift für analytische Chemie 74, 191 (1928).</p></li>
        </ol>
        `,
    },
  },
};

document.addEventListener("DOMContentLoaded", () => {
  const $contentSection = document.querySelector("#content-section");
  const $content = document.querySelector("#content");
  const $authorsList = document.querySelector("#author-data");
  const $affiliationsList = document.querySelector("#affiliations");
  const tabs = document.querySelectorAll(".tab");
  const $doi = document.querySelector("#doi");
  const $title = document.querySelector("#title");
  const $articleNav = document.querySelector("#article-nav > ul");
  const $publishedOn = document.querySelector(".published-on");
  const $reviewSection = document.querySelector("#reviews-section");
  const $dates = document.querySelectorAll(".dateblock");

  const handleTabChange = (e) => {
    const tab = e.target;
    tabs.forEach((tab) => tab.classList.remove("active"));
    tab.classList.add("active");
    tab.textContent !== "Preprint" && ($contentSection.style.display = "none");
    tab.textContent !== "Reviews" && ($reviewSection.style.display = "none");
    tab.textContent === "Preprint" && ($contentSection.style.display = "flex");
    tab.textContent === "Reviews" && ($reviewSection.style.display = "flex");
    const url = new URL(window.location);
    url.searchParams.set("tab", tab.textContent.toLowerCase());
    window.history.pushState({}, "", url);
  };

  tabs.forEach((tab) => {
    tab.addEventListener("click", handleTabChange);
  });

  const urlParams = new URLSearchParams(window.location.search);
  const tabParam = urlParams.get("tab");

  if (!tabParam || !["preprint", "reviews"].includes(tabParam)) {
    handleTabChange({ target: tabs[0] });
  } else {
    const activeTab = Array.from(tabs).find(
      (tab) => tab.textContent.toLowerCase() === tabParam
    );

    if (activeTab) {
      handleTabChange({ target: activeTab });
    }
  }

  const articleNumber = getArticleNumberFromURL();
  const manuscript = manuscriptData[articleNumber];
  const { doi, sections, title, authors, published, reviews, dates } =
    manuscript;

  const name = (name) => `<span class="author-name">${name}</span>`;
  const email = (email) =>
    email ? `<a class="email" href="mailto:${email}"></a>` : "";
  const orcid = (orcid) =>
    orcid
      ? `<a class="orcid" target="_blank" href="https://orcid.org/${orcid}"></a>`
      : "";

  $authorsList.innerHTML = authors
    .map(
      (author) =>
        `<li>${name(author.name)}${email(author.email)}${orcid(
          author.orcid
        )}</li>`
    )
    .join("");

  $affiliationsList.innerHTML = authors
    .map((author) => author.affiliations.join(","))
    .join(",");

  published && ($publishedOn.textContent = published);
  title && ($title.textContent = title);
  doi && ($doi.textContent = `https://doi.org/${doi}`);
  doi && ($doi.href = `https://doi.org/${doi}`);
  articleNumber && ($content.innerHTML = "");
  $dates.forEach((date) => {
    const dateContent = Object.entries(dates).map(([k, v]) => {
      return `<div class="date-block"><p>${k}:</p><span>${v}</span></div>`;
    });
    console.log(dateContent);
    console.log($dates);
    date.innerHTML = dateContent.join("");
  });

  reviews.forEach(({ name, orcid, review }) => {
    const reviewEl = document.createElement("div");
    reviewEl.innerHTML = `<h4>${name}</h4><p>${orcid}</p>${review}`;
    $reviewSection.appendChild(reviewEl);
  });

  Object.entries(sections).forEach(([k, v]) => {
    const sectionContent = document.createElement("p");
    sectionContent.innerHTML = v;
    if (k !== "rest") {
      const title = document.createElement("h2");
      title.textContent = k;
      title.id = k;
      $content.appendChild(title);
      !Object.keys(sections).includes("rest") &&
        ($articleNav.innerHTML += `<li><a href="#${k}">${k}</a></li>`);
    }
    console.log(sectionContent);
    $content.appendChild(sectionContent);

    if (k === "rest") {
      const headings = $content.querySelectorAll("h2");
      headings.forEach((h2) => {
        const id = h2.textContent;
        h2.id = id;
        $articleNav.innerHTML += `<li><a href="#${id}">${id}</a></li>`;
      });
    }
  });
});
